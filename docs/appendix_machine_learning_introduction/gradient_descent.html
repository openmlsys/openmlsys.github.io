<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>2. 梯度下降与反向传播 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. 经典机器学习方法" href="classic_machine_learning.html" />
    <link rel="prev" title="1. 神经网络" href="neural_network.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html">附录：机器学习介绍</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">2. </span>梯度下降与反向传播</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/appendix_machine_learning_introduction/gradient_descent.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">4. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">5. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">5.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">5.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">5.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">5.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">6. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">6.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">6.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">6.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">6.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">6.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">7. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">7.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">7.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">7.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">8. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">8.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">8.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">8.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">8.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">8.6. 章节总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">9. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">9.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">9.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">9.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">9.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">10. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">10.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">10.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">10.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">10.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">10.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">10.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">11. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">12. 联邦学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">13. 强化学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">14. 可解释性AI系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_online_machine_learning/index.html">15. 在线机器学习</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">附录：机器学习介绍</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">4. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">5. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">5.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">5.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">5.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">5.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">6. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">6.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">6.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">6.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">6.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">6.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">7. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">7.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">7.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">7.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">8. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">8.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">8.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">8.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">8.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">8.6. 章节总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">9. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">9.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">9.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">9.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">9.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">10. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">10.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">10.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">10.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">10.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">10.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">10.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">11. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">12. 联邦学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">13. 强化学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">14. 可解释性AI系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_online_machine_learning/index.html">15. 在线机器学习</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">附录：机器学习介绍</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="id1">
<h1><span class="section-number">2. </span>梯度下降与反向传播<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>上面大体上介绍了经典神经网络的内容，那么现在有一个问题，这些网络中的参数是如何确定的呢？如果要解决的问题是一个小感知器就能解决的话，参数可以人为地去确定。但是如果是一个深度网络的话，参数的确定需要自动化，也就是所谓的网络训练，而这个过程需要我们设定一个<strong>损失函数</strong>（Loss
Function）来作为训练优化的一个方向。
常见的损失函数有：1）用来衡量向量之间距离的均方误差(Mean Squared
Error，MSE)
<span class="math notranslate nohighlight">\(\mathcal{L} = \frac{1}{N}\|{y}-\hat{{y}}\|^{2}_{2} = \frac{1}{N}\sum_{i=1}^N(y_{i}-\hat{y}_{i})^{2}\)</span>
和 平均绝对误差(Mean Absolute Error，MAE)
<span class="math notranslate nohighlight">\(\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}|y_{i}-\hat{y}_{i}|\)</span>
，其中<span class="math notranslate nohighlight">\(N\)</span>代表数据样本的数量，用以求平均用，而<span class="math notranslate nohighlight">\(y\)</span>代表真实标签（Ground
Truth）、<span class="math notranslate nohighlight">\(\hat{y}\)</span>代表网络输出的预测标签。
2）分类任务可以用的交叉熵损失（Cross Entropy）
<span class="math notranslate nohighlight">\(\mathcal{L} = - \frac{1}{N} \sum_{i=1}^N \bigg(y_{i}\log\hat{y}_{i} + (1 - y_{i})\log(1 - \hat{y}_{i})\bigg)\)</span>来作为损失数，当且仅当输出标签和预测标签一样的时候损失值才为零。</p>
<p>有了损失值之后，我们就可以利用大量真实标签的数据和优化方法来更新模型参数了，其中最常用的方法是<strong>梯度下降</strong>（Gradient
Descent）。如 <a class="reference internal" href="#gradient-descent2"><span class="std std-numref">图2.1</span></a>所示，
开始的时候，模型的参数<span class="math notranslate nohighlight">\({w}\)</span>是随机选取的，然后求出损失值对参数的偏导数<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {w}}\)</span>，通过反复迭代
<span class="math notranslate nohighlight">\({w}:={w}-\alpha\frac{\partial \mathcal{L}}{\partial {w}}\)</span>完成优化。这个优化的过程其实就可以降低损失值以达到任务目标，其中<span class="math notranslate nohighlight">\(\alpha\)</span>是控制优化幅度的<strong>学习率</strong>（Learning
Rate）。
在实践中，梯度下降最终得到的最小值很大可能是一个局部最小值，而不是全局最小值。不过由于深度神经网络能提供一个很强的数据表达能力，所以局部最小值可以很接近全局最小值，损失值可以足够小。</p>
<figure class="align-default" id="id7">
<span id="gradient-descent2"></span><a class="reference internal image-reference" href="../_images/gradient_descent2.png"><img alt="../_images/gradient_descent2.png" src="../_images/gradient_descent2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图2.1 </span><span class="caption-text">梯度下降介绍。（左图）只有一个可以训练的参数<span class="math notranslate nohighlight">\(w\)</span>；（右图）有两个可以训练的参数<span class="math notranslate nohighlight">\({w}=[w_1,w_2]\)</span>。在不断更新迭代参数后，损失值<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>会逐渐地减小。但是由于存在很多局部最优解，我们往往不能更新到全局最优解。</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>那么接下来，在深度神经网络中如何实现梯度下降呢，这需要计算出网络中每层参数的偏导数<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {w}}\)</span>，我们可以用<strong>反向传播</strong>（Back-Propagation）
<a class="bibtex reference internal" href="../chapter_references/index.html#rumelhart1986learning" id="id2">[Rumelhart et al., 1986]</a><a class="bibtex reference internal" href="../chapter_references/index.html#lecun2015deep" id="id3">[LeCun et al., 2015]</a>来实现。 接下来，
我们引入一个中间量<span class="math notranslate nohighlight">\({\delta}=\frac{\partial \mathcal{L}}{\partial {z}}\)</span>来表示损失函数<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
对于神经网络输出<span class="math notranslate nohighlight">\({z}\)</span>（未经过激活函数，不是<span class="math notranslate nohighlight">\(a\)</span>）的偏导数，
并最终得到<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {w}}\)</span>。</p>
<p>我们下面用一个例子来介绍反向传播算法，
我们设层序号为<span class="math notranslate nohighlight">\(l=1, 2, \ldots L\)</span>（输出层（最后一层）序号为<span class="math notranslate nohighlight">\(L\)</span>）。
对于每个网络层，我们有输出<span class="math notranslate nohighlight">\({z}^l\)</span>，中间值<span class="math notranslate nohighlight">\({\delta}^l=\frac{\partial \mathcal{L}}{\partial {z}^l}\)</span>和一个激活值输出<span class="math notranslate nohighlight">\({a}^l=f({z}^l)\)</span>
（其中<span class="math notranslate nohighlight">\(f\)</span>为激活函数）。
我们假设模型是使用Sigmoid激活函数的多层感知器，损失函数是均方误差（MSE）。也就是说，我们设定：</p>
<ul class="simple">
<li><p>网络结构<span class="math notranslate nohighlight">\({z}^{l}={W}^{l}{a}^{l-1}+{b}^{l}\)</span></p></li>
<li><p>激活函数<span class="math notranslate nohighlight">\({a}^l=f({z}^l)=\frac{1}{1+{\rm e}^{-{z}^l}}\)</span></p></li>
<li><p>损失函数<span class="math notranslate nohighlight">\(\mathcal{L}=\frac{1}{2}\|{y}-{a}^{L}\|^2_2\)</span></p></li>
</ul>
<p>我们可以直接算出激活输出对于原输出的偏导数：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{\partial {a}^l}{\partial {z}^l}=f'({z}^l)=f({z}^l)(1-f({z}^l))={a}^l(1-{a}^l)\)</span></p></li>
</ul>
<p>和损失函数对于激活输出的偏导数：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {a}^{L}}=({a}^{L}-{y})\)</span></p></li>
</ul>
<p>有了这些后，为了进一步得到损失函数对于每一个参数的偏导数，可以使用<strong>链式法则</strong>（Chain
Rule），细节如下：</p>
<p>首先，从输出层（<span class="math notranslate nohighlight">\(l=L\)</span>，最后一层）开始向后方传播误差，根据链式法则，我们先计算输出层的中间量：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({\delta}^{L} =\frac{\partial \mathcal{L}}{\partial {z}^{L}} =\frac{\partial \mathcal{L}}{\partial {a}^{L}}\frac{\partial {a}^L}{\partial {z}^{L}}=({a}^L-{y})\odot({a}^L(1-{a}^L))\)</span></p></li>
</ul>
<p>除了输出层（<span class="math notranslate nohighlight">\(l=L\)</span>）的中间值<span class="math notranslate nohighlight">\({\delta}^{L}\)</span>，其他层（<span class="math notranslate nohighlight">\(l=1, 2, \ldots , L-1\)</span>）的中间值<span class="math notranslate nohighlight">\({\delta}^{l}\)</span>如何计算呢？</p>
<ul class="simple">
<li><p>已知模型结构<span class="math notranslate nohighlight">\({z}^{l+1}={W}^{l+1}{a}^{l}+{b}^{l+1}\)</span>，我们可以直接得到<span class="math notranslate nohighlight">\(\frac{\partial {z}^{l+1}}{\partial {a}^{l}}={W}^{l+1}\)</span>；而且我们已知<span class="math notranslate nohighlight">\(\frac{\partial {a}^l}{\partial {z}^l}={a}^l(1-{a}^l)\)</span></p></li>
<li><p>那么根据链式法则，我们可以得到
<span class="math notranslate nohighlight">\({\delta}^{l} =\frac{\partial \mathcal{L}}{\partial {z}^{l}} =\frac{\partial \mathcal{L}}{\partial {z}^{l+1}}\frac{\partial {z}^{l+1}}{\partial {a}^{l}}\frac{\partial {a}^{l}}{\partial {z}^{l}} =({W}^{l+1})^\top{\delta}^{l+1}\odot({a}^l(1-{a}^l))\)</span></p></li>
</ul>
<p>根据上面的计算有所有层的中间值<span class="math notranslate nohighlight">\({\delta}^l, l=1, 2, \ldots , L\)</span>后，我们就可以在此基础上求出损失函数对于每层参数的偏导数：<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {W}^l}\)</span>和<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {b}^l}\)</span>，以此来根据梯度下降的方法来更新每一层的参数。</p>
<ul class="simple">
<li><p>已知模型结构<span class="math notranslate nohighlight">\({z}^l={W}^l{a}^{l-1}+{b}^l\)</span>，我们可以求出
<span class="math notranslate nohighlight">\(\frac{\partial {z}^{l}}{\partial {W}^l}={a}^{l-1}\)</span> 和
<span class="math notranslate nohighlight">\(\frac{\partial {z}^{l}}{\partial {b}^l}=1\)</span></p></li>
<li><p>那么根据链式法则，我们可以得到<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {W}^l}=\frac{\partial \mathcal{L}}{\partial {z}^l}\frac{\partial {z}^l}{\partial {W}^l}={\delta}^l({a}^{l-1})^\top\)</span>
,
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {b}^l}=\frac{\partial \mathcal{L}}{\partial {z}^l}\frac{\partial {z}^l}{\partial {b}^l}={\delta}^l\)</span></p></li>
</ul>
<p>求得所有偏导数<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {W}^l}\)</span> 和
<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial {b}^l}\)</span>后，我们就可以用梯度下降更新所有参数<span class="math notranslate nohighlight">\({W}^l\)</span>
和 <span class="math notranslate nohighlight">\({b}^l\)</span>：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({W}^l:={W}^l-\alpha\frac{\partial \mathcal{L}}{\partial {W}^l}\)</span>,
<span class="math notranslate nohighlight">\({b}^l:={b}^l-\alpha\frac{\partial \mathcal{L}}{\partial {b}^l}\)</span></p></li>
</ul>
<p>但是还有一个问题需要解决，那就是梯度下降的时候每更新一次参数，都需要计算一次当前参数下的损失值。然而，当训练数据集很大时（<span class="math notranslate nohighlight">\(N\)</span>很大），若每次更新都用整个训练集来计算损失值的话，计算量会非常巨大。
为了减少计算量，我们使用<strong>随机梯度下降</strong>（Stochastic Gradient
Descent，SGD）来计算损失值。具体来说，我们计算损失值不用全部训练数据，而是从训练集中随机选取一些数据样本来计算损失值，比如选取16、32、64或者128个数据样本，样本的数量被称为<strong>批大小</strong>（Batch
Size）。
此外，学习率的设定也非常重要。如果学习率太大，可能无法接近最小值的山谷，如果太小，训练又太慢。
自适应学习率，例如Adam <a class="bibtex reference internal" href="../chapter_references/index.html#kingmaadam2014" id="id4">[Kingma &amp; Ba, 2014]</a>、RMSProp <a class="bibtex reference internal" href="../chapter_references/index.html#tieleman2012rmsprop" id="id5">[Tieleman &amp; Hinton, 2017]</a>和
Adagrad <a class="bibtex reference internal" href="../chapter_references/index.html#duchi2011adagrad" id="id6">[Duchi et al., 2011]</a>等，在训练的过程中通过自动的方法来修改学习率，实现训练的快速收敛，到达最小值点。</p>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="neural_network.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>1. 神经网络</div>
         </div>
     </a>
     <a id="button-next" href="classic_machine_learning.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>3. 经典机器学习方法</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>