<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <title>
   3.2. 计算图的基本构成 — 机器学习系统：设计和实现 1.0.0 documentation
  </title>
  <link href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/sphinx_materialdesign_theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fontawesome/all.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fonts.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/basic.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/d2l.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/d2l.js">
  </script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <link href="../_static/favicon.png" rel="shortcut icon"/>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="generation_of_computational_graph.html" rel="next" title="3.3. 计算图的生成"/>
  <link href="background_and_functionality.html" rel="prev" title="3.1. 计算图的设计背景和作用"/>
 </head>
 <body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer">
   <header class="mdl-layout__header mdl-layout__header--waterfall">
    <div class="mdl-layout__header-row">
     <nav class="mdl-navigation breadcrumb">
      <a class="mdl-navigation__link" href="index.html">
       <span class="section-number">
        3.
       </span>
       计算图
      </a>
      <i class="material-icons">
       navigate_next
      </i>
      <a class="mdl-navigation__link is-active">
       <span class="section-number">
        3.2.
       </span>
       计算图的基本构成
      </a>
     </nav>
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <form action="../search.html" class="form-inline pull-sm-right" method="get">
       <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label class="mdl-button mdl-js-button mdl-button--icon" for="waterfall-exp" id="quick-search-icon">
         <i class="material-icons">
          search
         </i>
        </label>
        <div class="mdl-textfield__expandable-holder">
         <input class="mdl-textfield__input" id="waterfall-exp" name="q" placeholder="Search" type="text"/>
         <input name="check_keywords" type="hidden" value="yes"/>
         <input name="area" type="hidden" value="default"/>
        </div>
       </div>
       <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
        Quick search
       </div>
      </form>
      <a class="mdl-button mdl-js-button mdl-button--icon" href="../_sources/chapter_computational_graph/components_of_computational_graph.rst.txt" id="button-show-source" rel="nofollow">
       <i class="material-icons">
        code
       </i>
      </a>
      <div class="mdl-tooltip" data-mdl-for="button-show-source">
       Show Source
      </div>
     </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <a class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
       <i class="fab fa-github">
       </i>
       GitHub
      </a>
     </nav>
    </div>
   </header>
   <header class="mdl-layout__drawer">
    <!-- Title -->
    <span class="mdl-layout-title">
     <a class="title" href="../index.html">
      <span class="title-text">
       机器学习系统：设计和实现
      </span>
     </a>
    </span>
    <div class="globaltoc">
     <span class="mdl-layout-title toc">
      Table Of Contents
     </span>
     <nav class="mdl-navigation">
      <ul class="current">
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_introduction/index.html">
         1. 导论
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">
           1.1. 机器学习应用
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">
           1.2. 机器学习系统的需求
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">
           1.3. 机器学习系统基本组成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/applicable_readers.html">
           1.4. 适用读者
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_programming_interface/index.html">
         2. 编程接口
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/development_history.html">
           2.1. 机器学习系统编程模型的演进
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">
           2.2. 机器学习工作流
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">
           2.3. 定义深度神经网络
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">
           2.4. C/C++编程接口
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/summary.html">
           2.5. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1 current">
        <a class="reference internal" href="index.html">
         3. 计算图
        </a>
        <ul class="current">
         <li class="toctree-l2">
          <a class="reference internal" href="background_and_functionality.html">
           3.1. 计算图的设计背景和作用
          </a>
         </li>
         <li class="toctree-l2 current">
          <a class="current reference internal" href="#">
           3.2. 计算图的基本构成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="generation_of_computational_graph.html">
           3.3. 计算图的生成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="schedule_of_computational_graph.html">
           3.4. 计算图的调度
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="summary.html">
           3.5. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_advanced/index.html">
         4. 第二部分：进阶篇
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_frontend_and_ir/index.html">
         5. 编译器前端
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">
           5.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">
           5.2. 中间表示
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/ad.html">
           5.3. 自动微分
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">
           5.4. 类型系统和静态分析
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">
           5.5. 常见前端编译优化方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/summary.html">
           5.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_backend_and_runtime/index.html">
         6. 编译器后端和运行时
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/overview.html">
           6.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">
           6.2. 计算图优化
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">
           6.3. 算子选择
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">
           6.4. 内存分配
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">
           6.5. 计算调度与执行
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/summary.html">
           6.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_accelerator/index.html">
         7. 硬件加速器
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">
           7.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">
           7.2. 加速器基本组成原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">
           7.3. 加速器基本编程原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/summary.html">
           7.4. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_data_processing/index.html">
         8. 数据处理框架
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/requirements.html">
           8.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/program_model.html">
           8.2. 易用性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/performance.html">
           8.3. 高效性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/data_order.html">
           8.4. 保序性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/extension.html">
           8.5. 单机数据处理性能的扩展
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/summary.html">
           8.6. 章节总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_model_deployment/index.html">
         9. 模型部署
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">
           9.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">
           9.2. 训练模型到推理模型的转换及优化
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_compression.html">
           9.3. 模型压缩
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_inference.html">
           9.4. 模型推理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_security.html">
           9.5. 模型的安全保护
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/summary.html">
           9.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_distributed_training/index.html">
         10. 分布式训练
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed_training/overview.html">
           10.1. 系统概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed_training/methods.html">
           10.2. 分布式方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed_training/pipeline.html">
           10.3. 流水线并行
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed_training/collective.html">
           10.4. 集合通讯
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">
           10.5. 参数服务器
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_distributed_training/summary.html">
           10.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_extension/index.html">
         11. 第三部分：拓展篇
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_recommender_system/index.html">
         12. 深度学习推荐系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/overview.html">
           12.1. 背景
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/system_architecture.html">
           12.2. 主流系统架构
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/system_problem.html">
           12.3. 现有解决方案及其存在的问题
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/future.html">
           12.4. 未来可以探索的方向
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/summary.html">
           12.5. 小结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_federated_learning/index.html">
         13. 联邦学习系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/overview.html">
           13.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/system_architecture.html">
           13.2. 系统架构
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/fedavg.html">
           13.3. 联邦平均算法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">
           13.4. 隐私加密算法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/challenge.html">
           13.5. 实际部署时的挑战
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">
           13.6. 纵向联邦学习
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/summary.html">
           13.7. 小结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
         14. 强化学习系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">
           14.1. 强化学习介绍
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">
           14.2. 单节点强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">
           14.3. 分布式强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/marl.html">
           14.4. 多智能体强化学习
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">
           14.5. 多智能体强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/summary.html">
           14.6. 小结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_explainable_AI/index.html">
         15. 可解释性AI系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">
           15.1. 背景
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">
           15.2. 可解释AI定义
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">
           15.3. 可解释AI算法现状介绍
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">
           15.4. 未来可解释AI
          </a>
         </li>
        </ul>
       </li>
      </ul>
      <ul>
       <li class="toctree-l1">
        <a class="reference internal" href="../appendix_machine_learning_introduction/index.html">
         附录：机器学习介绍
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">
           1. 神经网络
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">
           2. 梯度下降与反向传播
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">
           3. 经典机器学习方法
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_references/index.html">
         参考文献
        </a>
       </li>
      </ul>
     </nav>
    </div>
   </header>
   <main class="mdl-layout__content" tabindex="0">
    <script src="../_static/sphinx_materialdesign_theme.js " type="text/javascript">
    </script>
    <header class="mdl-layout__drawer">
     <!-- Title -->
     <span class="mdl-layout-title">
      <a class="title" href="../index.html">
       <span class="title-text">
        机器学习系统：设计和实现
       </span>
      </a>
     </span>
     <div class="globaltoc">
      <span class="mdl-layout-title toc">
       Table Of Contents
      </span>
      <nav class="mdl-navigation">
       <ul class="current">
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_introduction/index.html">
          1. 导论
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">
            1.1. 机器学习应用
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">
            1.2. 机器学习系统的需求
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">
            1.3. 机器学习系统基本组成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/applicable_readers.html">
            1.4. 适用读者
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_programming_interface/index.html">
          2. 编程接口
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/development_history.html">
            2.1. 机器学习系统编程模型的演进
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">
            2.2. 机器学习工作流
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">
            2.3. 定义深度神经网络
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">
            2.4. C/C++编程接口
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/summary.html">
            2.5. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1 current">
         <a class="reference internal" href="index.html">
          3. 计算图
         </a>
         <ul class="current">
          <li class="toctree-l2">
           <a class="reference internal" href="background_and_functionality.html">
            3.1. 计算图的设计背景和作用
           </a>
          </li>
          <li class="toctree-l2 current">
           <a class="current reference internal" href="#">
            3.2. 计算图的基本构成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="generation_of_computational_graph.html">
            3.3. 计算图的生成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="schedule_of_computational_graph.html">
            3.4. 计算图的调度
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="summary.html">
            3.5. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_advanced/index.html">
          4. 第二部分：进阶篇
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_frontend_and_ir/index.html">
          5. 编译器前端
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">
            5.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">
            5.2. 中间表示
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/ad.html">
            5.3. 自动微分
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">
            5.4. 类型系统和静态分析
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">
            5.5. 常见前端编译优化方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/summary.html">
            5.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_backend_and_runtime/index.html">
          6. 编译器后端和运行时
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/overview.html">
            6.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">
            6.2. 计算图优化
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">
            6.3. 算子选择
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">
            6.4. 内存分配
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">
            6.5. 计算调度与执行
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/summary.html">
            6.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_accelerator/index.html">
          7. 硬件加速器
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">
            7.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">
            7.2. 加速器基本组成原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">
            7.3. 加速器基本编程原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/summary.html">
            7.4. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_data_processing/index.html">
          8. 数据处理框架
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/requirements.html">
            8.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/program_model.html">
            8.2. 易用性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/performance.html">
            8.3. 高效性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/data_order.html">
            8.4. 保序性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/extension.html">
            8.5. 单机数据处理性能的扩展
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/summary.html">
            8.6. 章节总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_model_deployment/index.html">
          9. 模型部署
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">
            9.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">
            9.2. 训练模型到推理模型的转换及优化
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_compression.html">
            9.3. 模型压缩
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_inference.html">
            9.4. 模型推理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_security.html">
            9.5. 模型的安全保护
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/summary.html">
            9.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_distributed_training/index.html">
          10. 分布式训练
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed_training/overview.html">
            10.1. 系统概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed_training/methods.html">
            10.2. 分布式方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed_training/pipeline.html">
            10.3. 流水线并行
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed_training/collective.html">
            10.4. 集合通讯
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">
            10.5. 参数服务器
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_distributed_training/summary.html">
            10.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_extension/index.html">
          11. 第三部分：拓展篇
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_recommender_system/index.html">
          12. 深度学习推荐系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/overview.html">
            12.1. 背景
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/system_architecture.html">
            12.2. 主流系统架构
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/system_problem.html">
            12.3. 现有解决方案及其存在的问题
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/future.html">
            12.4. 未来可以探索的方向
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/summary.html">
            12.5. 小结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_federated_learning/index.html">
          13. 联邦学习系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/overview.html">
            13.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/system_architecture.html">
            13.2. 系统架构
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/fedavg.html">
            13.3. 联邦平均算法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">
            13.4. 隐私加密算法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/challenge.html">
            13.5. 实际部署时的挑战
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">
            13.6. 纵向联邦学习
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/summary.html">
            13.7. 小结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
          14. 强化学习系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">
            14.1. 强化学习介绍
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">
            14.2. 单节点强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">
            14.3. 分布式强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/marl.html">
            14.4. 多智能体强化学习
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">
            14.5. 多智能体强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/summary.html">
            14.6. 小结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_explainable_AI/index.html">
          15. 可解释性AI系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">
            15.1. 背景
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">
            15.2. 可解释AI定义
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">
            15.3. 可解释AI算法现状介绍
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">
            15.4. 未来可解释AI
           </a>
          </li>
         </ul>
        </li>
       </ul>
       <ul>
        <li class="toctree-l1">
         <a class="reference internal" href="../appendix_machine_learning_introduction/index.html">
          附录：机器学习介绍
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">
            1. 神经网络
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">
            2. 梯度下降与反向传播
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">
            3. 经典机器学习方法
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_references/index.html">
          参考文献
         </a>
        </li>
       </ul>
      </nav>
     </div>
    </header>
    <div class="document">
     <div class="page-content" role="main">
      <section id="id1">
       <h1>
        <span class="section-number">
         3.2.
        </span>
        计算图的基本构成
        <a class="headerlink" href="#id1" title="Permalink to this headline">
         ¶
        </a>
       </h1>
       <p>
        计算图是用来表示深度学习网络模型在训练与推理过程中计算逻辑与状态的工具。计算框架在后端会将前端语言构建的神经网络模型前向计算与反向梯度计算以计算图的形式来进行表示。计算图由基本数据结构张量(Tensor)和基本运算单元算子(Operator)构成。在计算图中通常使用节点来表示算子，节点间的有向线段来表示张量状态，同时也描述了计算间的依赖关系。如
        <a class="reference internal" href="#simpledag">
         <span class="std std-numref">
          图3.2.1
         </span>
        </a>
        所示，将
        <span class="math notranslate nohighlight">
         \(\boldsymbol{Z}=relu(\boldsymbol{X}*\boldsymbol{Y})\)
        </span>
        转化为计算图表示，数据流将根据图中流向与算子进行前向计算和反向梯度计算来更新图中张量状态，以此达到训练模型的目的。
       </p>
       <figure class="align-default" id="id6">
        <span id="simpledag">
        </span>
        <a class="reference internal image-reference" href="../_images/simpledag.svg">
         <img alt="../_images/simpledag.svg" src="../_images/simpledag.svg" width="300px"/>
        </a>
        <figcaption>
         <p>
          <span class="caption-number">
           图3.2.1
          </span>
          <span class="caption-text">
           简单计算图
          </span>
          <a class="headerlink" href="#id6" title="Permalink to this image">
           ¶
          </a>
         </p>
        </figcaption>
       </figure>
       <section id="id2">
        <h2>
         <span class="section-number">
          3.2.1.
         </span>
         张量和算子
         <a class="headerlink" href="#id2" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <p>
         在计算框架中，基础组件包含张量和算子，张量是基础数据结构，算子是基本运算单元。在数学中定义张量是基于向量与矩阵的推广，涵盖标量、向量与矩阵的概念。可以将标量理解为零阶张量，向量为一阶张量，我们熟悉的RGB彩色图像即为三阶张量。在计算框架中张量不仅存储数据，还存储数据类型、数据形状、维度或秩以及梯度传递状态等多个属性，如表3.2.1所示，列举了主要的属性和功能。
        </p>
        <span id="tensor-attr">
        </span>
        <table class="docutils align-default" id="id7" style="margin-left:auto;margin-right:auto;margin-top:10px;margin-bottom:20px;">
         <caption>
          <span class="caption-number">
           表3.2.1
          </span>
          <span class="caption-text">
           张量属性
          </span>
          <a class="headerlink" href="#id7" title="Permalink to this table">
           ¶
          </a>
         </caption>
         <colgroup>
          <col style="width: 26%"/>
          <col style="width: 74%"/>
         </colgroup>
         <thead>
          <tr class="row-odd">
           <th class="head">
            <p>
             张量属性
            </p>
           </th>
           <th class="head">
            <p>
             功能
            </p>
           </th>
          </tr>
         </thead>
         <tbody>
          <tr class="row-even">
           <td>
            <p>
             形状(shape)
            </p>
           </td>
           <td>
            <p>
             存储张量的每个维度的长度，如[3,3,3]
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             维度或秩(dim)
            </p>
           </td>
           <td>
            <p>
             表示张量维度的数量，标量为0，向量为1、矩阵为2
            </p>
           </td>
          </tr>
          <tr class="row-even">
           <td>
            <p>
             数据类型(dtype)
            </p>
           </td>
           <td>
            <p>
             表示存储的数
据类型，如bool、int8、int16、float32、float64等
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             存储位置(device)
            </p>
           </td>
           <td>
            <p>
             创建张量时可以指定存储的设备位置，如CPU、GPU等
            </p>
           </td>
          </tr>
          <tr class="row-even">
           <td>
            <p>
             名字(name)
            </p>
           </td>
           <td>
            <p>
             张量的标识符
            </p>
           </td>
          </tr>
         </tbody>
        </table>
        <p>
         张量的形状是一个重要的属性，它记录了每个轴的长度，也就是张量每个维度的元素数量。秩则代表张量的轴数或者阶数。张量中通常可以保存布尔类型、浮点数、整型数以及复数和字符串数据。每一个张量都具有唯一的数据类型，在计算过程中会对所有参与运算的张量进行类型检查，当发现类型不匹配时就会报错。部分特殊的计算则必须使用指定的数据类型，比如逻辑运算应为布尔类型。在部分计算框架中张量的属性中包含可以指明张量存储的设备位置，比如存储于CPU、GPU等。张量数据的存储状态可以分为可变和不可变两种，不可变张量一般用于用户初始化的数据或者网络模型输入的数据；而可变张量则存储网络权重参数，根据梯度信息更新自身数据。
        </p>
        <p>
         如
         <a class="reference internal" href="#tensor">
          <span class="std std-numref">
           图3.2.2
          </span>
         </a>
         ，标量就是一个零阶张量，包含单个数值但没有轴信息。向量即为一阶张量，具有一个轴。二阶张量具有两个轴即秩为二。
        </p>
        <figure class="align-default" id="id8">
         <span id="tensor">
         </span>
         <a class="reference internal image-reference" href="../_images/tensor.svg">
          <img alt="../_images/tensor.svg" src="../_images/tensor.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.2
           </span>
           <span class="caption-text">
            张量
           </span>
           <a class="headerlink" href="#id8" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         通常我们使用的张量是”整齐”的，每个轴上的具有相同的元素个数，就像一个”矩形”或者”立方体”。在特定的环境中，也会使用特殊类型的张量，比如不规则张量和稀疏张量，如
         <a class="reference internal" href="#tensorclass">
          <span class="std std-numref">
           图3.2.3
          </span>
         </a>
         中所示。不规则张量在某个轴上可能具有不同的元素个数，它们支持存储和处理包含非均匀形状的数据，在自然语言处理领域，不规则张量可以存储不同长度文本的信息。稀疏张量则通常应用于图数据与图神经网络中，采用特殊的存储格式如坐标表格式（Coordinate
List， COO），可以高效存储稀疏数据，节省存储空间。
        </p>
        <figure class="align-default" id="id9">
         <span id="tensorclass">
         </span>
         <a class="reference internal image-reference" href="../_images/tensorclass.svg">
          <img alt="../_images/tensorclass.svg" src="../_images/tensorclass.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.3
           </span>
           <span class="caption-text">
            张量分类
           </span>
           <a class="headerlink" href="#id9" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         算子是构成神经网络的基本计算单元。算子按照功能可以分为张量操作、神经网络操作、数据流操作和控制流操作等。
        </p>
        <ul class="simple">
         <li>
          <p>
           <strong>
            张量操作
           </strong>
           ：包括张量的结构操作和张量的数学运算。张量结构操作有：张量创建、索引切片、维度变换和合并分割等。张量的数学运算包含标量运算、向量运算和矩阵运算。标量运算符的特点是对张量实施逐元素运算。向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。矩阵运算包括矩阵乘法、矩阵范数、矩阵行列式、矩阵求特征值、矩阵分解等运算。
          </p>
         </li>
         <li>
          <p>
           <strong>
            神经网络操作
           </strong>
           ：包括特征提取、激活函数、损失函数、优化算法等。特征提取是机器学习中的常见操作，核心是提取比原输入更具代表性的张量，常见的卷积操作就是特征提取算子。激活函数(Activation
Function)负责将神经网络层的输入映射到输出端。引入激活函数是为了增加神经网络模型的非线性，没有激活函数的每层都相当于矩阵相乘。常见的激活函数包括S型生长曲线(Sigmoid)、线性矫正单元(Rectified
Linear Unit, ReLU)等。损失函数(Loss
Function)是用来估量模型的预测值与真实值之间的不一致程度。优化算法基于梯度采用不同策略更新参数权值来最小化损失函数，常见的优化算法有随机梯度下降法(Stochastic
Gradient Descent, SGD)、自适应矩估计(Adaptive Moment Estimation,
Adam)等。
          </p>
         </li>
         <li>
          <p>
           <strong>
            数据流操作
           </strong>
           ：包含数据的预处理与数据载入相关算子，数据预处理算子主要是针对图像数据和文本数据的裁剪填充、归一化、数据增强等操作。数据载入通常会对数据集进行随机乱序(Shuffle)、分批次载入(Batch)以及预载入(Prefetch)等操作。数据流操作主要功能是对原始数据进行处理后，转换为计算框架本身支持的数据格式，并且按照迭代次数输入给网络进行训练或者推理，提升数据载入速度，减少内存占用空间，降低网络训练等待时间。
          </p>
         </li>
         <li>
          <p>
           <strong>
            控制流操作
           </strong>
           ：可以控制计算图中的数据流向，当表示灵活复杂的模型时需要控制流。使用频率比较高的控制流算子有条件运算符和循环运算符。控制流操作一般分为两类，计算框架本身提供的控制流操作符和前端语言控制流操作符。控制流操作不仅会影响神经网络模型前向运算的数据流向，也会影响反向梯度运算的数据流向。
          </p>
         </li>
        </ul>
       </section>
       <section id="id3">
        <h2>
         <span class="section-number">
          3.2.2.
         </span>
         计算依赖
         <a class="headerlink" href="#id3" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <p>
         在计算图中，算子之间存在依赖关系，而这种依赖关系影响了算子的执行顺序与并行情况。此外在深度学习算法模型中，计算图是一个有向无环图，也即在计算图中造成循环依赖的数据流向是不被允许的。为了理解计算依赖关系并且分析计算图中循环与循环依赖之间的区别，下面将对计算图中的计算节点依赖关系进行讲解。
        </p>
        <figure class="align-default" id="id10">
         <span id="dependence">
         </span>
         <a class="reference internal image-reference" href="../_images/dependence.svg">
          <img alt="../_images/dependence.svg" src="../_images/dependence.svg" width="400px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.4
           </span>
           <span class="caption-text">
            计算依赖
           </span>
           <a class="headerlink" href="#id10" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         如
         <a class="reference internal" href="#dependence">
          <span class="std std-numref">
           图3.2.4
          </span>
         </a>
         中所示，在此简单的计算图中，若将
         <span class="math notranslate nohighlight">
          \(\mathbf{Matmul1}\)
         </span>
         算子移除则该节点无输出，导致后续的激活函数无法得到输入，从而计算图中的数据流动中断，这表明计算图中的算子间具有依赖关系并且存在传递性。我们对依赖关系进行区分如下：
        </p>
        <ul class="simple">
         <li>
          <p>
           <strong>
            直接依赖
           </strong>
           ：节点
           <span class="math notranslate nohighlight">
            \(\mathbf{ReLU1}\)
           </span>
           直接依赖于节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Matmul1}\)
           </span>
           ，即如果节点
           <span class="math notranslate nohighlight">
            \(\mathbf{ReLU1}\)
           </span>
           要执行运算，必须接受直接来自节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Matmul1}\)
           </span>
           的输出数据；
          </p>
         </li>
         <li>
          <p>
           <strong>
            间接依赖
           </strong>
           ：节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Add}\)
           </span>
           间接依赖于节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Matmul1}\)
           </span>
           ，即节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Matmul1}\)
           </span>
           的数据并未直接传输给节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Add}\)
           </span>
           ，而是经过了某个或者某些中间节点进行处理后再传输给节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Add}\)
           </span>
           ，而这些中间节点可能是节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Add}\)
           </span>
           的直接依赖节点，也可能是间接依赖节点；
          </p>
         </li>
         <li>
          <p>
           <strong>
            相互独立
           </strong>
           ：在计算图中节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Matmul1}\)
           </span>
           与节点
           <span class="math notranslate nohighlight">
            \(\mathbf{Matmul2}\)
           </span>
           之间并无数据输入输出依赖关系，所以这两个节点间相互独立。
          </p>
         </li>
        </ul>
        <p>
         掌握依赖关系后，分析
         <a class="reference internal" href="#recurrent">
          <span class="std std-numref">
           图3.2.5
          </span>
         </a>
         可以得出节点
         <span class="math notranslate nohighlight">
          \(\mathbf{Add}\)
         </span>
         间接依赖于节点
         <span class="math notranslate nohighlight">
          \(\mathbf{Matmul}\)
         </span>
         ，而节点
         <span class="math notranslate nohighlight">
          \(\mathbf{Matmul}\)
         </span>
         直接依赖于节点
         <span class="math notranslate nohighlight">
          \(\mathbf{Add}\)
         </span>
         ，此时两个节点互相等待对方计算完成输出数据，将无法执行计算任务。若我们手动同时给两个节点赋予输入，计算将持续不间断进行，模型训练将无法停止造成死循环。循环依赖产生正反馈数据流，被传递的数值可能在正方向上无限放大，导致数值上溢，或者负方向上放大导致数值下溢，也可能导致数值无限逼近于0，这些情况都会致使模型训练无法得到预期结果。在构建深度学习模型时，应避免算子间产生循环依赖。
        </p>
        <figure class="align-default" id="id11">
         <span id="recurrent">
         </span>
         <a class="reference internal image-reference" href="../_images/recurrent.svg">
          <img alt="../_images/recurrent.svg" src="../_images/recurrent.svg" width="300px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.5
           </span>
           <span class="caption-text">
            循环依赖
           </span>
           <a class="headerlink" href="#id11" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         在深度学习计算框架中，表示循环关系通常是以
         <strong>
          展开
         </strong>
         机制（Unrolling）来实现。当需要实现循环关系时，循环体的计算子图按照迭代次数进行复制，将代表相邻迭代轮次的子图进行串联，相邻迭代轮次的计算子图之间就是直接依赖关系。循环三次的计算图进行展开如
         <a class="reference internal" href="#unroll">
          <span class="std std-numref">
           图3.2.6
          </span>
         </a>
         。在计算图中，每一个张量和运算符都具有独特的标识符，即使是相同的操作运算，在参与不同计算任务时都具有不同的标识符。区分循环关系和循环依赖的关键在于，是否两个独特标识符之间的运算互相具有直接依赖和相互依赖。循环关系在展开复制计算子图的时候会给复制的所有张量和运算符赋予新的标识符，区分被复制的原始子图，以避免形成循环依赖。
        </p>
        <figure class="align-default" id="id12">
         <span id="unroll">
         </span>
         <a class="reference internal image-reference" href="../_images/unroll.svg">
          <img alt="../_images/unroll.svg" src="../_images/unroll.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.6
           </span>
           <span class="caption-text">
            循环展开
           </span>
           <a class="headerlink" href="#id12" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
       </section>
       <section id="id4">
        <h2>
         <span class="section-number">
          3.2.3.
         </span>
         控制流
         <a class="headerlink" href="#id4" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <p>
         控制流能够设定特定的顺序执行计算任务。若计算图中无控制流，则每个节点只执行一次，当所有节点按照顺序执行完时，计算图即完成计算。加入控制流后可以让计算图中某些节点循环执行任意次数，也可以根据条件判断选择某些节点不执行，控制流使得我们可以构建更加灵活和复杂的模型。许多机器学习模型依赖控制流进行训练和推理，特别是基于递归神经网络和强化学习的模型就依赖于循环递归关系和依据数据的条件执行。
        </p>
        <p>
         为了提高性能、可扩展性和表达能力，计算框架必须支持控制流。目前主流的计算框架中通常使用两种方式来提供控制流：
        </p>
        <ul class="simple">
         <li>
          <p>
           <strong>
            计算框架控制原语
           </strong>
           ：计算框架在内部设计了低级别细粒度的控制原语运算符，通过原语运算符的结合使用来实现控制流，这种实现方式也被称为图内方法（In-graph
approach）。此类方法的代表就是TensorFlow中的Switch、Merge、Enter、Exit、NextIteration五个原语。TensorFlow通过组合五个原语提供
           <em>
            tf.cond()
           </em>
           和
           <em>
            tf.while_loop()
           </em>
           来实现条件控制和循环控制。
          </p>
         </li>
         <li>
          <p>
           <strong>
            前端语言控制流
           </strong>
           ：通过高级语言Python、C++的控制流语句来进行计算图中的控制决策，这类实现方式也被称为图外方法（Out-of-graph
approach）。计算框架PyTorch、MindSpore中就直接使用Python的控制流，将控制流和数据流之间保持了严格的分离。
          </p>
         </li>
        </ul>
        <p>
         图内方法控制流采用框架原语实现，在进行模型编译、优化与运行时都具备优势，并且可以准确的判定机器学习模型中计算梯度时需要缓存的变量，提高运行效率，同时由于不依赖外部语言便于部署到不同环境中去。但由于控制原语缺乏进一步的抽象，对于用户不友好，需要掌握控制原语的使用方法，结合前端语言使用才能描述复杂模型结构。
        </p>
        <p>
         相对于图内方法，图外方法直接使用前端语言控制流则相对更加灵活易用，用户编写模型控制时更加便捷直观，其缺点在于若要将模型进行优化部署，则需要在编译阶段将前端语言的控制流转化为框架原语描述。
        </p>
        <p>
         目前在主流的深度学习计算框架中，均提供图外方法和图内方法支持。为了便于理解控制流对前向计算与反向计算的影响，后续的讲解均使用
         <strong>
          图外方法
         </strong>
         实现控制流。常见的控制流包括条件分支与循环两种。当模型包含控制流操作时，梯度在反向传播经过控制流时，需要在反向梯度计算图中也构造生成相应的控制流，才能够正确计算参与运算的张量梯度。
        </p>
        <p>
         下面这段代码描述了简单的条件控制，我们使用
         <em>
          matmul
         </em>
         表示矩阵乘法算子：
        </p>
        <div class="highlight-python notranslate">
         <div class="highlight">
          <pre><span></span><span class="k">def</span> <span class="nf">control</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">conditional</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">conditional</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre>
         </div>
        </div>
        <figure class="align-default" id="id13">
         <span id="if">
         </span>
         <a class="reference internal image-reference" href="../_images/if.svg">
          <img alt="../_images/if.svg" src="../_images/if.svg" width="600px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.7
           </span>
           <span class="caption-text">
            条件控制计算图
           </span>
           <a class="headerlink" href="#id13" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         <a class="reference internal" href="#if">
          <span class="std std-numref">
           图3.2.7
          </span>
         </a>
         描述上述代码的前向计算图和反向计算图。对于具有if-条件的模型，梯度计算需要知道采用了条件的哪个分支，然后将梯度逻辑应用于该分支。在前向计算图中张量
         <span class="math notranslate nohighlight">
          \({C}\)
         </span>
         经过条件控制不参与计算，在反向计算时同样遵守控制流决策，不会计算关于张量
         <span class="math notranslate nohighlight">
          \(C\)
         </span>
         的梯度。
        </p>
        <p>
         当模型中有循环控制时，循环中的操作可以执行零次或者多次。此时采用展开机制，对每一次操作都赋予独特的运算标识符，以此来区分相同运算操作的多次调用。每一次循环都直接依赖于前一次循环的计算结果，所以在循环控制中需要维护一个张量列表，将循环迭代的中间结果缓存起来，这些中间结果将参与前向计算和梯度计算。下面这段代码描述了简单的循环控制，将其展开得到等价代码后，可以清楚的理解需要维护张量
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y_i}\)
         </span>
         和
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W_i}\)
         </span>
         的列表。
        </p>
        <div class="highlight-python notranslate">
         <div class="highlight">
          <pre><span></span><span class="k">def</span> <span class="nf">recurrent_control</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">cur_num</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cur_num</span><span class="p">):</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span>
<span class="c1">#利用展开机制将上述代码展开，可得到等价表示</span>
<span class="k">def</span> <span class="nf">recurrent_control</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">cur_num</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span>
</pre>
         </div>
        </div>
        <p>
         如
         <a class="reference internal" href="#while">
          <span class="std std-numref">
           图3.2.8
          </span>
         </a>
         描述了上述代码的前向计算图和反向计算图，循环控制的梯度同样也是一个循环，它与前向循环相迭代次数相同，执行循环体的梯度计算。循环体输出的梯度值作为下一次梯度计算的初始值，直至循环结束。
        </p>
        <figure class="align-default" id="id14">
         <span id="while">
         </span>
         <a class="reference internal image-reference" href="../_images/while.svg">
          <img alt="../_images/while.svg" src="../_images/while.svg" width="600px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.8
           </span>
           <span class="caption-text">
            循环控制计算图
           </span>
           <a class="headerlink" href="#id14" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
       </section>
       <section id="id5">
        <h2>
         <span class="section-number">
          3.2.4.
         </span>
         基于链式法则计算梯度
         <a class="headerlink" href="#id5" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <p>
         在上一小节循环展开的例子中，当神经网络接收输入张量
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}\)
         </span>
         后，输入数据根据计算图逐层进行计算并保存中间结果变量，直至经过多层的计算后最终产生输出
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y_3}\)
         </span>
         ，这个过程我们称之为
         <strong>
          前向传播
         </strong>
         （Forward
propagation）。在深度神经网络模型训练过程中，前向传播的输出结果与标签值可以产生一个损失函数结果。模型将来自损失函数的数据信息通过计算图反向流动，执行梯度计算来进行更新训练参数，这个过程我们称之为
         <strong>
          反向传播
         </strong>
         （Back
propagation）。在神经网络模型中，反向传播通常使用损失函数关于参数的梯度来进行更新，也可以使用其他信息进行反向传播，在这里我们仅讨论一般情况。
        </p>
        <p>
         在这里我们简单回忆一下复合函数的链式法则公式。链式法则是微积分中的求导法则，用于求解复合函数中的导数。复合函数的导数是构成复合有限个函数在相应点的导数乘积。假设
         <em>
          f
         </em>
         和
         <em>
          g
         </em>
         是关于实数
         <em>
          x
         </em>
         的映射函数，设
         <span class="math notranslate nohighlight">
          \(y=g(x)\)
         </span>
         并且
         <span class="math notranslate nohighlight">
          \(z=f(y)=f(g(x))\)
         </span>
         ，则
         <em>
          z
         </em>
         对
         <em>
          x
         </em>
         的导数即为：
        </p>
        <div class="math notranslate nohighlight" id="equation-chapter-computational-graph-components-of-computational-graph-0">
         <span class="eqno">
          (3.2.1)
          <a class="headerlink" href="#equation-chapter-computational-graph-components-of-computational-graph-0" title="Permalink to this equation">
           ¶
          </a>
         </span>
         \[\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}\]
        </div>
        <p>
         神经网络的反向传播是根据反向计算图的特定运算顺序来执行链式法则的算法。由于神经网络的输入通常为三维张量，输出为一维向量。因此将上述复合函数关于标量的梯度法则进行推广和扩展。假设
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         是
         <em>
          m
         </em>
         维张量，
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}\)
         </span>
         为
         <em>
          n
         </em>
         维张量，
         <span class="math notranslate nohighlight">
          \(\boldsymbol{z}\)
         </span>
         为一维向量，
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}=g(\boldsymbol{X})\)
         </span>
         并且
         <span class="math notranslate nohighlight">
          \(\boldsymbol{z}=f(\boldsymbol{Y})\)
         </span>
         ，则
         <span class="math notranslate nohighlight">
          \(\boldsymbol{z}\)
         </span>
         关于
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         每一个元素的偏导数即为：
        </p>
        <div class="math notranslate nohighlight" id="equation-chapter-computational-graph-components-of-computational-graph-1">
         <span class="eqno">
          (3.2.2)
          <a class="headerlink" href="#equation-chapter-computational-graph-components-of-computational-graph-1" title="Permalink to this equation">
           ¶
          </a>
         </span>
         \[\frac{\partial z}{\partial x_i}=\sum_j\frac{\partial z}{\partial y_j}\frac{\partial y_j}{\partial x_i}\]
        </div>
        <p>
         上述公式可以等价的表示为：
        </p>
        <div class="math notranslate nohighlight" id="equation-chapter-computational-graph-components-of-computational-graph-2">
         <span class="eqno">
          (3.2.3)
          <a class="headerlink" href="#equation-chapter-computational-graph-components-of-computational-graph-2" title="Permalink to this equation">
           ¶
          </a>
         </span>
         \[\nabla_{\boldsymbol{X}}\boldsymbol{z} = (\frac{\partial \boldsymbol{Y}}{\partial \boldsymbol{X}})^{\top}\nabla_{\boldsymbol{Y}}\boldsymbol{z}\]
        </div>
        <p>
         其中
         <span class="math notranslate nohighlight">
          \(\nabla_{\boldsymbol{X}}\boldsymbol{z}\)
         </span>
         表示
         <span class="math notranslate nohighlight">
          \(\boldsymbol{z}\)
         </span>
         关于
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X}\)
         </span>
         的梯度矩阵。
        </p>
        <p>
         上一小节中简单的循环控制模型前向传播可以表示为
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}=\boldsymbol{W_2}(\boldsymbol{W_1}(\boldsymbol{W}(\boldsymbol{X})))\)
         </span>
         。在反向传播的过程中可以将前向计算等价为
         <span class="math notranslate nohighlight">
          \(\boldsymbol{Y}=\boldsymbol{W_2}\boldsymbol{X_2}\)
         </span>
         ，首先得到参数
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W_2}\)
         </span>
         的梯度表示。再接着根据
         <span class="math notranslate nohighlight">
          \(\boldsymbol{X_2}=\boldsymbol{W_1}\boldsymbol{X_1}\)
         </span>
         得到
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W_1}\)
         </span>
         的梯度表示，按照层级即可推导得出
         <span class="math notranslate nohighlight">
          \(\boldsymbol{W}\)
         </span>
         的梯度表示。
        </p>
        <div class="math notranslate nohighlight" id="equation-chapter-computational-graph-components-of-computational-graph-3">
         <span class="eqno">
          (3.2.4)
          <a class="headerlink" href="#equation-chapter-computational-graph-components-of-computational-graph-3" title="Permalink to this equation">
           ¶
          </a>
         </span>
         \[\begin{split}\begin{aligned}
\nabla\boldsymbol{X_2} &amp;= \nabla\boldsymbol{Y}\boldsymbol{W_2}^\top  \\
\nabla\boldsymbol{W_2} &amp;= \boldsymbol{X_2}^\top\nabla\boldsymbol{Y}   \\
\nabla\boldsymbol{X_1} &amp;= \nabla\boldsymbol{X_2}\boldsymbol{W_1}^\top = (\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)\boldsymbol{W_1}^\top   \\
\nabla\boldsymbol{W_1} &amp;= \boldsymbol{X_1}^\top\nabla\boldsymbol{X_2} = \boldsymbol{X_1}^\top(\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)  \\
\nabla\boldsymbol{X} &amp;= \nabla\boldsymbol{X_1}\boldsymbol{W}^\top = ((\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)\boldsymbol{W_1}^\top)\boldsymbol{W}^\top   \\
\nabla\boldsymbol{W} &amp;= \boldsymbol{X}^\top\nabla\boldsymbol{X_1} = \boldsymbol{X}^\top((\nabla\boldsymbol{Y}\boldsymbol{W_2}^\top)\boldsymbol{W_1}^\top)
\end{aligned}\end{split}\]
        </div>
        <p>
         根据链式法则，相应位置的导数乘积即可将网络得到的损失函数梯度信息传播到每一个权重参数，应用优化器的参数权重更新规则，即可达到神经网络模型参数训练迭代的目的。
        </p>
        <p>
         根据上述公式我们可以得出循环控制的反向梯度计算过程如下，在下面代码中伪变量的前缀
         <em>
          grad
         </em>
         代表变量梯度变量，
         <em>
          transpose
         </em>
         代表矩阵转置算子。
        </p>
        <div class="highlight-python notranslate">
         <div class="highlight">
          <pre><span></span><span class="n">grad_X2</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">grad_Y</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="n">W2</span><span class="p">))</span>
<span class="n">grad_W2</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X2</span><span class="p">),</span> <span class="n">grad_Y</span><span class="p">)</span>
<span class="n">grad_X1</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">grad_X2</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="n">W1</span><span class="p">))</span>
<span class="n">grad_W1</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X1</span><span class="p">),</span> <span class="n">grad_X2</span><span class="p">)</span>
<span class="n">grad_X</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">grad_X1</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
<span class="n">grad_W</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">grad_X1</span><span class="p">)</span>
</pre>
         </div>
        </div>
        <p>
         结合公式、代码以及
         <a class="reference internal" href="#chain">
          <span class="std std-numref">
           图3.2.9
          </span>
         </a>
         我们可以看出，在反向传播过程中使用到前向传播的中间变量。因此保存网络中间层输出状态和中间变量，尽管占用了部分内存但能够复用计算结果，达到了提高反向传播计算效率的目的。
        </p>
        <figure class="align-default" id="id15">
         <span id="chain">
         </span>
         <a class="reference internal image-reference" href="../_images/chain.svg">
          <img alt="../_images/chain.svg" src="../_images/chain.svg" width="600px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图3.2.9
           </span>
           <span class="caption-text">
            反向传播局部计算图
           </span>
           <a class="headerlink" href="#id15" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         在深度学习计算框架中，控制流可以进行嵌套，比如多重循环和循环条件控制，计算图会对复杂控制流进行准确的描述，以便于执行正确的计算调度与执行任务。
        </p>
       </section>
      </section>
     </div>
     <div class="side-doc-outline">
      <div class="side-doc-outline--content">
       <div class="localtoc">
        <p class="caption">
         <span class="caption-text">
          Table Of Contents
         </span>
        </p>
        <ul>
         <li>
          <a class="reference internal" href="#">
           3.2. 计算图的基本构成
          </a>
          <ul>
           <li>
            <a class="reference internal" href="#id2">
             3.2.1. 张量和算子
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id3">
             3.2.2. 计算依赖
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id4">
             3.2.3. 控制流
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id5">
             3.2.4. 基于链式法则计算梯度
            </a>
           </li>
          </ul>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <div class="clearer">
     </div>
    </div>
    <div class="pagenation">
     <a accesskey="P" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="background_and_functionality.html" id="button-prev" role="botton">
      <i class="pagenation-arrow-L fas fa-arrow-left fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Previous
       </span>
       <div>
        3.1. 计算图的设计背景和作用
       </div>
      </div>
     </a>
     <a accesskey="N" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="generation_of_computational_graph.html" id="button-next" role="botton">
      <i class="pagenation-arrow-R fas fa-arrow-right fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Next
       </span>
       <div>
        3.3. 计算图的生成
       </div>
      </div>
     </a>
    </div>
   </main>
  </div>
 </body>
</html>