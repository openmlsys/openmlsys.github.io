<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>10.4. 模型推理 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.5. 模型的安全保护" href="model_security.html" />
    <link rel="prev" title="10.3. 模型压缩" href="model_compression.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">10. </span>模型部署</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">10.4. </span>模型推理</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_model_deployment/model_inference.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 序言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">2.2. 设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">2.3. 基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">2.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">8. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">8.4. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id2">8.5. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">10. 模型部署</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">11. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">11.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">11.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">11.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">11.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id2">11.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">13.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">13.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">13.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">16.4. 未来可解释AI</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rl_sys/index.html">17. 机器人系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">17.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/perception.html">17.3. 感知系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/planning.html">17.4. 规划系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/control.html">17.5. 控制系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/summary.html">17.6. 小结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 序言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">2.2. 设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">2.3. 基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">2.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">8. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">8.4. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id2">8.5. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">10. 模型部署</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">11. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">11.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">11.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">11.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">11.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id2">11.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">13.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">13.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">13.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">16.4. 未来可解释AI</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rl_sys/index.html">17. 机器人系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">17.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/perception.html">17.3. 感知系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/planning.html">17.4. 规划系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/control.html">17.5. 控制系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/summary.html">17.6. 小结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">10.4. </span>模型推理<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>训练模型经过前面的转换、压缩等流程后，需要部署在计算硬件上进行推理。执行推理主要包含以下步骤：</p>
<ul class="simple">
<li><p>前处理：将原始数据处理成适合网络输入的数据。</p></li>
<li><p>执行推理：将离线转换得到的模型部署到设备上执行推理流程，根据输入数据计算得到输出数据。</p></li>
<li><p>后处理：模型的输出结果做进一步的加工处理，如筛选阈值。</p></li>
</ul>
<div class="section" id="id2">
<h2><span class="section-number">10.4.1. </span>前处理与后处理<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3><span class="section-number">10.4.1.1. </span>前处理<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>前处理主要完成数据预处理，在现实问题中，我们得到的原始数据往往非常混乱，机器学习模型无法识别并从中提取信息。数据预处理的目的是将原始数据例如图片、语音、文本等，处理成适合网络输入的tensor数据，并消除其中无关的信息，恢复有用的真实信息，增强有关信息的可检测性，最大限度地简化数据，从而改进模型的特征抽取、图像分割、匹配和识别等可靠性。</p>
<p>常见的数据预处理手段有：</p>
<ul class="simple">
<li><p>特征编码：将描述特征的原始数据编码成数字，输入给机器学习模型，因为它们只能处理数字数据。常见的编码方法有：离散化、序号编码、One-hot编码，二进制编码等；</p></li>
<li><p>数据归一化：修改数据的值使其达到共同的标度但不改变它们之间的相关性，消除数据指标之间的量纲影响。常用的技术有：Min-Max归一化将数据缩放到给定范围，Z-score归一化使数据符合正态分布；</p></li>
<li><p>处理离群值:
离群值是与数据中的其他值保持一定距离的数据点，适当地排除离群值可以提升模型的准确性。</p></li>
</ul>
</div>
<div class="section" id="id4">
<h3><span class="section-number">10.4.1.2. </span>后处理<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>通常，模型推理结束后，需要把推理的输出数据传递给用户完成后处理，常见的数据后处理手段有：</p>
<ul class="simple">
<li><p>连续数据离散化：模型实际用于预测离散数据，例如商品数量时，用回归模型预测得到的是连续值，需要四取五入、取上下限阈值等得到实际结果；</p></li>
<li><p>数据可视化：将数据图形化、表格化，便于找到数据之间的关系，来决定下一步的分析策略；</p></li>
<li><p>手动拉宽预测范围：回归模型往往预测不出很大或很小的值，结果都集中在中部区域。例如医院的化验数据，通常是要根据异常值诊断疾病。手动拉宽预测范围，将偏离正常范围的值乘一个系数，可以放大两侧的数据，得到更准确的预测结果。</p></li>
</ul>
</div>
</div>
<div class="section" id="ch08-sec-parallel-inference">
<span id="id5"></span><h2><span class="section-number">10.4.2. </span>并行计算<a class="headerlink" href="#ch08-sec-parallel-inference" title="Permalink to this headline">¶</a></h2>
<p>为提升推理的性能，需要重复利用多核的能力，所以一般推理框架会引入多线程机制。主要的思路是将算子的输入数据进行切分，通过多线程去执行不同数据切片，实现算子并行计算，从而成倍提升算子计算性能。</p>
<div class="figure align-default" id="id9">
<span id="ch08-fig-parallel"></span><a class="reference internal image-reference" href="../_images/parallel.png"><img alt="../_images/parallel.png" src="../_images/parallel.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图10.4.1 </span><span class="caption-text">矩阵乘数据切分</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>如图所示，对于矩阵乘可以按左矩阵的行进行切分，可以利用三个线程分别计算A1*B，A2*B，A3*B，实现矩阵乘多线程并行计算。</p>
<p>为方便算子并行计算，同时避免频繁创建销毁线程的开销，推理框架一般会使用线程池机制。业界有两种较为通用的做法：</p>
<ul class="simple">
<li><p>使用OpenMp编程接口：OpenMP（Open
Multi-Processing）是一套支持跨平台共享内存方式的多线程并发的编程API，如算子并行最常用的接口”parallel
for”，实现for循环体的代码被多线程并行执行。</p></li>
<li><p>推理框架实现针对算子并行计算的线程池，相对OpenMp提供的接口会更有针对性，性能会更高，且更轻量。</p></li>
</ul>
</div>
<div class="section" id="ch08-sec-kernel-optimization">
<span id="id6"></span><h2><span class="section-number">10.4.3. </span>算子优化<a class="headerlink" href="#ch08-sec-kernel-optimization" title="Permalink to this headline">¶</a></h2>
<p>在部署AI模型时，我们期望模型执行训练或推理的时间尽可能地短，以获得更优越的性能。对于一个固定的深度学习网络，框架调度的时间占比往往很小，性能的瓶颈就在算子的执行。下面从硬件指令和算法角度介绍一些算子优化的方法。</p>
<div class="section" id="id7">
<h3><span class="section-number">10.4.3.1. </span>硬件指令优化<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>绝大多数的设备上都有CPU，因此算子在CPU上的时间尤为重要，下面介绍一下在ARM
CPU硬件指令优化的方法。</p>
<ol class="arabic simple">
<li><p>汇编语言</p></li>
</ol>
<p>开发者使用的C++、Java等高级编程语言会通过编译器输出为机器指令码序列，而高级编程语言能做的事通常受编译器所限，汇编语言是靠近机器的语言，可以一对一实现任何指令码序列，编写的程序存储空间占用少、执行速度快、效率优于高级编程语言。</p>
<p>在实际应用中，最好是程序的大部分用高级语言编写，运行性能要求很高的部分用汇编语言来编写，通过混合编程实现优势互补。深度学习的卷积、矩阵乘等算子涉及大量的计算，使用汇编语言能够给模型训练和推理性能带来数十到数百倍量级的提升。</p>
<p>下面以ARMv8系列处理器为例，介绍和硬件指令相关的优化。</p>
<ol class="arabic simple" start="2">
<li><p>寄存器与NEON指令</p></li>
</ol>
<p>ARMv8系列的CPU上有32个NEON寄存器v0-v31，如
<a class="reference internal" href="#ch08-fig-register"><span class="std std-numref">图10.4.2</span></a>所示，NEON寄存器v0可存放128bit的数据，即4个float32，8个float16，16个int8等。</p>
<div class="figure align-default" id="id10">
<span id="ch08-fig-register"></span><a class="reference internal image-reference" href="../_images/register.png"><img alt="../_images/register.png" src="../_images/register.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">图10.4.2 </span><span class="caption-text">ARMv8处理器NEON寄存器v0的结构</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>针对该处理器，可以采用SIMD(Single Instruction，Multiple
Data，单指令、多数据)提升数据存取计算的速度。相比于单数据操作指令，NEON指令可以一次性操作NEON寄存器的多个数据。例如：对于浮点数的fmla指令，用法为fmla
v0.4s, v1.4s, v2.4s，如
<a class="reference internal" href="#ch08-fig-fmla"><span class="std std-numref">图10.4.3</span></a>所示，用于将v1和v2两个寄存器中相对应的float值相乘累加到v0的值上。</p>
<div class="figure align-default" id="id11">
<span id="ch08-fig-fmla"></span><a class="reference internal image-reference" href="../_images/fmla.png"><img alt="../_images/fmla.png" src="../_images/fmla.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图10.4.3 </span><span class="caption-text">fmla指令计算功能</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<ol class="arabic simple" start="3">
<li><p>汇编语言优化</p></li>
</ol>
<p>对于已知功能的汇编语言程序来说，计算类指令通常是固定的，性能的瓶颈就在非计算指令上。如
<a class="reference internal" href="model_converter_and_optimizer.html#ch08-fig-storage"><span class="std std-numref">图10.2.1</span></a>所示，计算机各存储设备类似于一个金字塔结构，最顶层空间最小，但是速度最快，最底层速度最慢，但是空间最大。L1-L3统称为cache(高速缓冲存储器)，CPU访问数据时，会首先访问位于CPU内部的cache，没找到再访问CPU之外的主存，此时引入了缓存命中率的概念来描述在cache中完成数据存取的占比。要想提升程序的性能，缓存命中率要尽可能的高。</p>
<p>下面简单列举一些提升缓存命中率、优化汇编性能的手段：</p>
<p>（1）循环展开：尽可能使用更多的寄存器，以代码体积换性能；</p>
<p>（2）指令重排：打乱不同执行单元的指令以提高流水线的利用率，提前有延迟的指令以减轻延迟，减少指令前后的数据依赖等；</p>
<p>（3）寄存器分块：合理分块NEON寄存器，减少寄存器空闲，增加寄存器复用；</p>
<p>（4）计算数据重排：尽量保证读写指令内存连续，提高缓存命中率；</p>
<p>（5）使用预取指令：将要使用到的数据从主存提前载入缓存，减少访问延迟。</p>
</div>
<div class="section" id="id8">
<h3><span class="section-number">10.4.3.2. </span>算法优化<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>多数AI模型的推理时间主要耗费在卷积、矩阵乘算子的计算上，占到了整网百分之九十甚至更多的时间。本小节主要介绍卷积算子算法方面的优化手段，可以应用到各种硬件设备上。
卷积的计算可以转换为两个矩阵相乘，在前述5.3.3小节中，已经详细介绍了矩阵乘GEMM运算的优化。对于不同的硬件，确定合适的矩阵分块，优化数据访存与指令并行，可以最大限度的发挥硬件的算力，提升推理性能。</p>
<p>1.Img2col</p>
<p>将卷积的计算转换为矩阵乘，一般采用Img2col的方法实现。在常见的神经网络中，卷积的输入通常都是4维的，默认采用的数据排布方式为NHWC，如
<a class="reference internal" href="#ch08-fig-conv-nhwc"><span class="std std-numref">图10.4.4</span></a>所示，是一个卷积示意图。输入维度为（1,IH,IW,IC），卷积核维度为（OC,KH,KW,IC），输出维度为（1,OH,OW,OC）。</p>
<div class="figure align-default" id="id12">
<span id="ch08-fig-conv-nhwc"></span><a class="reference internal image-reference" href="../_images/conv_nhwc.png"><img alt="../_images/conv_nhwc.png" src="../_images/conv_nhwc.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图10.4.4 </span><span class="caption-text">通用卷积示意图</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>对卷积的Img2col规则如下。如
<a class="reference internal" href="#ch08-fig-img2col-input"><span class="std std-numref">图10.4.5</span></a>所示，对该输入做重排，得到的矩阵见右侧，行数对应输出的OH*OW的个数；每个行向量里，先排列计算一个输出点所需要输入上第一个通道的KH*KW个数据，再按次序排列之后的通道，直到通道IC。</p>
<div class="figure align-default" id="id13">
<span id="ch08-fig-img2col-input"></span><a class="reference internal image-reference" href="../_images/img2col_input.png"><img alt="../_images/img2col_input.png" src="../_images/img2col_input.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图10.4.5 </span><span class="caption-text">输入Img2col的矩阵</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<p>如
<a class="reference internal" href="#ch08-fig-img2col-weight"><span class="std std-numref">图10.4.6</span></a>所示，对权重数据做重排。将1个卷积核展开为权重矩阵的一列，因此共有OC列，每个列向量上先排列第一个输入通道上KH*KW的数据，再依次排列后面的通道直到IC。通过重排，卷积的计算就可以转换为两个矩阵相乘的求解。在实际实现时，Img2col和GEMM的数据重排会同时进行，以节省运行时间。</p>
<div class="figure align-default" id="id14">
<span id="ch08-fig-img2col-weight"></span><a class="reference internal image-reference" href="../_images/img2col_weight.png"><img alt="../_images/img2col_weight.png" src="../_images/img2col_weight.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">图10.4.6 </span><span class="caption-text">卷积核Img2col的矩阵</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p>2.Winograd算法</p>
<p>卷积计算归根到底是矩阵乘法，两个二维矩阵相乘的时间复杂度是<span class="math notranslate nohighlight">\(O(n^3)\)</span>。我们可以使用Winograd来降低矩阵乘法的复杂度。</p>
<p>以一维卷积运算为例，记为F(m，r)，其中，m代表输出的个数，r为卷积核的个数。输入为<span class="math notranslate nohighlight">\(d=[d_0 \ d_1 \ d_2 \ d_3]\)</span>，卷积核为<span class="math notranslate nohighlight">\(g=[g_0 \ g_1 \ g_2]^T\)</span>，该卷积计算可以写成矩阵形式如公式
<a class="reference internal" href="#equation-ch08-equ-conv-matmul-one-dimension">(10.4.1)</a>所示，需要6次乘法和4次加法。</p>
<div class="math notranslate nohighlight" id="equation-ch08-equ-conv-matmul-one-dimension">
<span class="eqno">(10.4.1)<a class="headerlink" href="#equation-ch08-equ-conv-matmul-one-dimension" title="Permalink to this equation">¶</a></span>\[\begin{split}F(2, 3)=\left[ \begin{matrix} d_0 &amp; d_1 &amp; d_2 \\ d_1 &amp; d_2 &amp; d_3 \end{matrix} \right] \left[ \begin{matrix} g_0 \\ g_1 \\ g_2 \end{matrix} \right]=\left[ \begin{matrix} y_0 \\ y_1 \end{matrix} \right]\end{split}\]</div>
<p>可以观察到，卷积运算转换为矩阵乘法时输入矩阵中存在着重复元素<span class="math notranslate nohighlight">\(d_1\)</span>和<span class="math notranslate nohighlight">\(d_2\)</span>，因此，卷积转换的矩阵乘法相对一般的矩阵乘有了优化空间。可以通过计算中间变量<span class="math notranslate nohighlight">\(m_0-m_3\)</span>得到矩阵乘的结果，见公式
<a class="reference internal" href="#equation-ch08-equ-conv-2-winograd">(10.4.2)</a>：</p>
<div class="math notranslate nohighlight" id="equation-ch08-equ-conv-2-winograd">
<span class="eqno">(10.4.2)<a class="headerlink" href="#equation-ch08-equ-conv-2-winograd" title="Permalink to this equation">¶</a></span>\[\begin{split}F(2, 3)=\left[ \begin{matrix} d_0 &amp; d_1 &amp; d_2 \\ d_1 &amp; d_2 &amp; d_3 \end{matrix} \right] \left[ \begin{matrix} g_0 \\ g_1 \\ g_2 \end{matrix} \right]=\left[ \begin{matrix} m_0+m_1+m_2 \\ m_1-m_2+m_3 \end{matrix} \right]\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(m_0-m_3\)</span>的分别见公式
<a class="reference internal" href="#equation-ch08-equ-winograd-param">(10.4.3)</a>：</p>
<div class="math notranslate nohighlight" id="equation-ch08-equ-winograd-param">
<span class="eqno">(10.4.3)<a class="headerlink" href="#equation-ch08-equ-winograd-param" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}m_0=(d_0-d_2)*g_0 \\m_1=(d_1+d_2)*(\frac{g_0+g_1+g_2}{2}) \\m_2=(d_0-d_2)*(\frac{g_0-g_1+g_2}{2}) \\m_2=(d_1-d_3)*g_2\end{aligned}\end{split}\]</div>
<p>通过<span class="math notranslate nohighlight">\(m_0-m_3\)</span>间接计算r1，r2，需要的运算次数包括：输入d的4次加法；输出m的4次乘法和4次加法。在推理阶段，权重的数值是常量，因此卷积核上的运算可以在图编译阶段计算，不计入在线的run时间。所以总的运算次数为4次乘法和8次加法，与直接运算的6次乘法和4次加法相比，乘法次数减少，加法次数增加。在计算机中，乘法一般比加法慢，通过减少乘法次数，增加少量加法，可以实现加速。</p>
<p>计算过程写成矩阵形式如公式
<a class="reference internal" href="#equation-ch08-equ-winograd-matrix">(10.4.4)</a>所示，其中，⊙为对应位置相乘，A、B、G都是常量矩阵。这里写成矩阵计算是为了表达清晰，实际使用时，按照公式
<a class="reference internal" href="#equation-ch08-equ-winograd-param">(10.4.3)</a>手写展开的计算速度更快。</p>
<div class="math notranslate nohighlight" id="equation-ch08-equ-winograd-matrix">
<span class="eqno">(10.4.4)<a class="headerlink" href="#equation-ch08-equ-winograd-matrix" title="Permalink to this equation">¶</a></span>\[\mathbf{Y}=\mathbf{A^T}(\mathbf{G}g)*(\mathbf{B^T}d)\]</div>
<div class="math notranslate nohighlight" id="equation-ch08-equ-winograd-matrix-bt">
<span class="eqno">(10.4.5)<a class="headerlink" href="#equation-ch08-equ-winograd-matrix-bt" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{B^T}=\left[ \begin{matrix} 1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; -1 \end{matrix} \right]\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-ch08-equ-winograd-matrix-g">
<span class="eqno">(10.4.6)<a class="headerlink" href="#equation-ch08-equ-winograd-matrix-g" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{G}=\left[ \begin{matrix} 1 &amp; 0 &amp; 0 \\ 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; -0.5 &amp; 0.5 \\ 0 &amp; 0 &amp; 1 \end{matrix} \right]\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-ch08-equ-winograd-matrix-at">
<span class="eqno">(10.4.7)<a class="headerlink" href="#equation-ch08-equ-winograd-matrix-at" title="Permalink to this equation">¶</a></span>\[\begin{split}\mathbf{A^T}=\left[ \begin{matrix} 1 &amp; 1 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; -1 &amp; -1  \end{matrix} \right] \\\end{split}\]</div>
<p>通常深度学习领域通常使用的都是2D卷积，将F(2，3)扩展到F(2x2，3x3)，可以写成矩阵形式，如公式
<a class="reference internal" href="#equation-ch08-equ-winograd-two-dimension-matrix">(10.4.8)</a>所示。此时，Winograd算法的乘法次数为16，而直接卷积的乘法次数为36，降低了2.25倍的乘法计算复杂度。</p>
<div class="math notranslate nohighlight" id="equation-ch08-equ-winograd-two-dimension-matrix">
<span class="eqno">(10.4.8)<a class="headerlink" href="#equation-ch08-equ-winograd-two-dimension-matrix" title="Permalink to this equation">¶</a></span>\[\mathbf{Y}=\mathbf{A^T}(\mathbf{G}g\mathbf{G^T})*(\mathbf{B^T}d\mathbf{B})\mathbf{A}\]</div>
<p>Winograd算法的整个计算过程在逻辑上可以分为4步，如
<a class="reference internal" href="#ch08-fig-winograd"><span class="std std-numref">图10.4.7</span></a>所示：</p>
<div class="figure align-default" id="id15">
<span id="ch08-fig-winograd"></span><a class="reference internal image-reference" href="../_images/winograd.png"><img alt="../_images/winograd.png" src="../_images/winograd.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">图10.4.7 </span><span class="caption-text">winograd步骤示意图</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>针对任意的输出大小，要使用F(2x2，3x3)的Winograd算法，需要将输出切分成2x2的块，找到对应的输入，按照上述的四个步骤，就可以求出对应的输出值。当然，Winograd算法并不局限于求解F(2x2，3x3)，针对任意的F(m*m，r*r)，都可以找到适当的常量矩阵A、B、G，通过间接计算的方式减少乘法次数。但是随着m、r的增大，输入、输出涉及的加法以及常量权重的乘法次数都在增加，那么乘法次数带来的计算量下降会被加法和常量乘法所抵消。因此，在实际使用场景中，还需要根据Winograd的实际收益来选择。</p>
<p>本小节主要介绍了模型推理时的数据处理和性能优化手段。选择合适的数据处理方法，可以更好地提取输入特征，处理输出结果。并行计算以及算子级别的硬件指令与算法优化可以最大限度的发挥硬件的算力。除此之后，内存的占用及访问速率也是影响推理性能的重要因素，因此推理时需要设计合理的内存复用策略，关于内存复用的策略我们在编译器后端章节已经做了阐述。</p>
</div>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">10.4. 模型推理</a><ul>
<li><a class="reference internal" href="#id2">10.4.1. 前处理与后处理</a><ul>
<li><a class="reference internal" href="#id3">10.4.1.1. 前处理</a></li>
<li><a class="reference internal" href="#id4">10.4.1.2. 后处理</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ch08-sec-parallel-inference">10.4.2. 并行计算</a></li>
<li><a class="reference internal" href="#ch08-sec-kernel-optimization">10.4.3. 算子优化</a><ul>
<li><a class="reference internal" href="#id7">10.4.3.1. 硬件指令优化</a></li>
<li><a class="reference internal" href="#id8">10.4.3.2. 算法优化</a></li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="model_compression.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>10.3. 模型压缩</div>
         </div>
     </a>
     <a id="button-next" href="model_security.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>10.5. 模型的安全保护</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>