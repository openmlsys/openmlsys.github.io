<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>1. 神经网络 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. 梯度下降与反向传播" href="gradient_descent.html" />
    <link rel="prev" title="12. 附录：机器学习介绍" href="index.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">12. </span>附录：机器学习介绍</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">1. </span>神经网络</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/appendix_Introduction_machine_learning/neural_network.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/index.html">4. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/overview_of_frontend.html">4.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/intermediate_representation.html">4.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/ad.html">4.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/type_system_and_static_analysis.html">4.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/common_frontend_optimization_pass.html">4.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/summary.html">4.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/index.html">5. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/overview.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/graph_optimizer.html">5.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/kernel_selecter.html">5.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/memory_allocator.html">5.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/compute_schedule_and_execute.html">5.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hardware_accelerator/index.html">6. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/accelerator_introduction.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/accelerator_architecture.html">6.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/accelerator_programming.html">6.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/summary.html">6.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing_framework/index.html">7. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/requirements.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/program_model.html">7.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/performance.html">7.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/data_order.html">7.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/extension.html">7.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/summary.html">7.6. 章节总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/reference.html">7.7. 引用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">8. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">8.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">8.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">8.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">8.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">8.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training_system/index.html">分布式训练系统</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_transition_advanced/index.html">9. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_framework_expansion/index.html">10. 框架拓展</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">12. 附录：机器学习介绍</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/index.html">4. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/overview_of_frontend.html">4.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/intermediate_representation.html">4.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/ad.html">4.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/type_system_and_static_analysis.html">4.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/common_frontend_optimization_pass.html">4.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_frontend_and_ir/summary.html">4.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/index.html">5. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/overview.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/graph_optimizer.html">5.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/kernel_selecter.html">5.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/memory_allocator.html">5.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/compute_schedule_and_execute.html">5.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_compiler_backend_and_runtime/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hardware_accelerator/index.html">6. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/accelerator_introduction.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/accelerator_architecture.html">6.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/accelerator_programming.html">6.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hardware_accelerator/summary.html">6.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing_framework/index.html">7. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/requirements.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/program_model.html">7.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/performance.html">7.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/data_order.html">7.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/extension.html">7.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/summary.html">7.6. 章节总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing_framework/reference.html">7.7. 引用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">8. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">8.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">8.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">8.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">8.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">8.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training_system/index.html">分布式训练系统</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_transition_advanced/index.html">9. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_framework_expansion/index.html">10. 框架拓展</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">12. 附录：机器学习介绍</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="id1">
<h1><span class="section-number">1. </span>神经网络<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<section id="id2">
<h2><span class="section-number">1.1. </span>感知器<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<figure class="align-default" id="id8">
<span id="single-neuron"></span><a class="reference internal image-reference" href="../_images/single_neuron2.png"><img alt="../_images/single_neuron2.png" src="../_images/single_neuron2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.1.1 </span><span class="caption-text">有三个输入和单一输出的神经元</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#single-neuron"><span class="std std-numref">图1.1.1</span></a>是一个神经元的例子，输入数据<span class="math notranslate nohighlight">\(x\)</span>根据连线上的权重<span class="math notranslate nohighlight">\(w\)</span>做加权求和得到输出<span class="math notranslate nohighlight">\(z\)</span>，我们把这样的模型叫作<strong>感知器</strong>（Perceptron）。
因为输入和输出之间只有一层神经连接，这个模型也叫做单层感知器。
<a class="reference internal" href="#single-neuron"><span class="std std-numref">图1.1.1</span></a>的模型计算可以写为：<span class="math notranslate nohighlight">\(z = w_{1}x_{1}+ w_{2}x_{2} + w_{3}x_{3}\)</span>。</p>
<p>当输入数据用列向量<span class="math notranslate nohighlight">\(\bm{x}=[x_1,x_2,x_3]^T\)</span>表示，模型权重用行向量<span class="math notranslate nohighlight">\(\bm{w}=[w_1,w_2,w_3]\)</span>表示，那么输出的标量<span class="math notranslate nohighlight">\(z\)</span>可以写为：</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-0">
<span class="eqno">(1.1.1)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
z =
\begin{bmatrix}
w_1,w_2,w_3\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}
=\bm{w}\bm{x}\end{aligned}\end{split}\]</div>
<p>我们可以利用输出标量<span class="math notranslate nohighlight">\(z\)</span>为输入的加权组合来实现特定任务。
比如，可以对”好苹果”和”坏苹果”进行分类，输入的<span class="math notranslate nohighlight">\(x_1,x_2,x_3\)</span>分别代表三种不同的特征：1）红色的程度，2）有没有洞，3）大小。如果苹果的大小对这个判断没有影响，那么对应的权重就为零。
这个神经网络的训练，其实就是选择合适的权重，来实现我们的任务。比如我们可以选择合适的权重，使得当<span class="math notranslate nohighlight">\(z\)</span>小于等于<span class="math notranslate nohighlight">\(0\)</span>时代表”坏苹果”，而当<span class="math notranslate nohighlight">\(z\)</span>大于<span class="math notranslate nohighlight">\(0\)</span>时则是”好苹果”。
则最终的分类输出标签<span class="math notranslate nohighlight">\(y\)</span>如下，为<span class="math notranslate nohighlight">\(1\)</span>时代表好，<span class="math notranslate nohighlight">\(0\)</span>代表坏。这个神经元的输入和输出之间只有一层，所以可以成为单层神经网络。</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-1">
<span class="eqno">(1.1.2)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
y =
\begin{cases}
1 &amp;  z&gt;0 \\
0 &amp; z \leq 0 \\
\end{cases}\end{aligned}\end{split}\]</div>
</section>
<section id="vs">
<h2><span class="section-number">1.2. </span>决策边界vs.偏置<a class="headerlink" href="#vs" title="Permalink to this headline">¶</a></h2>
<p>通过选择合适的权重以<span class="math notranslate nohighlight">\(z\)</span>大于或小于<span class="math notranslate nohighlight">\(0\)</span>来对输入数据做分类的话，可以在数据空间上获得一个<strong>决策边界</strong>
（Decision Boundary）。如
<a class="reference internal" href="#single-neuron-decision-boundary2"><span class="std std-numref">图1.2.2</span></a>所示，以神经元输出<span class="math notranslate nohighlight">\(z=0\)</span>作为输出标签<span class="math notranslate nohighlight">\(y\)</span>的决策边界，没有偏置时决策边界必然经过坐标原点，如果数据样本点不以原点来分开，会导致分类错误。
为了解决这个问题，可以在神经元上加入一个<strong>偏置</strong>（Bias）。
<a class="reference internal" href="#single-neuron-bias2"><span class="std std-numref">图1.2.3</span></a>
是一个有偏置<span class="math notranslate nohighlight">\(b\)</span>的神经元模型，可以用公式 <a class="reference external" href="#equ:singleneuron_bias">[equ:singleneuron_bias]</a>
表达：</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-2">
<span class="eqno">(1.2.1)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-2" title="Permalink to this equation">¶</a></span>\[\begin{aligned}
\label{equ:singleneuron_bias}
z = w_{1}x_{1}+ w_{2}x_{2}+ w_{3}x_{3} + b\end{aligned}\]</div>
<figure class="align-default" id="id9">
<span id="single-neuron-decision-boundary2"></span><a class="reference internal image-reference" href="../_images/single_neuron_decision_boundary2.png"><img alt="../_images/single_neuron_decision_boundary2.png" src="../_images/single_neuron_decision_boundary2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.2.2 </span><span class="caption-text">两个输入（左）和三个输入（右）时的决策边界。不同形状的点代表不同类别的数据，需要找到<span class="math notranslate nohighlight">\(z=0\)</span>作为决策边界来把不同数据点分开。两个输入时决策边界是一直线，三个输入时决策边界是一个平面，高维度输入时决策边界称为<strong>超平面</strong>（Hyperplane）。
左：
<span class="math notranslate nohighlight">\(z=w_{1}x_{1}+w_{2}x_{2}+b\)</span>。右：<span class="math notranslate nohighlight">\(z=w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}+b\)</span>。没有偏置时，决策边界必然经过原点，所以不能分开不同类别的数据样本。</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id10">
<span id="single-neuron-bias2"></span><a class="reference internal image-reference" href="../_images/single_neuron_bias2.png"><img alt="../_images/single_neuron_bias2.png" src="../_images/single_neuron_bias2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.2.3 </span><span class="caption-text">一个有偏置的单层神经网络</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>有了偏置以后，决策边界（直线、平面或超平面）可以不经过坐标原点，因此能更好地分类样本。
准确来说，决策边界把这些样本数据分成两个不同的类别，这个边界是
<span class="math notranslate nohighlight">\(\{x_1, x_2, x_3 | w_{1}x_{1}+ w_{2}x_{2}+ w_{3}x_{3} + b = 0\}\)</span>。</p>
</section>
<section id="id3">
<h2><span class="section-number">1.3. </span>逻辑回归<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>上述神经元的输入和输出是线性关系，为了提供非线性的数据表达能力，可以在神经元输出上加上<strong>激活函数</strong>（Activation
Function），最常见的激活函数有Sigmoid、Tanh、ReLU和Softmax等。
比如，上述神经元以<span class="math notranslate nohighlight">\(z=0\)</span>为分界来做分类任务，那么我们可不可以让神经元输出一个概率呢？比如输出<span class="math notranslate nohighlight">\(0~1\)</span>，<span class="math notranslate nohighlight">\(1\)</span>代表输入数据<span class="math notranslate nohighlight">\(100\%\)</span>为某一类。
为了让神经元输出<span class="math notranslate nohighlight">\(0~1\)</span>，可以在<span class="math notranslate nohighlight">\(z\)</span>上加一个逻辑函数<strong>Sigmoid</strong>，
如公式 <a class="reference external" href="#equ:sigmoid">[equ:sigmoid]</a>
所示，Sigmoid把数值限制在0和1之中，通过一个简单的临界值（如：0.5）来决定最终输出的标签是否属于某个类别。这个方法叫做<strong>逻辑回归</strong>（Logistic
Regression）。</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-3">
<span class="eqno">(1.3.1)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-3" title="Permalink to this equation">¶</a></span>\[\begin{aligned}
\label{equ:sigmoid}
a = f(\bm{z}) = \frac{1}{1+{\rm e}^{-\bm{z}}}\end{aligned}\]</div>
</section>
<section id="id4">
<h2><span class="section-number">1.4. </span>多个神经元<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<figure class="align-default" id="id11">
<span id="two-neurons2"></span><a class="reference internal image-reference" href="../_images/two_neurons2.png"><img alt="../_images/two_neurons2.png" src="../_images/two_neurons2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.4.1 </span><span class="caption-text">多个神经元</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>上述网络只有一个输出，若多个神经元在一起就可以有多个输出。
<a class="reference internal" href="#two-neurons2"><span class="std std-numref">图1.4.1</span></a>是有两个输出的网络，每个输出都和所有输入相连，所以也被称<strong>全连接层</strong>（Fully-Connected(FC)
Layer），可由下述式子 <a class="reference external" href="#equ:foundation_DL/fullyconnected">[equ:foundation_DL/fullyconnected]</a>
表示X。</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-4">
<span class="eqno">(1.4.1)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-4" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\label{equ:foundation_DL/fullyconnected}
z_{1} &amp;= w_{11}x_{1} + w_{12}x_{2} + w_{13}x_{3} + b_1 \notag \\
z_{2} &amp;= w_{21}x_{1} + w_{22}x_{2} + w_{23}x_{3} + b_2\end{aligned}\end{split}\]</div>
<p>如下式子表示了矩阵方法的实现：</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-5">
<span class="eqno">(1.4.2)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-5" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\label{equ:foundation_DL/fullyconnect/colvector/all}
\bm{z} =
\begin{bmatrix}
z_1 \\
z_2
\end{bmatrix}
=
\begin{bmatrix}
w_{11} &amp; w_{12} &amp; w_{13}\\
w_{21} &amp; w_{22} &amp; w_{23}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\ b_2
\end{bmatrix}
= \bm{W}\bm{x} + \bm{b}\end{aligned}\end{split}\]</div>
<p>多输出的网络可以实现多分类问题，比如有10个数值输出，每个数值分别代表一类物品的概率，每个输出在<span class="math notranslate nohighlight">\(0\)</span>到<span class="math notranslate nohighlight">\(1\)</span>之间，10个输出之和为<span class="math notranslate nohighlight">\(1\)</span>。可用公式 <a class="reference external" href="#equ:softmax">[equ:softmax]</a>的<strong>Softmax</strong>
函数来实现，<span class="math notranslate nohighlight">\(K\)</span>为输出的个数：</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-6">
<span class="eqno">(1.4.3)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-6" title="Permalink to this equation">¶</a></span>\[\begin{aligned}
\label{equ:softmax}
f(\bm{z})_{i} = \frac{{\rm e}^{z_{i}}}{\sum_{k=1}^{K}{\rm e}^{z_{k}}}\end{aligned}\]</div>
</section>
<section id="id5">
<h2><span class="section-number">1.5. </span>多层感知器<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<figure class="align-default" id="id12">
<img alt="../_images/mlp2.png" src="../_images/mlp2.png" />
<figcaption>
<p><span class="caption-number">图1.5.1 </span><span class="caption-text">多层感知器例子。<span class="math notranslate nohighlight">\(a^l_i\)</span>表示神经元输出<span class="math notranslate nohighlight">\(z\)</span>经过激活函数后的值，其中<span class="math notranslate nohighlight">\(l\)</span>代表层的序号（<span class="math notranslate nohighlight">\(L\)</span>代表输出层），<span class="math notranslate nohighlight">\(i\)</span>代表输出的序号</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>多层感知器</strong>（Multi-Layer
Perceptron，MLP）[&#64;rosenblatt1958perceptron]通过叠加多层全连接层来提升网络的表达能力。相比单层网络，多层感知器有很多中间层的输出并不暴露给最终输出，这些层被称为<strong>隐含层</strong>（Hidden
Layers）。这个例子中的网络可以通过下方的串联式矩阵运算实现，其中<span class="math notranslate nohighlight">\(W^l\)</span>和<span class="math notranslate nohighlight">\(b^l\)</span>代表不同层的权重矩阵和偏置，<span class="math notranslate nohighlight">\(l\)</span>代表层号，<span class="math notranslate nohighlight">\(L\)</span>代表输出层。</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-7">
<span class="eqno">(1.5.1)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-7" title="Permalink to this equation">¶</a></span>\[\begin{aligned}
\label{equ:foundation_DL/xxxxx}
\bm{z} = f(\bm{W^L}f(\bm{W^3}f(\bm{W^2}f(\bm{W^1}\bm{x} + \bm{b^1}) + \bm{b^2}) + \bm{b^3}) + \bm{b^L})\end{aligned}\]</div>
<p>在深度学习时代，网络模型基本都是多层的神经网络层连接起来的，输入数据经过多层的特征提取，可以学到不同抽象层级的<strong>特征向量</strong>（Feature
Vector）。下面我们介绍一下其他常用的神经网络层。</p>
</section>
<section id="id6">
<h2><span class="section-number">1.6. </span>卷积网络<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<figure class="align-default" id="id13">
<span id="conv-computation-v4"></span><a class="reference internal image-reference" href="../_images/conv_computation_v4.png"><img alt="../_images/conv_computation_v4.png" src="../_images/conv_computation_v4.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.6.1 </span><span class="caption-text">卷积运算例子。
输入一个三通道的数据，其大小为<span class="math notranslate nohighlight">\(4 \times 4 \times 3\)</span>（高
<span class="math notranslate nohighlight">\(\times\)</span> 宽 <span class="math notranslate nohighlight">\(\times\)</span>
通道数），为了对各个通道做卷积，卷积核也必须有三个通道，一个卷积核的大小为<span class="math notranslate nohighlight">\(3 \times 3 \times 3 \times 1\)</span>（高
<span class="math notranslate nohighlight">\(\times\)</span>
宽<span class="math notranslate nohighlight">\(\times\)</span>输入通道数<span class="math notranslate nohighlight">\(\times\)</span>输出通道数（卷积核的个数））。有多少个卷积核就有多少个输出的<strong>特征图</strong>（Feature
Map），在这个例子中因为只有一个卷积核，所以输出的通道数为1，高宽为2。与此同时，我们把这种高维度的输入数据称为<strong>张量</strong>（Tensor），比如RGB图像、视频、前一层卷积层的输出等等。</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>卷积神经网络</strong> （Convolutional Neural
Network，CNN）[&#64;lecun1989backpropagation]由多层<strong>卷积层</strong>（Convolutional
Layer）组成，常用于计算机视觉任务 [&#64;krizhevsky2012imagenet;
&#64;he2016deep]。
<a class="reference internal" href="#conv-computation-v4"><span class="std std-numref">图1.6.1</span></a>描述了一个卷积运算的例子。
根据卷积的特点，我们可以知道两个事实：1）一个卷积核的通道数，等于输入的通道数；2）输出的通道数，等于卷积核的数量。</p>
<p><a class="reference internal" href="#conv-computation-v4"><span class="std std-numref">图1.6.1</span></a>例子中，卷积核每次滑动一个数值的范围来进行卷积操作，我们称它的<strong>步长</strong>（Stride）为1。此外，如果希望输入的边缘数值也能被考虑在内的话，则需要对边缘做<strong>填零</strong>（Zero
Padding）操作。
<a class="reference internal" href="#conv-computation-v4"><span class="std std-numref">图1.6.1</span></a>例子中，如果输入的每个通道上下左右都填充一圈零，那么输出的大小则为<span class="math notranslate nohighlight">\(4\times 4\times 1\)</span>。填零的圈数取决于卷积核的大小，卷积核越大则填零圈数越大。</p>
<p>为了对输入的图像数据做特征提取，卷积核数量往往比输入数据的通道数据要多，这样的话输出数据的数值会很多，计算量变大。然而图像数据中相邻像素的特征往往相似，所以我们可以对相邻的输出特征进行聚合操作。<strong>池化层</strong>就是为了实现这个目的，我们通常有两种池化方法最大值池化（Max
Pooling）和平均值池化（Mean Pooling）。如
<a class="reference internal" href="#pooling-v3"><span class="std std-numref">图1.6.2</span></a>所示，假设池化的卷积核高宽为<span class="math notranslate nohighlight">\(2\times2\)</span>，输入<span class="math notranslate nohighlight">\(4\times4\)</span>的数据，步长为2（步长为1时，则输出等于输入），则输出为<span class="math notranslate nohighlight">\(2\times2\)</span>。</p>
<figure class="align-default" id="id14">
<span id="pooling-v3"></span><a class="reference internal image-reference" href="../_images/pooling_v3.png"><img alt="../_images/pooling_v3.png" src="../_images/pooling_v3.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.6.2 </span><span class="caption-text"><span class="math notranslate nohighlight">\(2 \times 2\)</span>
最大值池化和平均值池化的例子，它们的步长为2，输入大小是<span class="math notranslate nohighlight">\(4 \times 4\)</span></span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>卷积层和全连接层都是很常用的，但是卷积层在输入是高维度的图像时，需要的参数量远远小于全连接层。卷积层的运算和全连接层是类似的，前者基于高维度张量运算，后者基于二维矩阵运算。</p>
</section>
<section id="id7">
<h2><span class="section-number">1.7. </span>时序模型<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>现实生活中除了图像还有大量时间序列数据，例如视频、股票价格等等。<strong>循环神经网络</strong>（Recurrent
Neural Networks，RNN）[&#64;rumelhart1986learning]
是一种处理序列数据的深度学习模型结构。序列数据是一串连续的数据<span class="math notranslate nohighlight">\(\{x_1, x_2, \dots, x_n\}\)</span>，比如每个<span class="math notranslate nohighlight">\(x\)</span>代表一个句子中的单词。</p>
<p>为了可以接收一连串的输入序列，如
<a class="reference internal" href="#rnn-simple-cell2"><span class="std std-numref">图1.7.1</span></a>所示，朴素循环神经网络使用了循环单元（Cell）作为计算单元，用隐状态（Hidden
State）来存储过去输入的信息。具体来说，对输入模型的每个数据<span class="math notranslate nohighlight">\(x\)</span>，根据公式 <a class="reference external" href="#eq1-25">[eq1-25]</a>，循环单元会反复计算新的隐状态，用于记录当前和过去输入的信息。而新的隐状态会被用到下一单元的计算中。</p>
<div class="math notranslate nohighlight" id="equation-appendix-introduction-machine-learning-neural-network-8">
<span class="eqno">(1.7.1)<a class="headerlink" href="#equation-appendix-introduction-machine-learning-neural-network-8" title="Permalink to this equation">¶</a></span>\[\begin{aligned}
\label{eq1-25}
\bm{h}_t = \bm{W}[\bm{x}_t; \bm{h}_{t-1}] + \bm{b}\end{aligned}\]</div>
<figure class="align-default" id="id15">
<span id="rnn-simple-cell2"></span><a class="reference internal image-reference" href="../_images/rnn_simple_cell2.png"><img alt="../_images/rnn_simple_cell2.png" src="../_images/rnn_simple_cell2.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">图1.7.1 </span><span class="caption-text">朴素循环神经网络。
在每一步的计算中，循环单元通过过去时刻的隐状态<span class="math notranslate nohighlight">\(\bm{h}_{t-1}\)</span>和当前的输入<span class="math notranslate nohighlight">\(\bm{x}_t\)</span>，求得当前的隐状态<span class="math notranslate nohighlight">\(\bm{h}_t\)</span>。</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>然而这种简单的朴素循环神经网络有严重的信息遗忘问题。比如说我们的输入是”我是中国人，<a href="#id16"><span class="problematic" id="id17">我的母语是__</span></a>_“，隐状态记住了”中国人”的信息，使得网络最后可以预测出”中文”一词；但是如果句子很长的时候，隐状态可能记不住太久之前的信息了，比如说”我是中国人，我去英国读书，后来在法国工作，<a href="#id16"><span class="problematic" id="id18">我的母语是__</span></a>_“，这时候在最后的隐状态中关于”中国人”的信息可能会被因为多次的更新而遗忘了。
为了解决这个问题，后面有人提出了各种各样的改进方法，其中最有名的是长短期记忆（Long
Short-Term
Memory，LSTM）[&#64;Hochreiter1997lstm]。关于时序的模型还有很多很多，比如近年来出现的Transformer [&#64;vaswani2017attention]等等。</p>
</section>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">1. 神经网络</a><ul>
<li><a class="reference internal" href="#id2">1.1. 感知器</a></li>
<li><a class="reference internal" href="#vs">1.2. 决策边界vs.偏置</a></li>
<li><a class="reference internal" href="#id3">1.3. 逻辑回归</a></li>
<li><a class="reference internal" href="#id4">1.4. 多个神经元</a></li>
<li><a class="reference internal" href="#id5">1.5. 多层感知器</a></li>
<li><a class="reference internal" href="#id6">1.6. 卷积网络</a></li>
<li><a class="reference internal" href="#id7">1.7. 时序模型</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>12. 附录：机器学习介绍</div>
         </div>
     </a>
     <a id="button-next" href="gradient_descent.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>2. 梯度下降与反向传播</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>