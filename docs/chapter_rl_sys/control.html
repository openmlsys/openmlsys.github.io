<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>17.5. 控制系统 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="17.6. 小结" href="summary.html" />
    <link rel="prev" title="17.4. 规划系统" href="planning.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">17. </span>机器人系统</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">17.5. </span>控制系统</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_rl_sys/control.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 序言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">2.2. 设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">2.3. 基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">2.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">8. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">8.4. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id2">8.5. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id3">8.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html#id2">9.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">10. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">11. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">11.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">11.4. 集合通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">11.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">11.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id2">11.7. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id3">11.8. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">13.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">13.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">13.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id3">13.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">15.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">16.4. 可解释AI系统及实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id12">16.5. 未来可解释AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id13">16.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">17. 机器人系统</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rl_sys_intro.html">17.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="perception.html">17.3. 感知系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="planning.html">17.4. 规划系统</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">17.5. 控制系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">17.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">17.7. 参考文献</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 序言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">2.2. 设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">2.3. 基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">2.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">8. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">8.4. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id2">8.5. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id3">8.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html#id2">9.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">10. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">11. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">11.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">11.4. 集合通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">11.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">11.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id2">11.7. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id3">11.8. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">13.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">13.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">13.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id3">13.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">15.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">16.4. 可解释AI系统及实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id12">16.5. 未来可解释AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id13">16.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">17. 机器人系统</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="rl_sys_intro.html">17.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="perception.html">17.3. 感知系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="planning.html">17.4. 规划系统</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">17.5. 控制系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">17.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">17.7. 参考文献</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">4. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">17.5. </span>控制系统<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>虽然控制理论已牢牢植根于基于模型（Model-based）的设计传统，但丰富的数据和机器学习给控制理论带来了新的机遇。控制理论和机器学习的交叉点涵盖了广泛的研究方向，包括但不限于动态系统的学习、在线学习和控制、深度学习的控制理论观点、强化学习以及在各种现实世界系统中的应用。
从机器学习的角度来看，未来的主要挑战之一是超越模式识别并解决数据驱动控制和动态过程优化方面的问题。</p>
<p>理论方面，线性二次控制（Linear-Quadratic
Control）是经典的控制方法，最近有关于图神经网络在分布式线性二次控制的研究。作者称将线性二次问题转换为自监督学习问题，能够找到基于图神经网络（Graph
Neural
Networks，GNN）的最佳分布式控制器，他们还推导出了所得闭环系统稳定的充分条件。随着基于数据和学习的机器人控制方法不断得到重视，研究人员必须了解何时以及如何在现实世界中最好地利用这些方法，因为安全是至关重要的，有的研究通过学习不确定的动力学来安全地提高性能，鼓励安全或稳健的强化学习方法，以及可以正式认证所学控制策略的安全性的方法。
<a class="reference internal" href="#safe-learning-control"><span class="std std-numref">图17.5.1</span></a>展示了安全学习控制（Safe Learning
Control）系统的框架图，用数据驱动的方法来学习控制策略，兼顾安全性。Lyapunov
函数是评估非线性动力系统稳定性的有效工具，最近有人提出Neural
Lyapunov来将安全性纳入考虑。</p>
<p>应用方面，有基于神经网络的自动驾驶汽车模型预测控制，也有研究将最优控制和学习相结合并应用在陌生环境中的视觉导航，该研究将基于模型的控制与基于学习的感知相结合来解决。基于学习的感知模块产生一系列航路点通过无碰撞路径引导机器人到达目标。基于模型的规划器使用这些航路点来生成平滑且动态可行的轨迹，该轨迹使用反馈控制在物理系统上执行。在模拟的现实世界杂乱环境和实际地面车辆上的实验表明，与纯粹基于几何映射或基于端到端学习的替代方案相比，这种新的系统可以在新环境中更可靠、更有效地到达目标位置。强化学习和模仿学习与控制论有密切联系：LEOC整合了强化学习和经典控制理论的原则方法。有人将基于模型的离线强化学习算法扩展到高维视觉观察空间并在真实机器人上执行基于图像的抽屉关闭任务方面表现出色。控制部分通过神经网络优化可以更加平滑、节能、安全，如何将
神经网络和传统控制理论结合，特别是和运动学算法相结合，将会是一个有趣的方向。</p>
<div class="figure align-default" id="id11">
<span id="safe-learning-control"></span><a class="reference internal image-reference" href="../_images/safe_learning_control.png"><img alt="../_images/safe_learning_control.png" src="../_images/safe_learning_control.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图17.5.1 </span><span class="caption-text">安全学习控制系统，数据被用来更新控制策略或或安全滤波器
<a class="bibtex reference internal" href="summary.html#brunke2021safe" id="id2">[Brunke et al., 2021]</a></span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p id="bibtex-bibliography-chapter_rl_sys/control-0"><dl class="citation">
<dt class="bibtex label" id="bastoul2004code"><span class="brackets">Bastoul, 2004</span></dt>
<dd><p>Bastoul, C. (2004). Code generation in the polyhedral model is easier than you think. <em>Proceedings. 13th International Conference on Parallel Architecture and Compilation Techniques, 2004. PACT 2004.</em> (pp. 7–16).</p>
</dd>
<dt class="bibtex label" id="berner2019dota"><span class="brackets">Berner et al., 2019</span></dt>
<dd><p>Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., … others. (2019). Dota 2 with large scale deep reinforcement learning. <em>arXiv preprint arXiv:1912.06680</em>.</p>
</dd>
<dt class="bibtex label" id="brunke2021safe"><span class="brackets"><a class="fn-backref" href="#id2">Brunke et al., 2021</a></span></dt>
<dd><p>Brunke, L., Greeff, M., Hall, A. W., Yuan, Z., Zhou, S., Panerati, J., &amp; Schoellig, A. P. (2021). Safe learning in robotics: from learning-based control to safe reinforcement learning. <em>Annual Review of Control, Robotics, and Autonomous Systems</em>, <em>5</em>.</p>
</dd>
<dt class="bibtex label" id="cassirer2021reverb"><span class="brackets">Cassirer et al., 2021</span></dt>
<dd><p>Cassirer, A., Barth-Maron, G., Brevdo, E., Ramos, S., Boyd, T., Sottiaux, T., &amp; Kroiss, M. (2021). Reverb: a framework for experience replay. <em>arXiv preprint arXiv:2102.04736</em>.</p>
</dd>
<dt class="bibtex label" id="fedbe"><span class="brackets">Chen &amp; Chao, 2021</span></dt>
<dd><p>Chen, Hong-You, &amp; Chao, Wei-Lun. (2021). Fedbe: making bayesian model ensemble applicable to federated learning. <em>9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>.</p>
</dd>
<dt class="bibtex label" id="chen2018tvm"><span class="brackets">Chen et al., 2018</span></dt>
<dd><p>Chen, T., Moreau, T., Jiang, Z., Shen, H., Yan, E. Q., Wang, L., … Krishnamurthy, A. (2018). Tvm: end-to-end optimization stack for deep learning. <em>arXiv preprint arXiv:1802.04799</em>, <em>11</em>, 20.</p>
</dd>
<dt class="bibtex label" id="id3"><span class="brackets">Cheng et al., 2016</span></dt>
<dd><p>Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., … Shah, H. (2016). Wide &amp;amp; deep learning for recommender systems. <em>Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</em> (pp. 7–10). New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2988450.2988454">https://doi.org/10.1145/2988450.2988454</a>, <a class="reference external" href="https://doi.org/10.1145/2988450.2988454">doi:10.1145/2988450.2988454</a></p>
</dd>
<dt class="bibtex label" id="id4"><span class="brackets">Chu et al., 2011</span></dt>
<dd><p>Chu, W., Zinkevich, M., Li, L., Thomas, A., &amp; Tseng, B. (2011). Unbiased online active learning in data streams. <em>Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 195–203). New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2020408.2020444">https://doi.org/10.1145/2020408.2020444</a>, <a class="reference external" href="https://doi.org/10.1145/2020408.2020444">doi:10.1145/2020408.2020444</a></p>
</dd>
<dt class="bibtex label" id="ding2020efficient"><span class="brackets">Ding et al., 2020</span></dt>
<dd><p>Ding, Z., Yu, T., Huang, Y., Zhang, H., Li, G., Guo, Q., … Dong, H. (2020). Efficient reinforcement learning development with rlzoo. <em>arXiv preprint arXiv:2009.08644</em>.</p>
</dd>
<dt class="bibtex label" id="dwork2014algorithmic"><span class="brackets">Dwork &amp; Roth, 2014</span></dt>
<dd><p>Dwork, C., &amp; Roth, A. (2014). The algorithmic foundations of differential privacy. <em>Foundations and Trends in Theoretical Computer Science</em>, <em>9</em>(3–4), 211–407.</p>
</dd>
<dt class="bibtex label" id="erhan2009visualizing"><span class="brackets">Erhan et al., 2009</span></dt>
<dd><p>Erhan, D., Bengio, Y., Courville, A., &amp; Vincent, P. (2009). Visualizing higher-layer features of a deep network. <em>University of Montreal</em>, <em>1341</em>(3), 1.</p>
</dd>
<dt class="bibtex label" id="espeholt2019seed"><span class="brackets">Espeholt et al., 2019</span></dt>
<dd><p>Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., &amp; Michalski, M. (2019). Seed rl: scalable and efficient deep-rl with accelerated central inference. <em>arXiv preprint arXiv:1910.06591</em>.</p>
</dd>
<dt class="bibtex label" id="espeholt2018impala"><span class="brackets">Espeholt et al., 2018</span></dt>
<dd><p>Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., … others. (2018). Impala: scalable distributed deep-rl with importance weighted actor-learner architectures. <em>arXiv preprint arXiv:1802.01561</em>.</p>
</dd>
<dt class="bibtex label" id="id5"><span class="brackets">Fang et al., 2021</span></dt>
<dd><p>Fang, J., Yu, Y., Zhao, C., &amp; Zhou, J. (2021). Turbotransformers: an efficient gpu serving system for transformer models. <em>Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</em> (pp. 389–402). New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3437801.3441578">https://doi.org/10.1145/3437801.3441578</a>, <a class="reference external" href="https://doi.org/10.1145/3437801.3441578">doi:10.1145/3437801.3441578</a></p>
</dd>
<dt class="bibtex label" id="fetterly2009dryadlinq"><span class="brackets">Fetterly et al., 2009</span></dt>
<dd><p>Fetterly, Y. Y. M. I. D., Budiu, M., Erlingsson, Ú., &amp; Currey, P. K. G. J. (2009). Dryadlinq: a system for general-purpose distributed data-parallel computing using a high-level language. <em>Proc. LSDS-IR</em>, <em>8</em>.</p>
</dd>
<dt class="bibtex label" id="foerster2018counterfactual"><span class="brackets">Foerster et al., 2018</span></dt>
<dd><p>Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., &amp; Whiteson, S. (2018). Counterfactual multi-agent policy gradients. <em>Proceedings of the AAAI conference on artificial intelligence</em>.</p>
</dd>
<dt class="bibtex label" id="ginart2021mixed"><span class="brackets">Ginart et al., 2021</span></dt>
<dd><p>Ginart, A., Naumov, M., Mudigere, D., Yang, J., &amp; Zou, J. (2021). <em>Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems</em>.</p>
</dd>
<dt class="bibtex label" id="gong2020edgerec"><span class="brackets">Gong et al., 2020</span></dt>
<dd><p>Gong, Y., Jiang, Z., Feng, Y., Hu, B., Zhao, K., Liu, Q., &amp; Ou, W. (2020). Edgerec: recommender system on edge in mobile taobao. <em>Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em> (pp. 2477–2484).</p>
</dd>
<dt class="bibtex label" id="rmpygil"><span class="brackets">Gross, 2021</span></dt>
<dd><p>Gross, S. (2021). <em>Multithreaded Python without the GIL</em>. <span><a class="reference external" href="#"></a></span>https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0/edit#heading=h.kcngwrty1lv.</p>
</dd>
<dt class="bibtex label" id="ijcai2017-239"><span class="brackets">Guo et al., 2017</span></dt>
<dd><p>Guo, H., TANG, R., Ye, Y., Li, Z., &amp; He, X. (2017). Deepfm: a factorization-machine based neural network for ctr prediction. <em>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</em> (pp. 1725–1731). URL: <a class="reference external" href="https://doi.org/10.24963/ijcai.2017/239">https://doi.org/10.24963/ijcai.2017/239</a>, <a class="reference external" href="https://doi.org/10.24963/ijcai.2017/239">doi:10.24963/ijcai.2017/239</a></p>
</dd>
<dt class="bibtex label" id="han2020tstarbot"><span class="brackets">Han et al., 2020</span></dt>
<dd><p>Han, L., Xiong, J., Sun, P., Sun, X., Fang, M., Guo, Q., … others. (2020). Tstarbot-x: an open-sourced and comprehensive study for efficient league training in starcraft ii full game. <em>arXiv preprint arXiv:2011.13729</em>.</p>
</dd>
<dt class="bibtex label" id="neurips2020-a1d4c20b"><span class="brackets">He et al., 2020</span></dt>
<dd><p>He, C., Annavaram, M., &amp; Avestimehr, S. (2020). Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., &amp; Lin, H. (Eds.). Group knowledge transfer: federated learning of large cnns at the edge. <em>Advances in Neural Information Processing Systems</em> (pp. 14068–14080). Curran Associates, Inc. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf</a></p>
</dd>
<dt class="bibtex label" id="he2016deep"><span class="brackets">He et al., 2016</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.</p>
</dd>
<dt class="bibtex label" id="id6"><span class="brackets">He et al., 2014</span></dt>
<dd><p>He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., … Candela, J. Q. (2014). Practical lessons from predicting clicks on ads at facebook. <em>Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</em> (pp. 1–9). New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/2648584.2648589">https://doi.org/10.1145/2648584.2648589</a>, <a class="reference external" href="https://doi.org/10.1145/2648584.2648589">doi:10.1145/2648584.2648589</a></p>
</dd>
<dt class="bibtex label" id="hochreiter1997lstm"><span class="brackets">Hochreiter et al., 1997</span></dt>
<dd><p>Hochreiter, S., Hochreiter, S., Schmidhuber, J., &amp; Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–80.</p>
</dd>
<dt class="bibtex label" id="hoffman2020acme"><span class="brackets">Hoffman et al., 2020</span></dt>
<dd><p>Hoffman, M., Shahriari, B., Aslanides, J., Barth-Maron, G., Behbahani, F., Norman, T., … others. (2020). Acme: a research framework for distributed reinforcement learning. <em>arXiv preprint arXiv:2006.00979</em>.</p>
</dd>
<dt class="bibtex label" id="horgan2018distributed"><span class="brackets">Horgan et al., 2018</span></dt>
<dd><p>Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., &amp; Silver, D. (2018). <em>Distributed Prioritized Experience Replay</em>.</p>
</dd>
<dt class="bibtex label" id="fedavg-momentum"><span class="brackets">Hsu et al., 2019</span></dt>
<dd><p>Hsu, Tzu-Ming H., Qi, H., &amp; Brown, M. (2019). Measuring the effects of non-identical data distribution for federated visual classification. <em>CoRR</em>, <em>abs/1909.06335</em>. URL: <a class="reference external" href="http://arxiv.org/abs/1909.06335">http://arxiv.org/abs/1909.06335</a>, <a class="reference external" href="https://arxiv.org/abs/1909.06335">arXiv:1909.06335</a></p>
</dd>
<dt class="bibtex label" id="minddata"><span class="brackets">HuaWei, 2020</span></dt>
<dd><p>HuaWei (2020). <em>Dataset Plugin</em>. <span><a class="reference external" href="#"></a></span>https://gitee.com/mindspore/dataset-plugin.</p>
</dd>
<dt class="bibtex label" id="mlsys2021-ec895663"><span class="brackets">Jiang et al., 2021</span></dt>
<dd><p>Jiang, W., He, Z., Zhang, S., Preuß er, T. B., Zeng, K., Feng, L., … Alonso, G. (2021). Smola, A., Dimakis, A., &amp; Stoica, I. (Eds.). Microrec: efficient recommendation inference by hardware and data structure solutions. <em>Proceedings of Machine Learning and Systems</em> (pp. 845–859). URL: <a class="reference external" href="https://proceedings.mlsys.org/paper/2021/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf">https://proceedings.mlsys.org/paper/2021/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf</a></p>
</dd>
<dt class="bibtex label" id="jiang2022signds"><span class="brackets">Jiang et al., 2022</span></dt>
<dd><p>Jiang, X., Zhou, X., &amp; Grossklags, J. (2022). Signds-fl: local differentially private federated learning with sign-based dimension selection. <em>ACM Transactions on Intelligent Systems and Technology (TIST)</em>.</p>
</dd>
<dt class="bibtex label" id="scaffold"><span class="brackets">Karimireddy et al., 2020</span></dt>
<dd><p>Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., &amp; Suresh, A. T. (2020). Scaffold: stochastic controlled averaging for federated learning. <em>International Conference on Machine Learning</em> (pp. 5132–5143).</p>
</dd>
<dt class="bibtex label" id="kim2018interpretability"><span class="brackets">Kim et al., 2018</span></dt>
<dd><p>Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., &amp; Sayres, R. (2018). <em>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</em>.</p>
</dd>
<dt class="bibtex label" id="krizhevsky2012imagenet"><span class="brackets">Krizhevsky et al., 2012</span></dt>
<dd><p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. <em>Advances in Neural Information Processing Systems</em> (pp. 1097–1105).</p>
</dd>
<dt class="bibtex label" id="lanctot2017unified"><span class="brackets">Lanctot et al., 2017</span></dt>
<dd><p>Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Pérolat, J., … Graepel, T. (2017). A unified game-theoretic approach to multiagent reinforcement learning. <em>Advances in neural information processing systems</em>, <em>30</em>.</p>
</dd>
<dt class="bibtex label" id="lattner2020mlir"><span class="brackets">Lattner et al., 2020</span></dt>
<dd><p>Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar, J., … Zinenko, O. (2020). Mlir: a compiler infrastructure for the end of moore’s law. <em>arXiv preprint arXiv:2002.11054</em>.</p>
</dd>
<dt class="bibtex label" id="lecun2015deep"><span class="brackets">LeCun et al., 2015</span></dt>
<dd><p>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <em>Nature</em>, <em>521</em>(7553), 436.</p>
</dd>
<dt class="bibtex label" id="lecun1989backpropagation"><span class="brackets">LeCun et al., 1989</span></dt>
<dd><p>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &amp; Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. <em>Neural computation</em>, <em>1</em>(4), 541–551.</p>
</dd>
<dt class="bibtex label" id="fedprox"><span class="brackets">Li et al., 2020a</span></dt>
<dd><p>Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., &amp; Smith, V. (2020). Federated optimization in heterogeneous networks. <em>Proceedings of Machine Learning and Systems</em> (pp. 429–450).</p>
</dd>
<dt class="bibtex label" id="tkde-li"><span class="brackets">Li et al., 2020</span></dt>
<dd><p>Li, X.-H., Cao, C. C., Shi, Y., Bai, W., Gao, H., Qiu, L., … Chen, L. (2020). A survey of data-driven and knowledge-aware explainable ai. <em>IEEE Transactions on Knowledge and Data Engineering</em>, (), 1-1. <a class="reference external" href="https://doi.org/10.1109/TKDE.2020.2983930">doi:10.1109/TKDE.2020.2983930</a></p>
</dd>
<dt class="bibtex label" id="liang2017ray"><span class="brackets">Liang et al., 2017</span></dt>
<dd><p>Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Gonzalez, J., … Stoica, I. (2017). Ray rllib: a composable and scalable reinforcement learning library. <em>arXiv preprint arXiv:1712.09381</em>, p. 85.</p>
</dd>
<dt class="bibtex label" id="ascend"><span class="brackets">Liao et al., 2021</span></dt>
<dd><p>Liao, H., Tu, J., Xia, J., Liu, H., Zhou, X., Yuan, H., &amp; Hu, Y. (2021). Ascend: a scalable and unified architecture for ubiquitous deep neural network computing : industry track paper. <em>2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em> (pp. 789–801). <a class="reference external" href="https://doi.org/10.1109/HPCA51647.2021.00071">doi:10.1109/HPCA51647.2021.00071</a></p>
</dd>
<dt class="bibtex label" id="makoviychuk2021isaac"><span class="brackets">Makoviychuk et al., 2021</span></dt>
<dd><p>Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., … others. (2021). Isaac gym: high performance gpu-based physics simulation for robot learning. <em>arXiv preprint arXiv:2108.10470</em>.</p>
</dd>
<dt class="bibtex label" id="fedavg"><span class="brackets">McMahan et al., 2017</span></dt>
<dd><p>McMahan, B., Moore, E., Ramage, D., Hampson, S., &amp; y Arcas, B. A. (2017). Communication-efficient learning of deep networks from decentralized data. <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em> (pp. 1273–1282). PMLR. URL: <a class="reference external" href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a></p>
</dd>
<dt class="bibtex label" id="mcsherry2007mechanism"><span class="brackets">McSherry &amp; Talwar, 2007</span></dt>
<dd><p>McSherry, F., &amp; Talwar, K. (2007). Mechanism design via differential privacy. <em>IEEE Symposium on Foundations of Computer Science</em> (pp. 94–103).</p>
</dd>
<dt class="bibtex label" id="meijer2006linq"><span class="brackets">Meijer et al., 2006</span></dt>
<dd><p>Meijer, E., Beckman, B., &amp; Bierman, G. (2006). Linq: reconciling object, relations and xml in the. net framework. <em>Proceedings of the 2006 ACM SIGMOD international conference on Management of data</em> (pp. 706–706).</p>
</dd>
<dt class="bibtex label" id="mnih2016asynchronous"><span class="brackets">Mnih et al., 2016</span></dt>
<dd><p>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., … Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. <em>International Conference on Machine Learning (ICML)</em> (pp. 1928–1937).</p>
</dd>
<dt class="bibtex label" id="mnih2013playing"><span class="brackets">Mnih et al., 2013</span></dt>
<dd><p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing atari with deep reinforcement learning. <em>arXiv preprint arXiv:1312.5602</em>.</p>
</dd>
<dt class="bibtex label" id="mohan2020analyzing"><span class="brackets">Mohan et al., 2020</span></dt>
<dd><p>Mohan, J., Phanishayee, A., Raniwala, A., &amp; Chidambaram, V. (2020). Analyzing and mitigating data stalls in dnn training. <em>arXiv preprint arXiv:2007.06775</em>.</p>
</dd>
<dt class="bibtex label" id="moritz2018ray"><span class="brackets">Moritz et al., 2018</span></dt>
<dd><p>Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., … others. (2018). Ray: a distributed framework for emerging $\$AI$\$ applications. <em>13th $\$USENIX$\$ Symposium on Operating Systems Design and Implementation ($\$OSDI$\$ 18)</em> (pp. 561–577).</p>
</dd>
<dt class="bibtex label" id="zionex"><span class="brackets">Mudigere et al., 2021</span></dt>
<dd><p>Mudigere, D., Hao, Y., Huang, J., Jia, Z., Tulloch, A., Sridharan, S., … others. (2021). Software-hardware co-design for fast and scalable training of deep learning recommendation models. <em>arXiv preprint arXiv:2104.05158</em>.</p>
</dd>
<dt class="bibtex label" id="murray2013naiad"><span class="brackets">Murray et al., 2013</span></dt>
<dd><p>Murray, D. G., McSherry, F., Isaacs, R., Isard, M., Barham, P., &amp; Abadi, M. (2013). Naiad: a timely dataflow system. <em>Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</em> (pp. 439–455).</p>
</dd>
<dt class="bibtex label" id="murray2021tf"><span class="brackets">Murray et al., 2021</span></dt>
<dd><p>Murray, D. G., Simsa, J., Klimovic, A., &amp; Indyk, I. (2021). Tf. data: a machine learning data processing framework. <em>arXiv preprint arXiv:2101.12127</em>.</p>
</dd>
<dt class="bibtex label" id="naumov2019deep"><span class="brackets">Naumov et al., 2019</span></dt>
<dd><p>Naumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sundaraman, N., Park, J., … others. (2019). Deep learning recommendation model for personalization and recommendation systems. <em>arXiv preprint arXiv:1906.00091</em>.</p>
</dd>
<dt class="bibtex label" id="nvidia"><span class="brackets">NVIDIA, 2017</span></dt>
<dd><p>NVIDIA (2017). <em>NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU</em>. <span><a class="reference external" href="#"></a></span>http://www.nvidia.com/object/volta-architecture-whitepaper.html.</p>
</dd>
<dt class="bibtex label" id="nvidia-dali"><span class="brackets">NVIDIA, 2018</span></dt>
<dd><p>NVIDIA (2018). <em>DALI</em>. <span><a class="reference external" href="#"></a></span>https://github.com/NVIDIA/DALI.</p>
</dd>
<dt class="bibtex label" id="hugectr"><span class="brackets">NVIDIA, 2022a</span></dt>
<dd><p>NVIDIA (2022). <em>NVIDIA HugeCTR</em>. Accessed on 2022-03-24.</p>
</dd>
<dt class="bibtex label" id="merlin"><span class="brackets">NVIDIA, 2022</span></dt>
<dd><p>NVIDIA (2022). <em>NVIDIA Merlin</em>. Accessed on 2022-03-24.</p>
</dd>
<dt class="bibtex label" id="nvtabular"><span class="brackets">NVIDIA, 2022c</span></dt>
<dd><p>NVIDIA (2022). <em>NVIDIA NVTabular</em>. Accessed on 2022-03-24.</p>
</dd>
<dt class="bibtex label" id="triton"><span class="brackets">NVIDIA, 2022d</span></dt>
<dd><p>NVIDIA (2022). <em>NVIDIA Triton</em>. Accessed on 2022-03-24.</p>
</dd>
<dt class="bibtex label" id="pate"><span class="brackets">Papernot et al., 2017</span></dt>
<dd><p>Papernot, N., Abadi, Mart’ın, Erlingsson, Ú., Goodfellow, I. J., &amp; Talwar, K. (2017). Semi-supervised knowledge transfer for deep learning from private training data. <em>5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings</em>. OpenReview.net.</p>
</dd>
<dt class="bibtex label" id="ragan2013halide"><span class="brackets">Ragan-Kelley et al., 2013</span></dt>
<dd><p>Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., &amp; Amarasinghe, S. (2013). Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. <em>Acm Sigplan Notices</em>, <em>48</em>(6), 519–530.</p>
</dd>
<dt class="bibtex label" id="modeling"><span class="brackets">Raihan et al., 2018</span></dt>
<dd><p>Raihan, M. A., Goli, N., &amp; Aamodt, T. (2018). Modeling deep learning accelerator enabled gpus. <em>arXiv e-prints arXiv:1811.08309</em>.</p>
</dd>
<dt class="bibtex label" id="rashid2018qmix"><span class="brackets">Rashid et al., 2018</span></dt>
<dd><p>Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., &amp; Whiteson, S. (2018). Qmix: monotonic value function factorisation for deep multi-agent reinforcement learning. <em>International Conference on Machine Learning</em> (pp. 4295–4304).</p>
</dd>
<dt class="bibtex label" id="riedl2019human"><span class="brackets">Riedl, 2019</span></dt>
<dd><p>Riedl, M. O. (2019). Human-centered artificial intelligence and machine learning. <em>Human Behavior and Emerging Technologies</em>, <em>1</em>(1), 33–36.</p>
</dd>
<dt class="bibtex label" id="rosenblatt1958perceptron"><span class="brackets">Rosenblatt, 1958</span></dt>
<dd><p>Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. <em>Psychological Review</em>, <em>65</em>(6), 386.</p>
</dd>
<dt class="bibtex label" id="rumelhart1986learning"><span class="brackets">Rumelhart et al., 1986</span></dt>
<dd><p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, <em>323</em>(6088), 533.</p>
</dd>
<dt class="bibtex label" id="nips2015-86df7dcf"><span class="brackets">Sculley et al., 2015</span></dt>
<dd><p>Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., … Dennison, D. (2015). Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., &amp; Garnett, R. (Eds.). Hidden technical debt in machine learning systems. <em>Advances in Neural Information Processing Systems</em> (p. ). Curran Associates, Inc. URL: <a class="reference external" href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</a></p>
</dd>
<dt class="bibtex label" id="id7"><span class="brackets">Shi et al., 2020</span></dt>
<dd><p>Shi, H.-J. M., Mudigere, D., Naumov, M., &amp; Yang, J. (2020). Compositional embeddings using complementary partitions for memory-efficient recommendation systems. <em>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining</em> (pp. 165–175). New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3394486.3403059">https://doi.org/10.1145/3394486.3403059</a>, <a class="reference external" href="https://doi.org/10.1145/3394486.3403059">doi:10.1145/3394486.3403059</a></p>
</dd>
<dt class="bibtex label" id="sunehag2017value"><span class="brackets">Sunehag et al., 2017</span></dt>
<dd><p>Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., … others. (2017). Value-decomposition networks for cooperative multi-agent learning. <em>arXiv preprint arXiv:1706.05296</em>.</p>
</dd>
<dt class="bibtex label" id="id8"><span class="brackets">Tian et al., 2018</span></dt>
<dd><p>Tian, H., Yu, M., &amp; Wang, W. (2018). Continuum: a platform for cost-aware, low-latency continual learning. <em>Proceedings of the ACM Symposium on Cloud Computing</em> (pp. 26–40). New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3267809.3267817">https://doi.org/10.1145/3267809.3267817</a>, <a class="reference external" href="https://doi.org/10.1145/3267809.3267817">doi:10.1145/3267809.3267817</a></p>
</dd>
<dt class="bibtex label" id="vasilache2022composable"><span class="brackets">Vasilache et al., 2022</span></dt>
<dd><p>Vasilache, N., Zinenko, O., Bik, A. J., Ravishankar, M., Raoux, T., Belyaev, A., … others. (2022). Composable and modular code generation in mlir: a structured and retargetable approach to tensor compiler construction. <em>arXiv preprint arXiv:2202.03293</em>.</p>
</dd>
<dt class="bibtex label" id="vaswani2017attention"><span class="brackets">Vaswani et al., 2017</span></dt>
<dd><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em> (pp. 5998–6008).</p>
</dd>
<dt class="bibtex label" id="verdoolaege2010isl"><span class="brackets">Verdoolaege, 2010</span></dt>
<dd><p>Verdoolaege, S. (2010). Isl: an integer set library for the polyhedral model. <em>International Congress on Mathematical Software</em> (pp. 299–302).</p>
</dd>
<dt class="bibtex label" id="vinyals2019grandmaster"><span class="brackets">Vinyals et al., 2019</span></dt>
<dd><p>Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., … others. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement learning. <em>Nature</em>, <em>575</em>(7782), 350–354.</p>
</dd>
<dt class="bibtex label" id="id9"><span class="brackets">Wang et al., 2017</span></dt>
<dd><p>Wang, R., Fu, B., Fu, G., &amp; Wang, M. (2017). Deep &amp;amp; cross network for ad click predictions. <em>Proceedings of the ADKDD‘17</em>. New York, NY, USA: Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3124749.3124754">https://doi.org/10.1145/3124749.3124754</a>, <a class="reference external" href="https://doi.org/10.1145/3124749.3124754">doi:10.1145/3124749.3124754</a></p>
</dd>
<dt class="bibtex label" id="wang2021scc"><span class="brackets">Wang et al., 2021</span></dt>
<dd><p>Wang, X., Song, J., Qi, P., Peng, P., Tang, Z., Zhang, W., … others. (2021). Scc: an efficient deep reinforcement learning agent mastering the game of starcraft ii. <em>International Conference on Machine Learning</em> (pp. 10905–10915).</p>
</dd>
<dt class="bibtex label" id="wang-etal-2021-lightseq"><span class="brackets">Wang et al., 2021</span></dt>
<dd><p>Wang, X., Xiong, Y., Wei, Y., Wang, M., &amp; Li, L. (2021 , June). LightSeq: a high performance inference library for transformers. <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</em> (pp. 113–120). Online: Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2021.naacl-industry.15">https://aclanthology.org/2021.naacl-industry.15</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2021.naacl-industry.15">doi:10.18653/v1/2021.naacl-industry.15</a></p>
</dd>
<dt class="bibtex label" id="id10"><span class="brackets">Xie et al., 2020</span></dt>
<dd><p>Xie, M., Ren, K., Lu, Y., Yang, G., Xu, Q., Wu, B., … Shu, J. (2020). Kraken: memory-efficient continual learning for large-scale real-time recommendations. <em>SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em> (pp. 1–17). <a class="reference external" href="https://doi.org/10.1109/SC41405.2020.00025">doi:10.1109/SC41405.2020.00025</a></p>
</dd>
<dt class="bibtex label" id="mlsys2021-979d472a"><span class="brackets">Yin et al., 2021</span></dt>
<dd><p>Yin, C., Acun, B., Wu, C.-J., &amp; Liu, X. (2021). Smola, A., Dimakis, A., &amp; Stoica, I. (Eds.). Tt-rec: tensor train compression for deep learning recommendation models. <em>Proceedings of Machine Learning and Systems</em> (pp. 448–462). URL: <a class="reference external" href="https://proceedings.mlsys.org/paper/2021/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf">https://proceedings.mlsys.org/paper/2021/file/979d472a84804b9f647bc185a877a8b5-Paper.pdf</a></p>
</dd>
<dt class="bibtex label" id="zaharia2010spark"><span class="brackets">Zaharia et al., 2010</span></dt>
<dd><p>Zaharia, M., Chowdhury, M., Franklin, M. J., Shenker, S., &amp; Stoica, I. (2010). Spark: cluster computing with working sets. <em>2nd USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 10)</em>.</p>
</dd>
<dt class="bibtex label" id="zhao2021akg"><span class="brackets">Zhao et al., 2021</span></dt>
<dd><p>Zhao, J., Li, B., Nie, W., Geng, Z., Zhang, R., Gao, X., … others. (2021). Akg: automatic kernel generation for neural processing units using polyhedral transformations. <em>Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</em> (pp. 1233–1248).</p>
</dd>
<dt class="bibtex label" id="mlsys2020-f7e6c855"><span class="brackets">Zhao et al., 2020</span></dt>
<dd><p>Zhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., &amp; Li, P. (2020). Dhillon, I., Papailiopoulos, D., &amp; Sze, V. (Eds.). Distributed hierarchical gpu parameter server for massive scale deep learning ads systems. <em>Proceedings of Machine Learning and Systems</em> (pp. 412–428). URL: <a class="reference external" href="https://proceedings.mlsys.org/paper/2020/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf">https://proceedings.mlsys.org/paper/2020/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf</a></p>
</dd>
<dt class="bibtex label" id="zheng2020ansor"><span class="brackets">Zheng et al., 2020</span></dt>
<dd><p>Zheng, L., Jia, C., Sun, M., Wu, Z., Yu, C. H., Haj-Ali, A., … others. (2020). Ansor: generating $\$High-Performance$\$ tensor programs for deep learning. <em>14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</em> (pp. 863–879).</p>
</dd>
</dl>
</p>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="planning.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>17.4. 规划系统</div>
         </div>
     </a>
     <a id="button-next" href="summary.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>17.6. 小结</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>