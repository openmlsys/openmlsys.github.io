<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>7.3. 加速器基本编程原理 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.4. 总结" href="summary.html" />
    <link rel="prev" title="7.2. 加速器基本组成原理" href="accelerator_architecture.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">7. </span>硬件加速器</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">7.3. </span>加速器基本编程原理</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_accelerator/accelerator_programming.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">4. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">5. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">5.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">5.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">5.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">5.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">6. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">6.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">6.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">6.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">6.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">6.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. 硬件加速器</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="accelerator_introduction.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_architecture.html">7.2. 加速器基本组成原理</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">7.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">8. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">8.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">8.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">8.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">8.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">8.6. 章节总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">9. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">9.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">9.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">9.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">9.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">10. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">10.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">10.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">10.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">10.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">10.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">10.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">11. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">12. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">12.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">12.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">12.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">12.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">12.5. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">13. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">13.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/system_architecture.html">13.2. 系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/fedavg.html">13.3. 联邦平均算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">13.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/challenge.html">13.5. 实际部署时的挑战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">13.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">14. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">14.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">14.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">14.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">14.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">14.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">15. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">15.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">15.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">15.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">15.4. 未来可解释AI</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">4. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">5. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">5.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">5.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">5.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">5.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">6. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">6.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">6.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">6.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">6.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">6.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. 硬件加速器</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="accelerator_introduction.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_architecture.html">7.2. 加速器基本组成原理</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">7.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">8. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">8.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">8.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">8.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">8.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">8.6. 章节总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">9. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">9.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">9.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">9.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">9.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">10. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">10.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">10.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">10.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">10.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">10.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">10.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">11. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">12. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">12.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">12.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">12.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">12.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">12.5. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">13. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">13.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/system_architecture.html">13.2. 系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/fedavg.html">13.3. 联邦平均算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">13.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/challenge.html">13.5. 实际部署时的挑战</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">13.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">14. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">14.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">14.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">14.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">14.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">14.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">15. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">15.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">15.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">15.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id8">15.4. 未来可解释AI</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/index.html">参考文献</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="accelerator-program-title">
<span id="id1"></span><h1><span class="section-number">7.3. </span>加速器基本编程原理<a class="headerlink" href="#accelerator-program-title" title="Permalink to this headline">¶</a></h1>
<p>本章前两节主要介绍了硬件加速器设计的意义、思路以及基本组成原理。软硬件协同优化作为构建高效AI系统的一个重要指导思想，需要软件算法/软件栈和硬件架构在神经网络应用中互相影响、紧密耦合。为了最大限度地发挥加速器的优势，要求能够基于硬件系统架构提供易用、高效的编程方法。因此，在本节中将着重介绍加速器的可编程性，包括编程接口直接调用方式及算子编译器优化方式。最后，通过示例介绍如何通过编程使能加速器，提升神经网络算子的计算效率。</p>
<section id="accelerator-programable-title">
<span id="id2"></span><h2><span class="section-number">7.3.1. </span>硬件加速器的可编程性<a class="headerlink" href="#accelerator-programable-title" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="accelerator_introduction.html#accelerator-design-title"><span class="std std-numref">7.1.2节</span></a>节中列出的硬件加速器均具有一定的可编程性，程序员可以通过软件编程，有效的使能上述加速器进行计算加速。现有硬件加速器常见的两类编程方式主要有编程接口调用以及算子编译器优化。</p>
<section id="id3">
<h3><span class="section-number">7.3.1.1. </span>编程接口使能加速器<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>硬件加速器出于计算效率和易用性等方面考虑，将编程接口使能方式分为不同等级，一般包括：算子库层级，编程原语层级，以及指令层级。为了更具象的解释上述层级的区别，我们以Volta架构的Tensor
Core加速器为例，由高层至底层对比介绍这三种不同编程方式：</p>
<ul class="simple">
<li><p><strong>算子库层级</strong>：如cuBLAS基本矩阵与向量运算库，cuDNN深度学习加速库，均通过Host端调用算子库提供的核函数使能TensorCore；</p></li>
<li><p><strong>编程原语层级</strong>：如基于CUDA的WMMA
API编程接口。同算子库相比，需要用户显式调用计算各流程，如矩阵存取至TensorCore、TensorCore执行矩阵乘累加运算、TensorCore累加矩阵数据初始化操作等；</p></li>
<li><p><strong>指令层级</strong>：如PTX ISA
MMA指令集，提供更细粒度的mma指令，便于用户组成更多种形状的接口，通过CUDA
Device端内联编程使能TensorCore。</p></li>
</ul>
</section>
<section id="id4">
<h3><span class="section-number">7.3.1.2. </span>算子编译器使能加速器<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>DSA架构的多维度AI加速器通常提供了更多的指令选择（3D-Matrix/2D-Vector/1D-Scalar），以及更加复杂的数据流处理，通过提供接口调用的方式对程序开发人员带来较大的挑战。此外，由于调度、切分的复杂度增加，直接提供算子库的方式由于缺少根据目标shape调优的能力，往往无法在所有shape下均得到最优的性能。因此，对于DSA加速器，业界通常采用算子编译器的解决方案。</p>
<p>随着深度学习模型的迭代更新及各类AI芯片的层出不穷，基于人工优化算子的方式给算子开发团队带来沉重的负担。因此，开发一种能够将High-level的算子表示编译成目标硬件可执行代码的算子编译器，逐渐成为学术界及工业界的共识。算子编译器前端通常提供了特定领域描述语言（DSL），用于定义算子的计算范式；类似于传统编译器，算子编译器也会将算子计算表示转换为中间表示，如HalideIR
<a class="bibtex reference internal" href="../chapter_references/index.html#ragan2013halide" id="id5">[Ragan-Kelley et al., 2013]</a>、TVM <a class="bibtex reference internal" href="../chapter_references/index.html#chen2018tvm" id="id6">[Chen et al., 2018]</a>的TIR、Schedule
Tree
<a class="bibtex reference internal" href="../chapter_references/index.html#verdoolaege2010isl" id="id7">[Verdoolaege, 2010]</a>等，基于模板（手动）、搜索算法或优化求解算法（自动）等方式完成循环变换、循环切分等调度相关优化，以及硬件指令映射、内存分配、指令流水等后端pass优化，最后通过codegen模块将IR转换为DSA加速器可执行的kernel。</p>
<p>当前业界的算子编译器/编译框架主要有TVM/Ansor
<a class="bibtex reference internal" href="../chapter_references/index.html#zheng2020ansor" id="id8">[Zheng et al., 2020]</a>、MLIR
<a class="bibtex reference internal" href="../chapter_references/index.html#lattner2020mlir" id="id9">[Lattner et al., 2020b]</a>、以及华为Ascend芯片上的TBE/AKG
<a class="bibtex reference internal" href="../chapter_references/index.html#zhao2021akg" id="id10">[Zhao et al., 2021]</a>等。</p>
<ul class="simple">
<li><p><strong>TVM/Ansor</strong></p></li>
</ul>
<figure class="align-default" id="id19">
<span id="tbe"></span><span id="tvm"></span><a class="reference internal image-reference" href="../_images/TVM.svg"><img alt="../_images/TVM.svg" src="../_images/TVM.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.1 </span><span class="caption-text">TVM</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>TVM是陈天奇博士等人开发的开源深度学习编译框架，提供了端到端的编译优化（图优化/算子优化）能力，在工业界应用较广。在架构上，主要包括relay和tir两层。通过relay导入推理模型，进行算子融合等图层优化，通过tir生成融合算子。在算子编译方面，TVM采用了计算和调度分离的技术，为不同的算子提供了不同的模板，同时支持自定义模板，优化特定算子类型调度。为了更进一步优化算子性能，TVM支持对算子进行自动tuning，来生成较优的切分参数。此外，为了简化用户开发模板的工作，TVM在0.8版本后提供了自动调度能力Ansor，通过搜索的方式，为目标算子生成调度及切分参数。</p>
<ul class="simple">
<li><p><strong>MLIR</strong>
前面的章节介绍过，Google开发的MLIR并不是一个单一的算子编译器，而是一套编译器基础设施，提供了工具链的组合与复用能力。基于MLIR，DSA加速器厂商可以快速的搭建其定制化算子编译器。如Google论文
<a class="bibtex reference internal" href="../chapter_references/index.html#vasilache2022composable" id="id11">[Vasilache et al., 2022]</a>中所述，当前的算子编译器大多提供了一整套自顶向下的编译优化pass，包括调度优化、切分优化、窥孔优化、后端优化、指令生成等，彼此之间大多无法复用，导致新的场景中通常又得从头开发。而在MLIR中，将功能相近的IR优化pass封装为方言（Dialect），并且提供了多个代码生成相关的基础方言，如vector、memref、tensor、scf、affine、linalg等。硬件厂商可以基于这些Dialect，快速构建一整套lower优化及codegen流程。如下图所示，利用scf、affine、linalg等方言，对结构化的计算IR完成循环并行优化、切分、向量化等，最后基于LLVM完成指令映射。</p></li>
<li><p><strong>华为TBE/AKG</strong> TBE（Tensor Boost
Engine）是华为的Ascend芯片及其CANN软件栈基于TVM
开发的一套算子编译优化工具，用于对Ascend芯片进行调度优化、指令映射、及后端pass优化等。不仅提供了一个优化过的神经网络标准算子库，同时还提供了算子开发能力及融合能力。通过TBE提供的API和自定义算子编程开发界面可以完成相应神经网络算子的开发，帮助用户较容易的去使能硬件加速器上的AI_CORE
相关指令，以实现高性能的神经网络计算。为了简化算子开发流程，TBE还实现了一个Auto
Schedule工具，开放了自定义算子编程DSL，用于自动完成复杂算子的调度生成。此外，TBE还实现了端到端的动态shape算子编译能力。
<img alt="TBE" src="../_images/tbe.png" /></p>
<dl class="field-list simple">
<dt class="field-odd">width</dt>
<dd class="field-odd"><p>800px</p>
</dd>
</dl>
</li>
</ul>
<p>AKG则是MindSpore社区的开源算子编译工具。与上述介绍的算子编译器不同，AKG基于Polyhedral多面体编译技术
<a class="bibtex reference internal" href="../chapter_references/index.html#bastoul2004code" id="id12">[Bastoul, 2004]</a>，支持在CPU/GPU/Ascend多硬件上自动生成满足并行性与数据局部性的调度。Polyhedral编译技术的核心思想是将程序中循环的迭代空间映射为高维空间多面体，通过分析语句读写依赖关系，将循环调度优化问题转换为整数规划求解问题。
AKG的编译流程如下图所示，主要包含程序规范化、自动调度优化、指令生成、后端优化几个模块。AKG同样基于TVM实现，支持TVM
compute/Hybrid
DSL编写的算子表示，以及MindSpore图算融合模块优化后的融合子图。通过IR规范化，将DSL/子图IR转换为polyhedral编译的调度树。在polyhedral模块中，利用其提供的调度算法，实现循环的自动融合、自动重排等变换，为融合算子自动生成满足并行性、数据局部性的初始调度。为了能够快速适配不同的硬件后端，我们在poly模块内将优化pass识别为硬件无关的通用优化与硬件相关的特定优化，编译时按照硬件特征拼接组合，实现异构硬件后端的快速适配。</p>
<figure class="align-default" id="id20">
<span id="akg"></span><a class="reference internal image-reference" href="../_images/akg.png"><img alt="../_images/akg.png" src="../_images/akg.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">图7.3.2 </span><span class="caption-text">AKG</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>在polyhedral模块中，实现了算子的自动调度生成、自动切分以及自动数据搬移。为了进一步提升算子的性能，我们针对不同硬件后端开发了相应的优化pass，如Ascend后端中实现数据对齐、指令映射，GPU后端中实现向量化存取，插入同步指令等，最终生成相应平台代码。</p>
</section>
</section>
<section id="diversified-programming-title">
<span id="id13"></span><h2><span class="section-number">7.3.2. </span>硬件加速器的多样化编程方法<a class="headerlink" href="#diversified-programming-title" title="Permalink to this headline">¶</a></h2>
<p>矩阵乘法运算作为深度学习网络中占比最大的计算，对其进行优化是十分必要的。因此本节将统一以矩阵乘法<span class="math notranslate nohighlight">\(D[M, N] = C[M, N] + A[M, K] \times B[K, N]\)</span>为实例，对比介绍如何通过不同编程方式使能加速器。</p>
<figure class="align-default" id="id21">
<span id="id14"></span><span id="gemm-algorith"></span><a class="reference internal image-reference" href="../_images/gemm.svg"><img alt="../_images/gemm.svg" src="../_images/gemm.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.3 </span><span class="caption-text">矩阵乘法GEMM运算</span><a class="headerlink" href="#id21" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="id15">
<h3><span class="section-number">7.3.2.1. </span>编程接口使能加速器<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>算子库层级</strong>
在上述不同层级的编程方式中，直接调用算子加速库使能加速器无疑是最快捷高效的方式。NVIDIA提供了cuBLAS/cuDNN两类算子计算库，cuBLAS提供了使能Tensor
Core单元的接口，用以加速矩阵乘法(GEMM)运算，cuDNN提供了对应接口加速卷积(CONV)运算等。
以
<a class="reference internal" href="#accelerator-programable-title"><span class="std std-numref">7.3.1节</span></a>小节的GEMM运算为例，与常规CUDA调用cuBLAS算子库相似，通过cuBLAS加速库使能Tensor
Core步骤包括：</p></li>
</ul>
<ol class="arabic simple">
<li><p>创建cuBLAS对象句柄且设置对应数学计算模式</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">cublasHandle_t</span><span class="w"> </span><span class="n">handle</span><span class="p">;</span><span class="w"></span>
<span class="n">cublasStatus_t</span><span class="w"> </span><span class="n">cublasStat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span><span class="w"></span>
<span class="n">cublasStat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cublasSetMathMode</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">CUBLAS_TENSOR_OP_MATH</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>分配和初始化矩阵内存空间及内容元素</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">size_t</span><span class="w"> </span><span class="n">matrixSizeA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="p">)</span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="p">;</span><span class="w"></span>
<span class="n">cublasStat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">devPtrA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">matrixSizeA</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">devPtrA</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]));</span><span class="w"></span>
<span class="n">cublasStat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cublasSetMatrix</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">devPtrA</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">M</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>调用对应计算函数接口</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">cublasStat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cublasGemmEx</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">transa</span><span class="p">,</span><span class="w"> </span><span class="n">transb</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"></span>
<span class="w">                          </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA_R_16F</span><span class="p">,</span><span class="w"> </span><span class="n">lda</span><span class="p">,</span><span class="w"></span>
<span class="w">                          </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA_R_16F</span><span class="p">,</span><span class="w"> </span><span class="n">ldb</span><span class="p">,</span><span class="w"></span>
<span class="w">                          </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA_R_16F</span><span class="p">,</span><span class="w"> </span><span class="n">ldc</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA_R_32F</span><span class="p">,</span><span class="w"> </span><span class="n">algo</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>传回结果数据</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">cublasStat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cublasGetMatrix</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="w"> </span><span class="n">devPtrD</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>释放内存和对象句柄</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">cudaFree</span><span class="p">(</span><span class="n">devPtrA</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaDestroy</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>当然，由于加速器一般会有矩阵形状、数据类型、排布方式等限制，因此在调用句柄和函数接口时要多加注意。如本例中，cuBLAS计算模式必须设置为<span class="math notranslate nohighlight">\(CUBLAS\_TENSOR\_OP\_MATH\)</span>，步长必须设置为8的倍数，输入数据类型必须为<span class="math notranslate nohighlight">\(CUDA\_R\_16F\)</span>等。按照如上方式即可通过cuBLAS算子库对
<a class="reference internal" href="#accelerator-programable-title"><span class="std std-numref">7.3.1节</span></a>实例使能Tensor
Core加速器，通过NVIDIA官方数据可知，该方式对于不同矩阵乘法计算规模，平均有4～10倍的提升，且矩阵规模越大，加速器提升效果越明显。</p>
<p>该方式由于能够隐藏体系结构细节，易用性较好，且一般官方提供的算子库吞吐量较高。但与此同时，这种算子颗粒度的库也存在一些问题，如不足以应对复杂多变的网络模型导致的算子长尾问题（虽然常规形式算子占据绝大多数样本，但仍有源源不断的新增算子，因其出现机会较少，算子库未对其进行有效优化。），以及错失了较多神经网络框架优化（如算子融合）的机会。</p>
<ul class="simple">
<li><p><strong>编程原语层级</strong>
第二种加速器编程方式为编程原语使能加速器，如通过在Device端调用CUDA
WMMA (Warp Matrix Multiply Accumulate)
API接口。以线程束（即Warp，是调度的基本单位）为操纵对象，使能多个Tensor
Core单元。该方式在CUDA
9.0中被公开，程序员可通过添加API头文件的引用和命名空间定义来使用上述API接口。基于软硬件协同设计的基本思想，该层级编程API的设计多与架构绑定，如WMMA操纵的总是<span class="math notranslate nohighlight">\(16\times16\)</span>大小的矩阵块，并且操作一次跨两个TensorCore进行处理，本质是与TensorCore如何集成进SM中强相关的。针对Float16输入数据类型，NVIDIA官方提供了三种不同矩阵规模的WMMA乘累加计算接口，分别为<span class="math notranslate nohighlight">\(16\times16\times16\)</span>，<span class="math notranslate nohighlight">\(32\times8\times16\)</span>，<span class="math notranslate nohighlight">\(8\times32\times16\)</span>。
该API接口操纵的基本单位为Fragment，是一种指明了矩阵含义（乘法器/累加器）、矩阵形状（<span class="math notranslate nohighlight">\(WMMA\_M, WMMA\_N, WMMA\_K\)</span>）、数据类型（Half/
Float）、排布方式（<span class="math notranslate nohighlight">\(row\_major/ col\_major\)</span>）等信息的模板类型，包括如下：</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">a_frag</span><span class="p">;</span><span class="w"></span>
<span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">b_frag</span><span class="p">;</span><span class="w"></span>
<span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">acc_frag</span><span class="p">;</span><span class="w"></span>
<span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">c_frag</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>使用时，我们需要将待执行乘法操作矩阵块的数据，作为Fragment，由寄存器加载至TensorCore，在将累加Fragment初始化/清零操作后，通过TensorCore单元执行乘累加运算，最后将运算结果的Fragment存回寄存器或其他内存区域。与上述操作对应的，NVIDIA提供了<span class="math notranslate nohighlight">\(wmma.load\_matrix\_sync(), wmma.store\_matrix\_sync()\)</span>接口用于将参与计算的子矩阵块写入/载出Fragment片段；<span class="math notranslate nohighlight">\(wmma.fill\_fragment()\)</span>接口用于初始化对应Fragment的数据；<span class="math notranslate nohighlight">\(wmma.mma\_sync()\)</span>接口用于对Fragment进行乘累加运算。</p>
<ul class="simple">
<li><p><strong>指令层级</strong></p></li>
</ul>
<p>在NVIDIA PTX ISA (Instruction Set
Architecture)中提供了另一个编程接口，如Volta架构中的<span class="math notranslate nohighlight">\(mma.sync.m8n8k4\)</span>指令，它使用<span class="math notranslate nohighlight">\(M=8, N=8, K=4\)</span>的形状配置执行乘累加操作。具体地，它由线程组（黑色椭圆表示）或octet执行
<a class="bibtex reference internal" href="../chapter_references/index.html#modeling" id="id16">[Raihan et al., 2018]</a>，如
<a class="reference internal" href="#ptx"><span class="std std-numref">图7.3.4</span></a>显示了线程和数据的映射关系。每个线程组由四个连续的线程组成，使用不同颜色的圆圈表示。图中还指出了一个octet里面的线程在线程束内的分布，Float16乘法器A或B的四个连续元素（使用具有相同颜色的块表示），以及Float32累加器C或D的八个分散元素（同样使用相同颜色的块表示）。彩色块上的数字代表对应的线程ID。</p>
<figure class="align-default" id="id22">
<span id="ptx"></span><a class="reference internal image-reference" href="../_images/ptx.svg"><img alt="../_images/ptx.svg" src="../_images/ptx.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.4 </span><span class="caption-text">mma指令之线程与矩阵元素映射关系</span><a class="headerlink" href="#id22" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>作为一个更细粒度的指令，mma可以组成更加多样化形状的Warp范围的WMMA
API接口，可以控制线程束内线程与数据的映射关系，并允许AI编译器自动/手动显式地管理内存层次结构之间的矩阵分解，因此相比于直接应用NVCUDA::WMMA
API具有更好的灵活性。</p>
</section>
<section id="id17">
<h3><span class="section-number">7.3.2.2. </span>算子编译器编程使能加速器<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>基于算子编译器使能加速器实现矩阵乘的流程则对用户更加友好。以在Ascend中使用TBE为例，用户只需基于python定义矩阵乘的tensor信息（数据类型及形状等），调用对应TBE接口即可。如下所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">b_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="p">)</span>
<span class="n">in_dtype</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span>
<span class="n">dst_dtype</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span>
<span class="n">tensor_a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">a_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tensor_a&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">in_dtype</span><span class="p">)</span>
<span class="n">tensor_b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">b_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tensor_b&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">in_dtype</span><span class="p">)</span>
<span class="n">tensor_bias</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tensor_bias&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dst_dtype</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="o">=</span><span class="n">dst_dtype</span><span class="p">,</span> <span class="n">tensor_bias</span><span class="o">=</span><span class="n">tensor_bias</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id18">
<h2><span class="section-number">7.3.3. </span>硬件加速器高性能编程实例<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h2>
<p>本节
<a class="reference internal" href="#accelerator-program-title"><span class="std std-numref">7.3节</span></a>前几个小节主要介绍了硬件加速器的不同层级的多样化编程方法。调用计算库的方式留给程序员的优化空间较少，合理利用硬件加速器不同层级的编程，可以实现更好的性能优化。
为了更好的让读者理解硬件加速器的使用，本节会继续
<a class="reference internal" href="#accelerator-programable-title"><span class="std std-numref">7.3.1节</span></a>节中的GEMM运算，仍以WMMA
API使能Tensor
Core加速单元为例，介绍如何通过矩阵分块、资源映射等方式更高效的利用硬件加速器。</p>
<figure class="align-default" id="id23">
<span id="gemm-tensor-core-algorith"></span><a class="reference internal image-reference" href="../_images/gemm_tensor_core.svg"><img alt="../_images/gemm_tensor_core.svg" src="../_images/gemm_tensor_core.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.5 </span><span class="caption-text">TensorCore矩阵乘法GEMM运算</span><a class="headerlink" href="#id23" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>若要得到高性能CUDA程序，提高并行性、增大吞吐量、优化指令执行是至关重要的三个优化目标。针对该实例，具体地实现和优化方案列出如下，对应到具体实例伪代码如
<a class="reference internal" href="#gemm-tensor-core-algorith"><span class="std std-numref">图7.3.5</span></a>所示：</p>
<ol class="arabic simple">
<li><p><strong>优化内存结构——增大吞吐量</strong>：将原始大规模矩阵根据不同阈值切分成不同层级的子矩阵块，使得子矩阵块能被如共享内存、寄存器等高性能体系结构存储下来，以此提高吞吐量。设置切分参数为<span class="math notranslate nohighlight">\(BlockTile[Ms, Ns, Ks]\)</span>和<span class="math notranslate nohighlight">\(WarpTile[Mw, Nw, Kw]\)</span>，对应的将BlockTile下的矩阵由全局内存搬移至共享内存，以提高全局内存合并访问和数据局部性，如
<a class="reference internal" href="#gemm-blocktile"><span class="std std-numref">图7.3.6</span></a>所示；再将内层WarpTile下的矩阵由共享内存搬移至寄存器中，如
<a class="reference internal" href="#gemm-warptile"><span class="std std-numref">图7.3.7</span></a>所示，以备Tensor Core加速器数据存取。</p></li>
</ol>
<figure class="align-default" id="id24">
<span id="gemm-blocktile"></span><a class="reference internal image-reference" href="../_images/G2S.svg"><img alt="../_images/G2S.svg" src="../_images/G2S.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.6 </span><span class="caption-text">全局内存与共享内存数据交互</span><a class="headerlink" href="#id24" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id25">
<span id="gemm-warptile"></span><a class="reference internal image-reference" href="../_images/S2R.svg"><img alt="../_images/S2R.svg" src="../_images/S2R.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.7 </span><span class="caption-text">共享内存与寄存器数据交互</span><a class="headerlink" href="#id25" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<ol class="arabic simple" start="2">
<li><p><strong>并行资源映射——提高并行性</strong>：将多层级的并行资源（Block、Warp、Thread）与对应需要计算/搬移的数据建立映射关系，提高程序并行性。将可并行的计算/数据搬移操作映射到并行资源上，对于GEMM实例，M/N轴即为可并行轴，将数据搬移操作中的循环指令映射分配到Block层级（即
<a class="reference internal" href="#gemm-tensor-core-algorith"><span class="std std-numref">图7.3.5</span></a>中的2-4行<span class="math notranslate nohighlight">\(For\)</span>循环），将内层循环指令映射分配到Warp层级（即
<a class="reference internal" href="#gemm-tensor-core-algorith"><span class="std std-numref">图7.3.5</span></a>中的8-9行<span class="math notranslate nohighlight">\(For\)</span>循环）。（前文介绍，线程束Warp作为调度的基本单位，且是WMMA
API操纵的基本层级，因此对Warp层级进行数据映射比Thread层级映射更为合适）</p></li>
<li><p><strong>Warp统一的Tensor Core数据交互——增大吞吐量</strong>：根据
<a class="reference internal" href="#diversified-programming-title"><span class="std std-numref">7.3.2节</span></a>节中介绍的编程方法，除调用算子库外，均需要使用或将指令封装成WMMA接口形式统一进行Warp层级的数据存取和计算。如
<a class="reference internal" href="#gemm-tensorcore"><span class="std std-numref">图7.3.8</span></a>所示，Tensor
Core加速器需要从局部内存/寄存器中读取数据，存于虚拟Fragment数据结构中，对应使用<span class="math notranslate nohighlight">\(wmma.load\_matrix\_sync()\)</span>接口，将累加Fragment
<span class="math notranslate nohighlight">\(C\)</span>
通过<span class="math notranslate nohighlight">\(wmma.fill\_fragment()\)</span>接口进行初始化后，使用<span class="math notranslate nohighlight">\(wmma.mma\_sync()\)</span>使能加速器进行乘累加运算，后将结果Fragment
<span class="math notranslate nohighlight">\(D\)</span>通过调用<span class="math notranslate nohighlight">\(wmma.store\_matrix\_sync()\)</span>接口拷贝至目标内存地址。</p></li>
</ol>
<figure class="align-default" id="id26">
<span id="gemm-tensorcore"></span><a class="reference internal image-reference" href="../_images/R2TC.svg"><img alt="../_images/R2TC.svg" src="../_images/R2TC.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.8 </span><span class="caption-text">寄存器与硬件加速器交互</span><a class="headerlink" href="#id26" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<ol class="arabic simple" start="4">
<li><p><strong>优化数据访存——提高并行性</strong>：在进行内存结构变化（矩阵数据搬移）时，需要注意全局内存的合并访问、共享内存的存储体冲突等常见性能瓶颈点。</p></li>
<li><p><strong>资源负载均衡——增大吞吐量</strong>：调整平衡每个线程处理的数据量、共享内存使用量、寄存器使用量，以获得更高的SM占用率。一般在实际程序中BlockTile和WarpTile的选取至关重要。</p></li>
</ol>
</section>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">7.3. 加速器基本编程原理</a><ul>
<li><a class="reference internal" href="#accelerator-programable-title">7.3.1. 硬件加速器的可编程性</a><ul>
<li><a class="reference internal" href="#id3">7.3.1.1. 编程接口使能加速器</a></li>
<li><a class="reference internal" href="#id4">7.3.1.2. 算子编译器使能加速器</a></li>
</ul>
</li>
<li><a class="reference internal" href="#diversified-programming-title">7.3.2. 硬件加速器的多样化编程方法</a><ul>
<li><a class="reference internal" href="#id15">7.3.2.1. 编程接口使能加速器</a></li>
<li><a class="reference internal" href="#id17">7.3.2.2. 算子编译器编程使能加速器</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id18">7.3.3. 硬件加速器高性能编程实例</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="accelerator_architecture.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>7.2. 加速器基本组成原理</div>
         </div>
     </a>
     <a id="button-next" href="summary.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>7.4. 总结</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>