<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>7.3. 加速器基本编程原理 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.4. 总结" href="summary.html" />
    <link rel="prev" title="7.2. 加速器基本组成原理" href="accelerator_architecture.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">7. </span>硬件加速器</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">7.3. </span>加速器基本编程原理</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_accelerator/accelerator_programming.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">4. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">5. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">5.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">5.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">5.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">5.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">6. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">6.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">6.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">6.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">6.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">6.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. 硬件加速器</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="accelerator_introduction.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_architecture.html">7.2. 加速器基本组成原理</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">7.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">8. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">8.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">8.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">8.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">8.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">8.6. 章节总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/reference.html">8.7. 引用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">9. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">9.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">9.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">9.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">9.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">10. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">10.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">10.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">10.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">10.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">10.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">10.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">11. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">12. 联邦学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">13. 强化学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">14. 可解释性AI系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_online_machine_learning/index.html">15. 在线机器学习</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  机器学习系统：设计和实现
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">1.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">1.2. 机器学习系统的需求</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">1.3. 机器学习系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applicable_readers.html">1.4. 适用读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">2. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">2.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">2.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">2.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">2.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">2.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">3. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">3.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">3.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">3.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">3.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">4. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">5. 编译器前端</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">5.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">5.2. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">5.3. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">5.4. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">5.5. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">5.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">6. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">6.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">6.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">6.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">6.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">6.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">6.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. 硬件加速器</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="accelerator_introduction.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_architecture.html">7.2. 加速器基本组成原理</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">7.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">8. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">8.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">8.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">8.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">8.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">8.6. 章节总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/reference.html">8.7. 引用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">9. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">9.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">9.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">9.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">9.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">9.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">10. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">10.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">10.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">10.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">10.4. 集合通讯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">10.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">10.6. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">11. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">12. 联邦学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">13. 强化学习系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">14. 可解释性AI系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_online_machine_learning/index.html">15. 在线机器学习</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <section id="id1">
<h1><span class="section-number">7.3. </span>加速器基本编程原理<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>本章前两节主要介绍了这些硬件加速器设计的意义、思路以及基本组成原理。软硬件协同优化作为构建高效AI系统的一个重要指导思想，需要软件算法/软件栈和硬件架构在神经网络应用中互相影响、紧密耦合。为了最大限度地发挥加速器的优势，要求能够基于硬件系统架构设计出一套较为匹配的指令或编程（操纵）方法。因此，本节将以<a class="reference external" href="#硬件加速器的计算单元">1.2.3</a>中介绍的Tensor
Core为例，着重介绍加速器的可编程性，以及如何通过编程使能加速器，提升神经网络算子的计算效率。</p>
<section id="id2">
<h2><span class="section-number">7.3.1. </span>硬件加速器的可编程性<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="#硬件加速器设计的思路">1.1.2</a>节中列出的硬件加速器均具有一定的可编程性，程序员可以通过软件编程，有效的使能上述加速器进行计算加速。但出于计算效率和易用性等方面考虑，将编程使能方式分为不同等级，一般包括：算子库层级，编程原语层级，以及指令层级。为了更具象的解释上述层级的区别，仍以Volta架构的Tensor
Core加速器为例，由高层至底层对比介绍这三种不同编程方式：</p>
<ul class="simple">
<li><p><strong>算子库层级</strong>：如cuBLAS基本矩阵与向量运算库，cuDNN深度学习加速库，均通过Host端调用算子库提供的核函数使能TensorCore；</p></li>
<li><p><strong>编程原语层级</strong>：如基于CUDA的WMMA
API编程接口。同算子库相比，需要用户显式调用计算各流程，如矩阵存取至TensorCore、TensorCore执行矩阵乘累加运算、TensorCore累加矩阵数据初始化操作等；</p></li>
<li><p><strong>指令层级</strong>：如PTX ISA
MMA指令集，提供更细粒度的mma指令，便于用户组成更多种形状的接口，通过CUDA
Device端内联编程使能TensorCore。</p></li>
</ul>
<p>矩阵乘法运算作为深度学习网络中占比最大的计算，对其进行优化是十分必要的。因此本节将统一以矩阵乘法<span class="math notranslate nohighlight">\(D[M, N] = C[M, N] + A[M, K] * B[K, N]\)</span>为实例，对比介绍如何通过不同编程方式使能加速器。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$A, B$矩阵 $D$矩阵  $C[i][j] \gets 0$
$C[i][j] \gets C[i][j] + A[i, k] \times B[k, j]$
$D[i][j] \gets C[i][j]$
</pre></div>
</div>
</section>
<section id="id3">
<h2><span class="section-number">7.3.2. </span>硬件加速器的多样化编程方法<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<section id="id4">
<h3><span class="section-number">7.3.2.1. </span>算子库使能加速器<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>在上述三种层级的编程方式中，直接调用算子加速库使能加速器无疑是最快捷高效的方式。NVIDIA提供了cuBLAS/cuDNN两类算子计算库，cuBLAS提供了使能Tensor
Core单元的接口，用以加速矩阵乘法(GEMM)运算，cuDNN提供了对应接口加速卷积(CONV)运算等。</p>
<p>以<a class="reference external" href="#硬件加速器的可编程性">1.3.1</a>小节的GEMM运算为例，与常规CUDA调用cuBLAS算子库相似，通过cuBLAS加速库使能Tensor
Core步骤包括：</p>
<ol class="arabic simple">
<li><p>创建cuBLAS对象句柄且设置对应数学计算模式</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cublasHandle_t</span> <span class="n">handle</span><span class="p">;</span>
<span class="n">cublasStatus_t</span> <span class="n">cublasStat</span> <span class="o">=</span> <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>
<span class="n">cublasStat</span> <span class="o">=</span> <span class="n">cublasSetMathMode</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">CUBLAS_TENSOR_OP_MATH</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>分配和初始化矩阵内存空间及内容元素</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">size_t</span> <span class="n">matrixSizeA</span> <span class="o">=</span> <span class="p">(</span><span class="n">size_t</span><span class="p">)</span><span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
<span class="n">cublasStat</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">devPtrA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">matrixSizeA</span> <span class="o">*</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">devPtrA</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]));</span>
<span class="n">cublasStat</span> <span class="o">=</span> <span class="n">cublasSetMatrix</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">A</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">devPtrA</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">M</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>调用对应计算函数接口</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cublasStat</span> <span class="o">=</span> <span class="n">cublasGemmEx</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">transa</span><span class="p">,</span> <span class="n">transb</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span>
                          <span class="n">A</span><span class="p">,</span> <span class="n">CUDA_R_16F</span><span class="p">,</span> <span class="n">lda</span><span class="p">,</span>
                          <span class="n">B</span><span class="p">,</span> <span class="n">CUDA_R_16F</span><span class="p">,</span> <span class="n">ldb</span><span class="p">,</span>
                          <span class="n">beta</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">CUDA_R_16F</span><span class="p">,</span> <span class="n">ldc</span><span class="p">,</span> <span class="n">CUDA_R_32F</span><span class="p">,</span> <span class="n">algo</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>传回结果数据</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cublasStat</span> <span class="o">=</span> <span class="n">cublasGetMatrix</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">sizeof</span><span class="p">(</span><span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">devPtrD</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">M</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>释放内存和对象句柄</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cudaFree</span><span class="p">(</span><span class="n">devPtrA</span><span class="p">);</span>
<span class="n">cudaDestroy</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span>
</pre></div>
</div>
<p>当然，由于加速器一般会有矩阵形状、数据类型、排布方式等限制，因此在调用句柄和函数接口时要多加注意。如本例中，cuBLAS计算模式必须设置为<span class="math notranslate nohighlight">\(CUBLAS\_TENSOR\_OP\_MATH\)</span>，步长必须设置为8的倍数，输入数据类型必须为<span class="math notranslate nohighlight">\(CUDA\_R\_16F\)</span>等。按照如上方式即可通过cuBLAS算子库对<a class="reference external" href="#硬件加速器的可编程性">1.3.1</a>实例使能Tensor
Core加速器，通过NVIDIA官方数据可知，该方式对于不同矩阵乘法计算规模，平均有4～10倍的提升，且矩阵规模越大，加速器提升效果越明显。</p>
<p>该方式由于能够隐藏体系结构细节，易用性较好，且一般官方提供的算子库吞吐量较高。但与此同时，这种算子颗粒度的库也存在一些问题，如不足以应对复杂多变的网络模型导致的算子长尾问题（虽然常规形式算子占据绝大多数样本，但仍有源源不断的新增算子，因其出现机会较少，算子库未对其进行有效优化。），以及错失了较多神经网络框架优化（如算子融合）的机会。</p>
</section>
<section id="id5">
<h3><span class="section-number">7.3.2.2. </span>编程原语使能加速器<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>第二种加速器编程方式为编程原语使能加速器，如通过在Device端调用CUDA WMMA
(Warp Matrix Multiply Accumulate)
API接口。以线程束（即Warp，是调度的基本单位）为操纵对象，使能多个Tensor
Core单元。该方式在CUDA
9.0中被公开，程序员可通过添加API头文件的引用和命名空间定义来使用上述API接口。基于软硬件协同设计的基本思想，该层级编程API的设计多与架构绑定，如WMMA操纵的总是<span class="math notranslate nohighlight">\(16x16\)</span>大小的矩阵块，并且操作一次跨两个TensorCore进行处理，本质是与TensorCore如何集成进SM中强相关的。针对Float16输入数据类型，NVIDIA官方提供了三种不同矩阵规模的WMMA乘累加计算接口，分别为<span class="math notranslate nohighlight">\(16x16x16\)</span>，<span class="math notranslate nohighlight">\(32x8x16\)</span>，<span class="math notranslate nohighlight">\(8x32x16\)</span>。</p>
<p>该API接口操纵的基本单位为Fragment，是一种指明了矩阵含义（乘法器/累加器）、矩阵形状（<span class="math notranslate nohighlight">\(WMMA\_M, WMMA\_N, WMMA\_K\)</span>）、数据类型（Half/
Float）、排布方式（<span class="math notranslate nohighlight">\(row\_major/ col\_major\)</span>）等信息的模板类型，包括如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wmma</span><span class="p">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="p">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="p">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">a_frag</span><span class="p">;</span>
<span class="n">wmma</span><span class="p">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="p">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="p">::</span><span class="n">col_major</span><span class="o">&gt;</span> <span class="n">b_frag</span><span class="p">;</span>
<span class="n">wmma</span><span class="p">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="p">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="nb">float</span><span class="o">&gt;</span> <span class="n">acc_frag</span><span class="p">;</span>
<span class="n">wmma</span><span class="p">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="p">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="nb">float</span><span class="o">&gt;</span> <span class="n">c_frag</span><span class="p">;</span>
</pre></div>
</div>
<p>使用时，我们需要将待执行乘法操作矩阵块的数据，作为Fragment，由寄存器加载至TensorCore，在将累加Fragment初始化/清零操作后，通过TensorCore单元执行乘累加运算，最后将运算结果的Fragment存回寄存器或其他内存区域。与上述操作对应的，NVIDIA提供了<span class="math notranslate nohighlight">\(wmma.load\_matrix\_sync(), wmma.store\_matrix\_sync()\)</span>接口用于将参与计算的子矩阵块写入/载出Fragment片段；<span class="math notranslate nohighlight">\(wmma.fill\_fragment()\)</span>接口用于初始化对应Fragment的数据；<span class="math notranslate nohighlight">\(wmma.mma\_sync()\)</span>接口用于对Fragment进行乘累加运算。</p>
</section>
<section id="id6">
<h3><span class="section-number">7.3.2.3. </span>指令集编程使能加速器<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>在NVIDIA PTX ISA (Instruction Set
Architecture)中提供了另一个编程接口，如Volta架构中的<span class="math notranslate nohighlight">\(mma.sync.m8n8k4\)</span>指令，它使用<span class="math notranslate nohighlight">\(M=8, N=8, K=4\)</span>的形状配置执行乘累加操作。具体地，它由线程组（黑色椭圆表示）或octet执行[&#64;2018Modeling]，如图
<a class="reference internal" href="#ptx"><span class="std std-numref">图7.3.1</span></a>显示了线程和数据的映射关系。每个线程组由四个连续的线程组成，使用不同颜色的圆圈表示。图中还指出了一个octet里面的线程在线程束内的分布，Float16乘法器A或B的四个连续元素（使用具有相同颜色的块表示），以及Float32累加器C或D的八个分散元素（同样使用相同颜色的块表示）。彩色块上的数字代表对应的线程ID。</p>
<figure class="align-default" id="id9">
<span id="ptx"></span><a class="reference internal image-reference" href="../_images/ptx.svg"><img alt="../_images/ptx.svg" src="../_images/ptx.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.1 </span><span class="caption-text">mma指令之线程与矩阵元素映射关系</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>作为一个更细粒度的指令，mma可以组成更加多样化形状的Warp范围的WMMA
API接口，可以控制线程束内线程与数据的映射关系，并允许AI编译器自动/手动显式地管理内存层次结构之间的矩阵分解，因此相比于直接应用NVCUDA::WMMA
API具有更好的灵活性。</p>
</section>
<section id="id7">
<h3><span class="section-number">7.3.2.4. </span>算子编译器编程使能加速器<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>除了上述三种层级的编程方式，算子编译器也逐渐成为DSA加速器的关注热点。随着深度学习模型的迭代更新以及各类DSA加速器的层出不穷，手写算子或高性能算子库（如cuDNN/cuBLAS）等基于人工优化算子的方式给算子开发团队带来沉重的负担。因此，开发一种能够将High-level的算子表示编译成目标硬件可执行代码的算子编译器，成为了学术界、业界的共识。</p>
<p>近年来涌现出许多算子编译器/编译框架，如TVM，为不同的硬件后端提供了编译优化支持。在昇腾芯片上，基于TVM开发了TBE(Tensor
Boost
Engine)，不仅提供了一个优化过的神经网络标准算子库，同时还提供了算子开发能力及融合能力。通过TBE提供的API和自定义算子编程开发界面可以完成相应神经网络算子的开发，帮助用户较容易的去使能硬件加速器上的AI_CORE相关指令，以实现高性能的神经网络计算。此外，为了更好的支持复杂算子融合场景，还提供了基于polyhedral多面体编译技术的AKG(Auto
kernel generator)，提供算子的自动生成能力。</p>
<p>基于算子编译器使能加速器实现矩阵乘的流程则对用户更加友好，用户只需基于python定义矩阵乘的tensor信息（数据类型及形状等），调用对应TBE接口即可。如下所示：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">b_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="p">)</span>
<span class="n">in_dtype</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span>
<span class="n">dst_dtype</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span>
<span class="n">tensor_a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">a_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tensor_a&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">in_dtype</span><span class="p">)</span>
<span class="n">tensor_b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">b_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tensor_b&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">in_dtype</span><span class="p">)</span>
<span class="n">tensor_bias</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tensor_bias&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dst_dtype</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="n">cce</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dst_dtype</span><span class="o">=</span><span class="n">dst_dtype</span><span class="p">,</span> <span class="n">tensor_bias</span><span class="o">=</span><span class="n">tensor_bias</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id8">
<h2><span class="section-number">7.3.3. </span>硬件加速器高性能编程实例<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>本节<a class="reference external" href="#加速器基本编程原理">1.3</a>前几个小节主要介绍了硬件加速器的不同层级的多样化编程方法。调用计算库的方式留给程序员的优化空间较少，合理利用硬件加速器不同层级的编程，可以实现更好的性能优化。
为了更好的让读者理解硬件加速器的使用，本节会继续<a class="reference external" href="#硬件加速器的可编程性">1.3.1</a>节中的GEMM运算，仍以WMMA
API使能Tensor
Core加速单元为例，介绍如何通过矩阵分块、资源映射等方式更高效的利用硬件加速器。</p>
<p>[alg:TensorCore]</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$A, B$矩阵 $D$矩阵   Mapping to Block.Idx  Mapping to Block.Idy  Mapping
to Block.Idz
$A_{Shared}[i_o][k_o] \gets A[i_o][k_o]$ $B_{Shared}[k_o][j_o] \gets B[k_o][j_o]$ $Syncthreads()$
 Mapping to Warp.Idx  Mapping to Warp.Idy
$A_{Register}[i_i][k_i] \gets A_{Shared}[i_i][k_i]$ $B_{Register}[k_i][j_i] \gets B_{Shared}[k_i][j_i]$ $pragma\ unroll$
$wmma.load\_matrix\_sync(A_{Fragment}, A_{Register})$ $wmma.load\_matrix\_sync(B_{Fragment}, B_{Register})$
$wmma.fill\_fragment(C_{Fragment}, 0)$
$wmma.mma\_sync(D_{Fragment}, C_{Fragment}, A_{Fragment}, B_{Fragment})$
$Syncthreads()$  $wmma.store\_matrix\_sync(D, D_{Fragment})$
</pre></div>
</div>
<p>若要得到高性能CUDA程序，提高并行性、增大吞吐量、优化指令执行是至关重要的三个优化目标。针对该实例，具体地实现和优化方案列出如下，对应到具体实例伪代码如算法2所示：</p>
<ol class="arabic simple">
<li><p><strong>优化内存结构——增大吞吐量</strong>：将原始大规模矩阵根据不同阈值切分成不同层级的子矩阵块，使得子矩阵块能被如共享内存、寄存器等高性能体系结构存储下来，以此提高吞吐量。设置切分参数为<span class="math notranslate nohighlight">\(BlockTile[Ms, Ns, Ks]\)</span>和<span class="math notranslate nohighlight">\(WarpTile[Mw, Nw, Kw]\)</span>，对应的将BlockTile下的矩阵由全局内存搬移至共享内存，以提高全局内存合并访问和数据局部性，如图
<a class="reference internal" href="#gemm-blocktile"><span class="std std-numref">图7.3.2</span></a>所示；再将内层WarpTile下的矩阵由共享内存搬移至寄存器中，如图
<a class="reference internal" href="#gemm-warptile"><span class="std std-numref">图7.3.3</span></a>所示，以备Tensor Core加速器数据存取。</p></li>
</ol>
<figure class="align-default" id="id10">
<span id="gemm-blocktile"></span><a class="reference internal image-reference" href="../_images/G2S.svg"><img alt="../_images/G2S.svg" src="../_images/G2S.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.2 </span><span class="caption-text">全局内存与共享内存数据交互</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id11">
<span id="gemm-warptile"></span><a class="reference internal image-reference" href="../_images/S2R.svg"><img alt="../_images/S2R.svg" src="../_images/S2R.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.3 </span><span class="caption-text">共享内存与寄存器数据交互</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<ol class="arabic simple" start="2">
<li><p><strong>并行资源映射——提高并行性</strong>：将多层级的并行资源（Block、Warp、Thread）与对应需要计算/搬移的数据建立映射关系，提高程序并行性。将可并行的计算/数据搬移操作映射到并行资源上，对于GEMM实例，M/N轴即为可并行轴，将数据搬移操作中的循环指令映射分配到Block层级（即算法3中的2-4行<span class="math notranslate nohighlight">\(For\)</span>循环），将内层循环指令映射分配到Warp层级（即算法3中的8-10行<span class="math notranslate nohighlight">\(For\)</span>循环）。（前文介绍，线程束Warp作为调度的基本单位，且是WMMA
API操纵的基本层级，因此对Warp层级进行数据映射比Thread层级映射更为合适）</p></li>
<li><p><strong>Warp统一的Tensor
Core数据交互——增大吞吐量</strong>：根据<a class="reference external" href="#硬件加速器的多样化编程方法">1.3.2</a>节中介绍的编程方法，除调用算子库外，均需要使用或将指令封装成WMMA接口形式统一进行Warp层级的数据存取和计算。如图
<a class="reference internal" href="#gemm-tensorcore"><span class="std std-numref">图7.3.4</span></a>所示，Tensor
Core加速器需要从局部内存/寄存器中读取数据，存于虚拟Fragment数据结构中，对应使用<span class="math notranslate nohighlight">\(wmma.load\_matrix\_sync()\)</span>接口，将累加Fragment
<span class="math notranslate nohighlight">\(C\)</span>
通过<span class="math notranslate nohighlight">\(wmma.fill\_fragment()\)</span>接口进行初始化后，使用<span class="math notranslate nohighlight">\(wmma.mma\_sync()\)</span>使能加速器进行乘累加运算，后将结果Fragment
<span class="math notranslate nohighlight">\(D\)</span>通过调用<span class="math notranslate nohighlight">\(wmma.store\_matrix\_sync()\)</span>接口拷贝至目标内存地址。</p></li>
</ol>
<figure class="align-default" id="id12">
<span id="gemm-tensorcore"></span><a class="reference internal image-reference" href="../_images/R2TC.svg"><img alt="../_images/R2TC.svg" src="../_images/R2TC.svg" width="800px" /></a>
<figcaption>
<p><span class="caption-number">图7.3.4 </span><span class="caption-text">寄存器与硬件加速器交互</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<ol class="arabic simple" start="4">
<li><p><strong>优化数据访存——提高并行性</strong>：在进行内存结构变化（矩阵数据搬移）时，需要注意全局内存的合并访问、共享内存的存储体冲突等常见性能瓶颈点。</p></li>
<li><p><strong>资源负载均衡——增大吞吐量</strong>：调整平衡每个线程处理的数据量、共享内存使用量、寄存器使用量，以获得更高的SM占用率。一般在实际程序中BlockTile和WarpTile的选取至关重要。</p></li>
<li><p><strong>优化指令执行</strong>：使用#unroll功能进行循环展开以避免分支冲突，如算法3中13行；使用向量化加载指令减少PTX指令执行次数以提高带宽等，对于GPU
Volta架构，最大向量化加载指令为ldg128，即128比特带宽，对于算法3中5-6行数据由全局内存加载至共享内存时，即可采用Float4*类型指针进行内存读取。</p></li>
</ol>
</section>
</section>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">7.3. 加速器基本编程原理</a><ul>
<li><a class="reference internal" href="#id2">7.3.1. 硬件加速器的可编程性</a></li>
<li><a class="reference internal" href="#id3">7.3.2. 硬件加速器的多样化编程方法</a><ul>
<li><a class="reference internal" href="#id4">7.3.2.1. 算子库使能加速器</a></li>
<li><a class="reference internal" href="#id5">7.3.2.2. 编程原语使能加速器</a></li>
<li><a class="reference internal" href="#id6">7.3.2.3. 指令集编程使能加速器</a></li>
<li><a class="reference internal" href="#id7">7.3.2.4. 算子编译器编程使能加速器</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id8">7.3.3. 硬件加速器高性能编程实例</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="accelerator_architecture.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>7.2. 加速器基本组成原理</div>
         </div>
     </a>
     <a id="button-next" href="summary.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>7.4. 总结</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>