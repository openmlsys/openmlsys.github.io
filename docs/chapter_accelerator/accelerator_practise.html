<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>8.4. 加速器实践 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.5. 总结" href="summary.html" />
    <link rel="prev" title="8.3. 加速器基本编程原理" href="accelerator_programming.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">8. </span>硬件加速器</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">8.4. </span>加速器实践</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_accelerator/accelerator_practise.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/design.html">2.2. 机器学习框架的设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/architecture.html">2.3. 机器学习框架的基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/ecosystem.html">2.4. 机器学习系统生态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/readers.html">2.5. 图书结构和读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_programming_paradigm.html">3.5. 机器学习框架的编程范式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. AI编译器和前端技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ai_compiler_design_principle.html">6.1. AI编译器设计原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.2. AI编译器前端技术概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.3. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.4. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.5. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.6. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/op_compiler.html">7.6. 算子编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">8. 硬件加速器</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">8.4. 加速器实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">8.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">8.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id3">8.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html#id2">9.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">10. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">11. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">11.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">11.4. 集合通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">11.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">11.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id2">11.7. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id3">11.8. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">13.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">13.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">13.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id3">13.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">15.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id17">16.4. 可解释AI系统及实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id21">16.5. 未来可解释AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id22">16.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rl_sys/index.html">17. 机器人系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">17.1. 机器人系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros_code_ex.html">17.3. 案例分析：使用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/summary.html">17.4. 总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">4. 参考文献</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/design.html">2.2. 机器学习框架的设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/architecture.html">2.3. 机器学习框架的基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/ecosystem.html">2.4. 机器学习系统生态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/readers.html">2.5. 图书结构和读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_programming_paradigm.html">3.5. 机器学习框架的编程范式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. AI编译器和前端技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ai_compiler_design_principle.html">6.1. AI编译器设计原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.2. AI编译器前端技术概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.3. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.4. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.5. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.6. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/op_compiler.html">7.6. 算子编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">8. 硬件加速器</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">8.4. 加速器实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">8.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">8.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id3">8.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html#id2">9.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">10. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_distributed_training/index.html">11. 分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/methods.html">11.2. 分布式方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/pipeline.html">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/collective.html">11.4. 集合通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/parameter_servers.html">11.5. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html">11.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id2">11.7. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_distributed_training/summary.html#id3">11.8. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/overview.html">13.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.2. 主流系统架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_problem.html">13.3. 现有解决方案及其存在的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/future.html">13.4. 未来可以探索的方向</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id3">13.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">15.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id17">16.4. 可解释AI系统及实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id21">16.5. 未来可解释AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id22">16.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rl_sys/index.html">17. 机器人系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">17.1. 机器人系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros_code_ex.html">17.3. 案例分析：使用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/summary.html">17.4. 总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">4. 参考文献</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">8.4. </span>加速器实践<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>在本节中会通过具体的CUDA代码向读者介绍如何编写一个并行计算的广义矩阵乘法程序，通过提高计算强度、使用共享内存、优化内存读取流水线等方法最终取得接近硬件加速器性能峰值的实现。虽然在以上章节介绍了张量计算核心相关的内容，但由于篇幅限制，在本节中不使用此硬件结构。而是通过使用更为基本的CUDA代码实现FP32的广义矩阵乘法，来讲解若干实用优化策略。
### 环境</p>
<p>本节的实践有以下的软件环境依赖：</p>
<ul class="simple">
<li><p>Eigen：Eigen是一个线性代数C++模板库，用户可以只使用几条语句完成多线程线性代数运算。</p></li>
<li><p>OpenMP（可选）：OpenMP是用于共享内存并行系统的多处理器程序设计的一套指导性编译处理方案，可以使用OpenMP对Eigen的计算进行加速。</p></li>
<li><p>CUDA Toolkit：CUDA
Toolkit是英伟达发布的CUDA工具包，其包含了CUDA编译器（NVCC），CUDA线性代数库（cuBLAS）等组件。
本节的实践都是在CPU Intex Xeon E5-2650 v3，GPU Nvidia Geforce RTX
3080；系统Ubuntu 18.04版本，CUDA Toolkit 11.1进行的。</p></li>
</ul>
<p>安装相关依赖如下：</p>
<ul class="simple">
<li><p>Eigen：Eigen的安装可以通过使用包管理器安装（如使用指令<code class="docutils literal notranslate"><span class="pre">apt</span> <span class="pre">install</span> <span class="pre">libeigen3-dev</span></code>），也可以从<a class="reference external" href="https://eigen.tuxfamily.org/index.php?title=Main_Page">官网</a>下载。</p></li>
<li><p>OpenMP（可选）：通常会被大多数编译器默认支持，如果没有被支持的话可以使用包管理器安装（如使用指令<code class="docutils literal notranslate"><span class="pre">apt</span> <span class="pre">install</span> <span class="pre">libomp-dev</span></code>）。</p></li>
<li><p>CUDA Toolkit：CUDA
Toolkit的安装建议按照<a class="reference external" href="https://developer.nvidia.com/cuda-downloads">官方的提示</a>安装，也可以通过使用包管理器安装（如使用指令<code class="docutils literal notranslate"><span class="pre">apt</span> <span class="pre">install</span> <span class="pre">cuda</span></code>）。</p></li>
</ul>
<div class="section" id="sec-accelerator-naive">
<span id="id2"></span><h2><span class="section-number">8.4.1. </span>广义矩阵乘法的朴素实现<a class="headerlink" href="#sec-accelerator-naive" title="Permalink to this headline">¶</a></h2>
<p>依照算法.. _algo-accelerator-gemm:，编写CPU代码如下所示：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">M</span><span class="p">][</span><span class="n">K</span><span class="p">];</span>
<span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">K</span><span class="p">][</span><span class="n">N</span><span class="p">];</span>
<span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">M</span><span class="p">][</span><span class="n">N</span><span class="p">];</span>
<span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">;</span>

<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="o">++</span><span class="n">m</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">c</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">n</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>可以看到，矩阵<span class="math notranslate nohighlight">\(C\)</span>
中各个元素的计算是独立的。可以利用GPU的大量线程去分别计算矩阵<span class="math notranslate nohighlight">\(C\)</span>
中相应的元素，以达到并行计算的目的，GPU核函数将如下所示：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">gemmKernel</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span>
                           <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span> <span class="n">C</span><span class="p">,</span>
                           <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">M</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">N</span><span class="p">,</span>
                           <span class="kt">unsigned</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">m</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">m</span> <span class="o">&gt;=</span> <span class="n">M</span> <span class="o">||</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="n">N</span><span class="p">)</span>
      <span class="k">return</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">c</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">n</span><span class="p">];</span>
  <span class="p">}</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span>
  <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="n">c</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">beta</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">+</span> <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="n">C</span><span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>其可视化结构如
<a class="reference internal" href="#cuda-naive-gemm"><span class="std std-numref">图8.4.1</span></a>所示，矩阵<span class="math notranslate nohighlight">\(C\)</span>中每一个元素由一个线程计算，在GPU
Kernel的第5和6行计算该线程对应矩阵<span class="math notranslate nohighlight">\(C\)</span>中的元素行号<span class="math notranslate nohighlight">\(m\)</span>及列号<span class="math notranslate nohighlight">\(n\)</span>，然后在第9到11行该线程利用行号与列号读取矩阵<span class="math notranslate nohighlight">\(A\)</span>和矩阵<span class="math notranslate nohighlight">\(B\)</span>中相应的行列向量元素并计算向量内积，最后在第17行将结果写回<span class="math notranslate nohighlight">\(C\)</span>矩阵。</p>
<div class="figure align-default" id="id9">
<span id="cuda-naive-gemm"></span><a class="reference internal image-reference" href="../_images/naive.png"><img alt="../_images/naive.png" src="../_images/naive.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.1 </span><span class="caption-text">矩阵乘法的朴素实现</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>使用以下代码启动核函数：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">gemmNaive</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span>
               <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">M</span><span class="p">,</span>
               <span class="kt">unsigned</span> <span class="n">N</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">dim3</span> <span class="n">block</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">);</span>
  <span class="n">dim3</span> <span class="n">grid</span><span class="p">((</span><span class="n">M</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">block</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">block</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>

  <span class="n">gemmKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>在这里令每个线程块处理矩阵<span class="math notranslate nohighlight">\(C\)</span>中<span class="math notranslate nohighlight">\(16\times16\)</span>个元素，因此开启<span class="math notranslate nohighlight">\((M - 1) / 16 + 1 \times (N - 1) / 16 + 1\)</span>个线程块用于计算整个矩阵<span class="math notranslate nohighlight">\(C\)</span>。</p>
<p>使用Eigen生成数据并计算得到CPU端的广义矩阵乘法结果，同时实现了GPU端计算结果的误差计算、时间测试的代码，详情见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/first_attempt.cu">first_attempt.cu</a>，编译及执行得到输出结果为：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">48.961</span> <span class="n">ms</span>
<span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
</pre></div>
</div>
<p>可以使用以下公式粗略的计算GPU的峰值吞吐量：2<span class="math notranslate nohighlight">\(\times\)</span>频率<span class="math notranslate nohighlight">\(\times\)</span>单精度计算单元数量
，其中单精度计算单元数量等于GPU中流多处理器（SM）数量乘每个流多处理器中单精度计算单元数量，计算可以得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FP32</span> <span class="n">peak</span> <span class="n">throughput</span> <span class="mf">29767.680</span> <span class="n">GFLOPS</span>
<span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">185.313</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>可以发现目前的代码距离设备峰值性能仍有较大的差距。在整个计算过程中计算密集最大的过程为矩阵乘法<span class="math notranslate nohighlight">\(A\times B\)</span>，其时间复杂度为<span class="math notranslate nohighlight">\(O(M*N*K)\)</span>，而整个计算过程时间复杂度为<span class="math notranslate nohighlight">\(O(M*N*K+2*M*N)\)</span>，因此对矩阵乘法的优化是提升性能的关键。</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">8.4.2. </span>提高计算强度<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>计算强度（Compute
Intensity）指计算指令数量与访存指令数量的比值，在现代GPU中往往有大量计算单元但只有有限的访存带宽，程序很容易出现计算单元等待数据读取的问题，因此提高计算强度是提升程序性能的一条切实有效的指导思路。对于之前实现的GPU核函数，可以粗略计算其计算强度：在<span class="math notranslate nohighlight">\(K\)</span>次循环的内积计算中，对矩阵<span class="math notranslate nohighlight">\(A\)</span>与矩阵<span class="math notranslate nohighlight">\(B\)</span>的每次读取会计算一次浮点乘法与浮点加法，因此计算强度为1——两次浮点运算除以两次数据读取。之前的版本是每个线程负责处理矩阵<span class="math notranslate nohighlight">\(C\)</span>的一个元素——计算矩阵﻿﻿<span class="math notranslate nohighlight">\(A\)</span>的一行与矩阵<span class="math notranslate nohighlight">\(B\)</span>的一列的内积，可以通过使每个线程计算<span class="math notranslate nohighlight">\(C\)</span>更多的元素——计算矩阵<span class="math notranslate nohighlight">\(A\)</span>的多行与矩阵<span class="math notranslate nohighlight">\(B\)</span>的多列的内积——从而提升计算强度。具体地，如果在<span class="math notranslate nohighlight">\(K\)</span>次循环的内积计算中一次读取矩阵<span class="math notranslate nohighlight">\(A\)</span>中的<span class="math notranslate nohighlight">\(m\)</span>个元素和矩阵<span class="math notranslate nohighlight">\(B\)</span>中的<span class="math notranslate nohighlight">\(n\)</span>个元素，那么访存指令为<span class="math notranslate nohighlight">\(m+n\)</span>条，而计算指令为<span class="math notranslate nohighlight">\(2mn\)</span>条，所以计算强度为<span class="math notranslate nohighlight">\(\frac{2mn}{m+n}\)</span>，因此可以很容易发现提高<span class="math notranslate nohighlight">\(m\)</span>和<span class="math notranslate nohighlight">\(n\)</span>会带来计算强度的提升。</p>
<p>在上一小节中对全局内存的访问与存储都是借助 <code class="docutils literal notranslate"><span class="pre">float</span></code>
指针完成的，具体到硬件指令集上实际是使用指令 <code class="docutils literal notranslate"><span class="pre">LDG.E</span></code> 与 <code class="docutils literal notranslate"><span class="pre">STG.E</span></code>
完成的。可以使用128位宽指令<code class="docutils literal notranslate"><span class="pre">LDG.E.128</span></code> 与 <code class="docutils literal notranslate"><span class="pre">STG.E.128</span></code> 一次读取多个
<code class="docutils literal notranslate"><span class="pre">float</span></code>
数。使用宽指令的好处是一方面简化了指令序列，使用一个宽指令代替四个标准指令可以节省十几个指令的发射周期，这可以为计算指令的发射争取到额外的时间；另一方面128比特正好等于一个cache
line的长度，使用宽指令也有助于提高cache
line的命中率。但并不提倡在一切代码中过度追求宽指令的使用，开发者应当将更多的时间关注并行性设计和局部数据复用等更直接的优化手段。</p>
<p>具体的实现如下，由于每个 <code class="docutils literal notranslate"><span class="pre">float</span></code> 类型大小为32个比特，可以将4个
<code class="docutils literal notranslate"><span class="pre">float</span></code> 堆叠在一起构成一个128比特的 <code class="docutils literal notranslate"><span class="pre">float4</span></code> 类，对 <code class="docutils literal notranslate"><span class="pre">float4</span></code>
的访存将会是使用宽指令完成。其具体代码实现见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/util.cuh">util.cuh</a>中。</p>
<p>在实现GPU核函数过程中要注意，每个线程需要从原本各读取矩阵<span class="math notranslate nohighlight">\(A\)</span>和矩阵<span class="math notranslate nohighlight">\(B\)</span>中一个
<code class="docutils literal notranslate"><span class="pre">float</span></code> 数据变为各读取4个 <code class="docutils literal notranslate"><span class="pre">float</span></code>
数据，这就要求现在每个线程负责处理矩阵<span class="math notranslate nohighlight">\(C\)</span>中<span class="math notranslate nohighlight">\(4\times 4\)</span>的矩阵块，称之为
<code class="docutils literal notranslate"><span class="pre">thread</span> <span class="pre">tile</span></code>
。如图:numref:<cite>use_float4</cite>所示，每个线程从左到右、从上到下分别读取矩阵<span class="math notranslate nohighlight">\(A\)</span>和矩阵<span class="math notranslate nohighlight">\(B\)</span>的数据并运算，最后写入到矩阵<span class="math notranslate nohighlight">\(C\)</span>中。</p>
<div class="figure align-default" id="id10">
<span id="use-float4"></span><a class="reference internal image-reference" href="../_images/use_float4.png"><img alt="../_images/use_float4.png" src="../_images/use_float4.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.2 </span><span class="caption-text">提高计算强度</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>完整代码见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_use_128.cu">gemm_use_128.cu</a>。我们可以进一步让每个线程处理更多的数据，从而进一步提升计算强度，如图:numref:<cite>use_tile</cite>所示。完整代码见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_use_tile.cu">gemm_use_tile.cu</a>。</p>
<div class="figure align-default" id="id11">
<span id="use-tile"></span><a class="reference internal image-reference" href="../_images/use_tile.png"><img alt="../_images/use_tile.png" src="../_images/use_tile.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.3 </span><span class="caption-text">通过提高线程所处理矩阵块的数量来进一步提高计算强度</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
<p>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">6.232</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">1378.317</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>使用分析工具Nsight Compute分析取得性能提升的具体原因。Nsight
Compute是英伟达发布的主要针对GPU核函数的性能分析工具，它通过劫持驱动的方式对GPU底层数据采样和输出。可以使用以下指令进行性能分析：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash
ncu --set full -o &lt;profile_output_file&gt; &lt;profile_process&gt;
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">full</span></code> 代表采样所有数据， <code class="docutils literal notranslate"><span class="pre">-o</span></code> 代表以文件的形式输出结果；
<code class="docutils literal notranslate"><span class="pre">&lt;profile_output_file&gt;</span></code> 填输出文件名但注意不要加后缀名，
<code class="docutils literal notranslate"><span class="pre">&lt;profile_process&gt;</span></code> 填待分析的可执行文件及其参数。 比如需要分析
<code class="docutils literal notranslate"><span class="pre">first_attempt</span></code> ，将输出结果命名为 <code class="docutils literal notranslate"><span class="pre">first_attepmt_prof_result</span></code>
可以使用以下指令：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ncu</span> <span class="o">--</span><span class="n">set</span> <span class="n">full</span> <span class="o">-</span><span class="n">o</span> <span class="n">first_attepmt_prof_result</span> <span class="p">.</span><span class="o">/</span><span class="n">first_attempt</span>
</pre></div>
</div>
<p>如果提示权限不足可以使在指令前加<code class="docutils literal notranslate"><span class="pre">sudo</span></code> 。
在得到输出文件之后，可以使用 <code class="docutils literal notranslate"><span class="pre">nv-nsight-cu</span></code>
查看文件。对改动的GPU核函数与上一版本的GPU核函数进行对比分析，发现：</p>
<p>首先 <code class="docutils literal notranslate"><span class="pre">LDG</span></code> 指令数量下降了84%，且指标 <code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">LG</span> <span class="pre">Throttle</span></code>
下降33%，说明使用宽指令增加计算密度确实可以通过减少全局内存访问的指令数目而减少发射等待时间。最后指标
<code class="docutils literal notranslate"><span class="pre">Arithmetic</span> <span class="pre">Intensity</span></code> 的提升也和之前的关于计算强度的分析相吻合。</p>
<p>我们对<code class="docutils literal notranslate"><span class="pre">gemm_use_tile.cu</span></code>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">3.188</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">2694.440</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>使用Nsight Compute分析发现：类似地，本次优化在 <code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">LG</span> <span class="pre">Throttle</span></code>
等指标上取得了进一步的提升。</p>
</div>
<div class="section" id="sec-accelerator-use-smem">
<span id="id4"></span><h2><span class="section-number">8.4.3. </span>使用共享内存缓存复用数据<a class="headerlink" href="#sec-accelerator-use-smem" title="Permalink to this headline">¶</a></h2>
<p>虽然令一个线程一次读取更多的数据能取得计算强度的提升进而带来性能的提升，但是这种令单个线程处理数据增多的设计会导致开启总的线程数量减少，进而导致并行度下降，因此需要使用其他硬件特性在尽可能不影响并行度的前提下取得性能提升。在之前的代码中，开启若干个线程块，每个线程块处理矩阵<span class="math notranslate nohighlight">\(C\)</span>中的一个或多个矩阵块。在
<a class="reference internal" href="#duplicated-data"><span class="std std-numref">图8.4.4</span></a>
中，可以观察到，处理矩阵<span class="math notranslate nohighlight">\(C\)</span>同一行的线程<span class="math notranslate nohighlight">\(x, y\)</span>会读取矩阵<span class="math notranslate nohighlight">\(A\)</span>中相同的数据，可以借助共享内存让同一个线程块中不同的线程读取不重复的数据而提升程序吞吐量。</p>
<div class="figure align-default" id="id12">
<span id="duplicated-data"></span><a class="reference internal image-reference" href="../_images/duplicated_data.png"><img alt="../_images/duplicated_data.png" src="../_images/duplicated_data.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.4 </span><span class="caption-text">线程间重复读取数据</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</div>
<p>具体地，需要对代码进行如下改造：首先此前代码在计算内积过程是进行<span class="math notranslate nohighlight">\(K\)</span>次循环读取数据并累加计算，在此设定下每次循环中处理矩阵<span class="math notranslate nohighlight">\(C\)</span>中相同行的线程会读取相同的矩阵<span class="math notranslate nohighlight">\(A\)</span>的数据，处理矩阵<span class="math notranslate nohighlight">\(C\)</span>中相同列的线程会读取相同的矩阵<span class="math notranslate nohighlight">\(B\)</span>的数据。可以通过将此<span class="math notranslate nohighlight">\(K\)</span>次循环拆解成两层循环，外层循环<span class="math notranslate nohighlight">\(\frac{K}{tileK}\)</span>次，每次外层循环的迭代读取一整块数据，内层循环<span class="math notranslate nohighlight">\(tileK\)</span>次进行累加数据。数据从全局内存向共享内存的搬运过程如图
<a class="reference internal" href="#use-smem-store"><span class="std std-numref">图8.4.5</span></a>
所示，每次内层循环开始前将矩阵<span class="math notranslate nohighlight">\(A\)</span>和矩阵<span class="math notranslate nohighlight">\(B\)</span>中一整个
<code class="docutils literal notranslate"><span class="pre">tile</span></code> 读取到共享内存中；数据从共享内存到寄存器的搬运如图
<a class="reference internal" href="#use-smem-load"><span class="std std-numref">图8.4.6</span></a>
所示，每次内层循环循环从共享内存读取数据并计算。这种设计带来的好处是，可以让每个线程不必独自从全局内存读取所有需要的数据，整个线程块将共同需要的数据从全局内存中读取并写入到共享内存中，此后每个线程在计算过程中只需要从共享内存中读取所需要的数据即可。</p>
<div class="figure align-default" id="id13">
<span id="use-smem-store"></span><a class="reference internal image-reference" href="../_images/use_smem_store.png"><img alt="../_images/use_smem_store.png" src="../_images/use_smem_store.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.5 </span><span class="caption-text">向共享内存中写入数据</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id14">
<span id="use-smem-load"></span><a class="reference internal image-reference" href="../_images/use_smem_load.png"><img alt="../_images/use_smem_load.png" src="../_images/use_smem_load.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.6 </span><span class="caption-text">从共享内存中读取数据</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
<p>完整代码见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_use_smem.cu">gemm_use_smem.cu</a>。</p>
<p>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">0.617</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">13925.168</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>通过使用Nsight
Compute对核函数分析并与上一个核函数进行对比，可以观察到一些主要的变化：首先
<code class="docutils literal notranslate"><span class="pre">LDG</span></code> 指令数量下降了97%，与此前设计相吻合。同时观察到
<code class="docutils literal notranslate"><span class="pre">SM</span> <span class="pre">Utilization</span></code>
提升了218%也可以侧面证实使用共享内存减少了内存访问延迟从而提升了利用率，此外还可以观察到各项指标如
<code class="docutils literal notranslate"><span class="pre">Pipe</span> <span class="pre">Fma</span> <span class="pre">Cycles</span> <span class="pre">Active</span></code>
等都有显著提升，这都能充分解释了使用共享内存的改进是合理且有效的。</p>
</div>
<div class="section" id="id5">
<h2><span class="section-number">8.4.4. </span>减少寄存器使用<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>可以注意到在向共享内存中存储矩阵<span class="math notranslate nohighlight">\(A\)</span>的数据块是按照行优先的数据排布进行的，而对此共享内存的读取是逐行读取的。可以将矩阵<span class="math notranslate nohighlight">\(A\)</span>的数据块在共享内存中数据按照列优先的形式排布，这样可以减少循环及循环变量从而带来寄存器使用数量减少进而带来性能提升。</p>
<p>完整代码见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_transpose_smem.cu">gemm_transpose_smem.cu</a>。</p>
<p>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">0.610</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">14083.116</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>使用Nsight Compute分析有以下观察发现主要的变化：<code class="docutils literal notranslate"><span class="pre">Occupancy</span></code>
提升1.3%，而带来此提升的原因是寄存器使用111个，相比上一个GPU核函数使用128个寄存器减少了17个，从而带来了性能提升。但这个变化会因为GPU架构不同导致有不同的变化，同时可以观察到
<code class="docutils literal notranslate"><span class="pre">STS</span></code> 指令数量提升且带来一些 bank confilct
，因此在其他GPU架构上此改动可能不会带来正面影响。</p>
</div>
<div class="section" id="id6">
<h2><span class="section-number">8.4.5. </span>隐藏共享内存读取延迟<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>在GPU中使用指令 <code class="docutils literal notranslate"><span class="pre">LDS</span></code>
读取共享内存中的数据，在这条指令发出后并不会等待数据读取到寄存器后再执行下一条语句，只有执行到依赖
<code class="docutils literal notranslate"><span class="pre">LDS</span></code>
指令读取的数据的指令时才会等待读取的完成。而在上一小节中，在内层<span class="math notranslate nohighlight">\(tileK\)</span>次循环中，每次发射完读取共享内存的指令之后就会立即执行依赖于读取数据的数学运算，这样就会导致计算单元等待数据从共享内存的读取，如
<a class="reference internal" href="#use-smem-pipeline"><span class="std std-numref">图8.4.7</span></a>
所示。事实上，对共享内存的访问周期能多达几十个时钟周期，而计算指令的执行往往只有几个时钟周期，因此通过一定方式隐藏对共享内存的访问会取得不小的收益。可以通过重新优化流水线隐藏一定的数据读取延迟。如图
<a class="reference internal" href="#hide-smem-latency"><span class="std std-numref">图8.4.8</span></a>
所示，可以在内层的<span class="math notranslate nohighlight">\(tileK\)</span>次循环中每次循环开始时读取发射下一次内层循环数据的读取指令。由于在执行本次运算时计算指令并不依赖于下一次循环的数据，因此计算过程不会等待之前发出的读取下一次内层循环数据的指令。</p>
<div class="figure align-default" id="id15">
<span id="use-smem-pipeline"></span><a class="reference internal image-reference" href="../_images/use_smem_pipeline.png"><img alt="../_images/use_smem_pipeline.png" src="../_images/use_smem_pipeline.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.7 </span><span class="caption-text">上一个GPU核函数的流水线</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id16">
<span id="hide-smem-latency"></span><a class="reference internal image-reference" href="../_images/hide_smem_latency.png"><img alt="../_images/hide_smem_latency.png" src="../_images/hide_smem_latency.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.8 </span><span class="caption-text">隐藏共享内存读取延迟的流水线</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p>完整代码见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_hide_smem_latency.cu">gemm_hide_smem_latency.cu</a>。</p>
<p>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">0.585</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">14686.179</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>使用Nsight Compute观察发现：相比上一个GPU核函数，指标
<code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Short</span> <span class="pre">Scoreboard</span></code>
减少了67%。而此前提过GPU内存读写指令发出后并不会等待数据读取到寄存器后再执行下一条语句，但是会在Scoreboard设置符号并在完成读取后置回符号，等到之后有数据依赖的指令执行前会等待Scoreboard中符号的置回。所以这里
<code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Short</span> <span class="pre">Scoreboard</span></code> 的减少充分说明了内存延迟是有效的。</p>
</div>
<div class="section" id="id7">
<h2><span class="section-number">8.4.6. </span>隐藏全局内存读取延迟<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>上一小节中介绍了对共享内存读取流水线优化的方法，事实上，GPU再读取全局内存中使用的指令
<code class="docutils literal notranslate"><span class="pre">LDG</span></code> 也有与共享内存读取指令 <code class="docutils literal notranslate"><span class="pre">LDS</span></code>
类似的行为特性。因此类似的在<span class="math notranslate nohighlight">\(\frac{K}{tileK}\)</span>次外层循环中每次循环开始时发出下一次外层循环需要的矩阵<span class="math notranslate nohighlight">\(A\)</span>中的数据块的读取指令，而本次外循环的整个内层循环过程中不依赖下一次外循环的数据，因此本次外循环的内循环过程中不会等待对下一次外层循环需要的矩阵<span class="math notranslate nohighlight">\(A\)</span>中的数据块的读取指令完成，从而实现隐藏全局内存读取延迟的目的。具体流水线可视化见
<a class="reference internal" href="#hide-global-latency"><span class="std std-numref">图8.4.9</span></a> 。</p>
<p>上一小节中介绍了对共享内存读取流水线优化的方法，事实上，GPU在读取全局内存中使用的指令
<code class="docutils literal notranslate"><span class="pre">LDG</span></code> 也有与共享内存读取指令 <code class="docutils literal notranslate"><span class="pre">LDS</span></code>
类似的行为特性。因此类似的在<span class="math notranslate nohighlight">\(\frac{K}{tileK}\)</span>次外层循环中每次循环开始时发出下一次外层循环需要的矩阵<span class="math notranslate nohighlight">\(A\)</span>中的数据块的读取指令，而本次外循环的整个内层循环过程中不依赖下一次外循环的数据，因此本次外循环的内循环过程中不会等待对下一次外层循环需要的矩阵<span class="math notranslate nohighlight">\(A\)</span>中的数据块的读取指令完成，从而实现隐藏全局内存读取延迟的目的。此外，可以让内层循环先执行<span class="math notranslate nohighlight">\(tileK - 1\)</span>次，在最后一次执行前将
<code class="docutils literal notranslate"><span class="pre">buffer</span></code> 中的数据写入 <code class="docutils literal notranslate"><span class="pre">tile</span></code>
，其后再执行内层循环的最后一次迭代，这样能更进一步隐藏向 <code class="docutils literal notranslate"><span class="pre">tile</span></code>
写入的内存延迟。具体流水线可视化见图 <a class="reference internal" href="#hide-global-latency"><span class="std std-numref">图8.4.9</span></a> 。</p>
<div class="figure align-default" id="id17">
<span id="hide-global-latency"></span><a class="reference internal image-reference" href="../_images/hide_global_latency.png"><img alt="../_images/hide_global_latency.png" src="../_images/hide_global_latency.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图8.4.9 </span><span class="caption-text">隐藏全局内存读取延迟的流水线</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>完整代码见<a class="reference external" href="https://github.com/openmlsys/openmlsys-cuda/blob/main/gemm_final.cu">gemm_final.cu</a>。</p>
<p>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">0.542</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Average</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">15838.302</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>使用Nsight Compute分析可以观察到指标 <code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Long</span> <span class="pre">Scoreboard</span></code>
减少了67%，与上一小结的 <code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Short</span> <span class="pre">Scoreboard</span></code>
概念相对应，<code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Long</span> <span class="pre">Scoreboard</span></code>
主要是针对全局内存的指标。该指标的显著减少充分说明预取数据可以在一定程度上隐藏全局内存的读取。</p>
</div>
<div class="section" id="cublas">
<span id="practise-cublas"></span><h2><span class="section-number">8.4.7. </span>与cuBLAS对比<a class="headerlink" href="#cublas" title="Permalink to this headline">¶</a></h2>
<p>按照节 <a class="reference internal" href="accelerator_programming.html#sec-accelerator-use-cublas"><span class="std std-numref">图8.3.5</span></a>
中介绍的cuBLAS的接口使用方法，可以很容易地写出代码使用cuBLAS完成矩阵乘法，如代码
<a class="reference internal" href="#practise-cublas"><span class="std std-numref">8.4.7节</span></a> 所示。</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span> <span class="nf">cublasGemm</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span> <span class="kt">float</span> <span class="n">alf</span><span class="p">,</span> <span class="kt">float</span> <span class="n">bet</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">lda</span> <span class="o">=</span> <span class="n">N</span><span class="p">,</span> <span class="n">ldb</span> <span class="o">=</span> <span class="n">K</span><span class="p">,</span> <span class="n">ldc</span> <span class="o">=</span> <span class="n">N</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">alpha</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">alf</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">beta</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">bet</span><span class="p">;</span>
  <span class="n">cublasHandle_t</span> <span class="n">handle</span><span class="p">;</span>
  <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>
  <span class="n">cublasSgemm</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">CUBLAS_OP_N</span><span class="p">,</span> <span class="n">CUBLAS_OP_N</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">lda</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">ldb</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">ldc</span><span class="p">);</span>
  <span class="n">cublasDestroy</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>需要注意的是cuBLAS默认矩阵在GPU中是按列优先存储的，而我们的矩阵是按行优先存储的，而两者可以通过转置相互转换，所以<span class="math notranslate nohighlight">\(A\times B = (B^T\times A^T)^T\)</span>，因此在输入时需要调整矩阵的顺序，即可保证输出结果仍是行优先矩阵。</p>
<p>测试得到以下结果：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">Error</span><span class="p">:</span> <span class="mf">0.000092</span>
<span class="n">Average</span> <span class="n">Time</span><span class="p">:</span> <span class="mf">0.613</span> <span class="n">ms</span><span class="p">,</span> <span class="n">Throughput</span><span class="p">:</span> <span class="mf">14002.600</span> <span class="n">GFLOPS</span>
</pre></div>
</div>
<p>使用Nsight Compute分析发现 <code class="docutils literal notranslate"><span class="pre">LDG</span></code> 和 <code class="docutils literal notranslate"><span class="pre">STS</span></code>
等指令使用较多，导致指令发射压力较大，具体体现在 <code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Wait</span></code> 与
<code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Dispatch</span> <span class="pre">Stall</span></code> 指标相比较差。但其他指标诸如
<code class="docutils literal notranslate"><span class="pre">Stall</span> <span class="pre">Long</span> <span class="pre">Scoreboard</span></code> 等cuBLAS更优，但总体上我们略胜一筹。
尽管我们的代码相比cuBLAS已经取得了一定的性能提升，但是需要强调的是cuBLAS内部为各种不同的矩阵尺寸以及不同的设备实现了若干不同的GPU核函数，我们实现的核函数在其他尺寸或其他设备设备上性能可能无法取得此加速比。</p>
</div>
<div class="section" id="id8">
<h2><span class="section-number">8.4.8. </span>小结<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>要实现一个高性能算子需要依照硬件特性适应性进行若干优化。本节优化策略可总结为以下几点：</p>
<ul class="simple">
<li><p>并行资源映射——提高并行性：将多层级的并行资源（<code class="docutils literal notranslate"><span class="pre">block</span></code>
、<code class="docutils literal notranslate"><span class="pre">warp</span></code> 、<code class="docutils literal notranslate"><span class="pre">thread</span></code>
）与对应需要计算和搬移的数据建立映射关系，提高程序并行性。将可并行的计算和数据搬移操作映射到并行资源上，对于广义矩阵乘法实例，在节~ref{sec-accelerator-naive<code class="docutils literal notranslate"><span class="pre">朴素实现的例子中，令每个</span></code>block<code class="docutils literal notranslate"><span class="pre">与矩阵$C$中的一个矩阵块建立映射关系，每个</span></code>thread`
与矩阵块中的一个元素建立映射关系。</p></li>
<li><p>优化内存结构——减小访存延迟：观察计算过程中同一个<code class="docutils literal notranslate"><span class="pre">block</span></code>
中数据复用的情况，将复用的数据被如共享内存、寄存器等高性能体系结构存储下来，以此提高吞吐量。如在节
:numref<code class="docutils literal notranslate"><span class="pre">sec-accelerator-naive</span></code>
中将矩阵<span class="math notranslate nohighlight">\(A\)</span>与矩阵<span class="math notranslate nohighlight">\(B\)</span>中会被同一个 <code class="docutils literal notranslate"><span class="pre">block</span></code> 内不同
<code class="docutils literal notranslate"><span class="pre">thread</span></code> 共同访问的数据缓存到共享内存中。</p></li>
<li><p>优化指令执行——减小指令发射开销：使用 <code class="docutils literal notranslate"><span class="pre">#pragma</span> <span class="pre">unroll</span></code>
功能进行循环展开来提升指令级并行，减少逻辑判断；使用向量化加载指令以提高带宽等，对于Ampere架构，最大向量化加载指令为
<code class="docutils literal notranslate"><span class="pre">LDG.E.128</span></code> ，可以采用 <code class="docutils literal notranslate"><span class="pre">float4</span></code> 类型的数据进行读取。</p></li>
<li><p>优化访存流水线——隐藏访存延迟：在进行内存结构变化（矩阵数据搬移）时，可以优化访存流水线，在数据搬移的间隔执行计算操作以隐藏数据搬移的延迟。</p></li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">8.4. 加速器实践</a><ul>
<li><a class="reference internal" href="#sec-accelerator-naive">8.4.1. 广义矩阵乘法的朴素实现</a></li>
<li><a class="reference internal" href="#id3">8.4.2. 提高计算强度</a></li>
<li><a class="reference internal" href="#sec-accelerator-use-smem">8.4.3. 使用共享内存缓存复用数据</a></li>
<li><a class="reference internal" href="#id5">8.4.4. 减少寄存器使用</a></li>
<li><a class="reference internal" href="#id6">8.4.5. 隐藏共享内存读取延迟</a></li>
<li><a class="reference internal" href="#id7">8.4.6. 隐藏全局内存读取延迟</a></li>
<li><a class="reference internal" href="#cublas">8.4.7. 与cuBLAS对比</a></li>
<li><a class="reference internal" href="#id8">8.4.8. 小结</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="accelerator_programming.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>8.3. 加速器基本编程原理</div>
         </div>
     </a>
     <a id="button-next" href="summary.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>8.5. 总结</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>