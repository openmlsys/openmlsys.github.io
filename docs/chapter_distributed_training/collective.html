<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>11.5. 集合通信 &#8212; 机器学习系统：设计和实现 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.6. 参数服务器" href="parameter_servers.html" />
    <link rel="prev" title="11.4. 机器学习集群架构" href="cluster.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">11. </span>分布式训练</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">11.5. </span>集合通信</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_distributed_training/collective.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://openmlsys.github.io/html-en">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/design.html">2.2. 机器学习框架的设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/architecture.html">2.3. 机器学习框架的基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/ecosystem.html">2.4. 机器学习系统生态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/readers.html">2.5. 图书结构和读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_programming_paradigm.html">3.5. 机器学习框架的编程范式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. AI编译器和前端技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ai_compiler_design_principle.html">6.1. AI编译器设计原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.2. AI编译器前端技术概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.3. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.4. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.5. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.6. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/op_compiler.html">7.6. 算子编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">8. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_practise.html">8.4. 加速器实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">8.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id2">8.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id3">8.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html#id2">9.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">10. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">11. 分布式训练</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="methods.html">11.2. 实现方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="methods.html#id6">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster.html">11.4. 机器学习集群架构</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.5. 集合通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="parameter_servers.html">11.6. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">11.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">11.8. 拓展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.1. 系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/multi_stage_recommender_system.html">13.2. 多阶段推荐系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/model_update.html">13.3. 模型更新</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/case_study.html">13.4. 案例分析：支持在线模型更新的大型推荐系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id3">13.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">15.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id17">16.4. 可解释AI系统及实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id21">16.5. 未来可解释AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id22">16.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rl_sys/index.html">17. 机器人系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">17.1. 机器人系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros_code_ex.html">17.3. 案例分析：使用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/summary.html">17.4. 总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">4. 参考文献</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="机器学习系统：设计和实现"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">2. 导论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/applications.html">2.1. 机器学习应用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/design.html">2.2. 机器学习框架的设计目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/architecture.html">2.3. 机器学习框架的基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/ecosystem.html">2.4. 机器学习系统生态</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_introduction/readers.html">2.5. 图书结构和读者</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_programming_interface/index.html">3. 编程接口</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/development_history.html">3.1. 机器学习系统编程模型的演进</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">3.2. 机器学习工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">3.3. 定义深度神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">3.4. C/C++编程接口</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/ml_programming_paradigm.html">3.5. 机器学习框架的编程范式</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html">3.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_programming_interface/summary.html#id2">3.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational_graph/index.html">4. 计算图</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">4.1. 计算图的设计背景和作用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">4.2. 计算图的基本构成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">4.3. 计算图的生成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">4.4. 计算图的调度</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html">4.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational_graph/summary.html#id2">4.6. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_advanced/index.html">5. 第二部分：进阶篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_frontend_and_ir/index.html">6. AI编译器和前端技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ai_compiler_design_principle.html">6.1. AI编译器设计原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">6.2. AI编译器前端技术概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">6.3. 中间表示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/ad.html">6.4. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">6.5. 类型系统和静态分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">6.6. 常见前端编译优化方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html">6.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">6.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_backend_and_runtime/index.html">7. 编译器后端和运行时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/overview.html">7.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">7.2. 计算图优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">7.3. 算子选择</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">7.4. 内存分配</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">7.5. 计算调度与执行</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/op_compiler.html">7.6. 算子编译器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html">7.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">7.8. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_accelerator/index.html">8. 硬件加速器</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">8.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">8.2. 加速器基本组成原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">8.3. 加速器基本编程原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/accelerator_practise.html">8.4. 加速器实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html">8.5. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id2">8.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_accelerator/summary.html#id3">8.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_data_processing/index.html">9. 数据处理框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/requirements.html">9.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/program_model.html">9.2. 易用性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/performance.html">9.3. 高效性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/data_order.html">9.4. 保序性设计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/extension.html">9.5. 单机数据处理性能的扩展</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html">9.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_data_processing/summary.html#id2">9.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_model_deployment/index.html">10. 模型部署</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">10.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">10.2. 训练模型到推理模型的转换及优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_compression.html">10.3. 模型压缩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_inference.html">10.4. 模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/model_security.html">10.5. 模型的安全保护</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html">10.6. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_model_deployment/summary.html#id2">10.7. 扩展阅读</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">11. 分布式训练</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="overview.html">11.1. 系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="methods.html">11.2. 实现方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="methods.html#id6">11.3. 流水线并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="cluster.html">11.4. 机器学习集群架构</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.5. 集合通信</a></li>
<li class="toctree-l2"><a class="reference internal" href="parameter_servers.html">11.6. 参数服务器</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html">11.7. 总结</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary.html#id2">11.8. 拓展阅读</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface_extension/index.html">12. 第三部分：拓展篇</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender_system/index.html">13. 深度学习推荐系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/system_architecture.html">13.1. 系统基本组成</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/multi_stage_recommender_system.html">13.2. 多阶段推荐系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/model_update.html">13.3. 模型更新</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/case_study.html">13.4. 案例分析：支持在线模型更新的大型推荐系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html">13.5. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id2">13.6. 扩展阅读</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender_system/summary.html#id3">13.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_federated_learning/index.html">14. 联邦学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/overview.html">14.1. 概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">14.2. 横向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">14.3. 纵向联邦学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">14.4. 隐私加密算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/outlook.html">14.5. 展望</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_federated_learning/summary.html">14.6. 小结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement_learning/index.html">15. 强化学习系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">15.1. 强化学习介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">15.2. 单节点强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">15.3. 分布式强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl.html">15.4. 多智能体强化学习</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">15.5. 多智能体强化学习系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html">15.6. 小结</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">15.7. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_explainable_AI/index.html">16. 可解释性AI系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">16.1. 背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">16.2. 可解释AI定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">16.3. 可解释AI算法现状介绍</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id17">16.4. 可解释AI系统及实践</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id21">16.5. 未来可解释AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id22">16.6. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_rl_sys/index.html">17. 机器人系统</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">17.1. 机器人系统概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros.html">17.2. 通用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/ros_code_ex.html">17.3. 案例分析：使用机器人操作系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_rl_sys/summary.html">17.4. 总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../appendix_machine_learning_introduction/index.html">附录：机器学习介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">1. 神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">2. 梯度下降与反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">3. 经典机器学习方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">4. 参考文献</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">11.5. </span>集合通信<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<p>下面讨论如何利用集合通信在机器学习集群中实现分布式训练系统。作为并行计算的一个重要概念，集合通信经常被用来构建高性能的单程序流/多数据流（Single
Program-Multiple Data,
SPMD）程序。接下来，首先会介绍集合通信中的常见算子。然后描述如何使用AllReduce算法解决分布式训练系统中网络瓶颈，并且讨论AllReduce算法在不同网络拓扑结构下的差异性以及重要性能指标的计算方法。最后介绍现有机器学习系统对不同集合通信算法的支持</p>
<div class="section" id="id2">
<h2><span class="section-number">11.5.1. </span>常见集合通信算子<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>下面首先定义一个简化的集合通信模型，然后引入常见的集合通信算子：Broadcast、Reduce、AllGather、Scatter和
AllReduce。需要指出的是，在分布式机器学习的实际场景下，人们还会使用许多其他的集合通信算子，如ReduceScatter、Prefix
Sum、Barrier、All-to-All等，但由于篇幅限制，便不再赘述。</p>
<div class="section" id="id3">
<h3><span class="section-number">11.5.1.1. </span>通信模型<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>假定在一个分布式机器学习集群中，存在<span class="math notranslate nohighlight">\(p\)</span>个计算设备，并由一个网络来连接所有的设备。每个设备有自己的独立内存，并且所有设备间的通信都通过该网络传输。同时，每个设备都有一个编号<span class="math notranslate nohighlight">\(i\)</span>，其中<span class="math notranslate nohighlight">\(i\)</span>的范围从<span class="math notranslate nohighlight">\(1\)</span>到<span class="math notranslate nohighlight">\(p\)</span>。
设备之间的点对点（Point-to-Point, P2P）通信由全双工传输（Full-Duplex
Transmission)实现。该通信模型的基本行为可以定义如下：</p>
<ul class="simple">
<li><p>每次通信有且仅有一个发送者（Sender）和一个接收者（Receiver）。在某个特定时刻，每个设备仅能至多发送或接收一个消息（Message）。每个设备可以同时发送一个消息和接收一个消息。一个网络中可以同时传输多个来自于不同设备的消息。</p></li>
<li><p>传输一个长度为<span class="math notranslate nohighlight">\(l\)</span>个字节（Byte）的消息会花费<span class="math notranslate nohighlight">\(a+b \times l\)</span>的时间，其中<span class="math notranslate nohighlight">\(a\)</span>代表延迟（Latency），即一个字节通过网络从一个设备出发到达另一个设备所需的时间；<span class="math notranslate nohighlight">\(b\)</span>代表传输延迟（Transmission
Delay），即传输一个具有<span class="math notranslate nohighlight">\(l\)</span>个字节的消息所需的全部时间。前者取决于两个设备间的物理距离（如跨设备、跨机器、跨集群等），后者取决于通信网络的带宽。需要注意的是，这里简化了传输延迟的定义，其并不考虑在真实网络传输中会出现的丢失的消息（Dropped
Message）和损坏的消息（Corrupted Message）的情况。</p></li>
</ul>
<p>根据上述通信模型，我们可以定义集合通信算子，并且分析算子的通信性能。下面介绍一些常见的集合通信算子。</p>
</div>
<div class="section" id="broadcast">
<h3><span class="section-number">11.5.1.2. </span>Broadcast<a class="headerlink" href="#broadcast" title="Permalink to this heading">¶</a></h3>
<div class="figure align-default" id="id9">
<span id="ch10-collective-operators"></span><a class="reference internal image-reference" href="../_images/ch10-collective-operators.png"><img alt="../_images/ch10-collective-operators.png" src="../_images/ch10-collective-operators.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图11.5.1 </span><span class="caption-text">常用集合通信算子</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</div>
<p>一个分布式机器学习系统经常需要将一个设备<span class="math notranslate nohighlight">\(i\)</span>上的模型参数或者配置文件广播（Broadcast）给其余全部设备。因此，可以把Broadcast算子定义为从编号为<span class="math notranslate nohighlight">\(i\)</span>的设备发送长度为<span class="math notranslate nohighlight">\(l\)</span>字节的消息给剩余的<span class="math notranslate nohighlight">\(p-1\)</span>个设备。
<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>展示了设备1（在三个设备的集群里）调用Broadcast的初始和结束状态。</p>
<p>一种简单实现Broadcast的算法是在设备<span class="math notranslate nohighlight">\(i\)</span>上实现一个循环，该循环使用<span class="math notranslate nohighlight">\(p-1\)</span>次Send/Receive操作来将数据传输给相应设备。然而，该算法不能达到并行通信的目的（该算法只有<span class="math notranslate nohighlight">\((a+b \times l) \times (p-1)\)</span>的线性时间复杂度）。为此，可以利用分治思想对上述简单实现的Broadcast算法进行优化。假设所有的设备可以重新对编号进行排列，使得Broadcast的发送者为编号为<span class="math notranslate nohighlight">\(1\)</span>的设备。同时，为了简化计算过程，假设对某个自然数<span class="math notranslate nohighlight">\(n\)</span>，<span class="math notranslate nohighlight">\(p = 2^n\)</span>。
现在，可以通过从1 向 <span class="math notranslate nohighlight">\(p/2\)</span>
发送一次信息把问题转换为两个大小为<span class="math notranslate nohighlight">\(p/2\)</span>的子问题：编号为1的设备对编号1到编号<span class="math notranslate nohighlight">\(p/2-1\)</span>
的Broadcast，以及编号为<span class="math notranslate nohighlight">\(p/2\)</span>的设备对编号<span class="math notranslate nohighlight">\(p/2\)</span>到编号<span class="math notranslate nohighlight">\(p\)</span>的Broadcast。我们便可以通过在这两个子问题上进行递归来完成这个算法，并把临界条件定义为编号为<span class="math notranslate nohighlight">\(i\)</span>的设备在<span class="math notranslate nohighlight">\([i,i]\)</span>这个区间中的Broadcast。此时，由于<span class="math notranslate nohighlight">\(i\)</span>本身已经拥有该信息，不需要做任何操作便可直接完成Broadcast。这个优化后的算法为<span class="math notranslate nohighlight">\((a+b \times l) \times \log p\)</span>
时间复杂度，因为在算法的每一阶段（编号为<span class="math notranslate nohighlight">\(t\)</span>），有<span class="math notranslate nohighlight">\(2^t\)</span>个设备在并行运行Broadcast算子。同时，算法一定会在<span class="math notranslate nohighlight">\(\log p\)</span>
步之内结束。</p>
</div>
<div class="section" id="reduce">
<h3><span class="section-number">11.5.1.3. </span>Reduce<a class="headerlink" href="#reduce" title="Permalink to this heading">¶</a></h3>
<p>在分布式机器学习系统中，另一个常见的操作是将不同设备上的计算结果进行聚合（Aggregation）。例如，将每个设备计算的本地梯度进行聚合，计算梯度之和（Summation）。这些聚合函数（表达为<span class="math notranslate nohighlight">\(f\)</span>）往往符合结合律（Associative
Law）和交换律（Commutative
Law）。这些函数由全部设备共同发起，最终聚合结果存在编号为<span class="math notranslate nohighlight">\(i\)</span>的设备上。常见聚合函数有加和、乘积、最大值和最小值。集合通信将这些函数表达为Reduce算子。
<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>展示了设备1调用Reduce来进行加和的初始和结束状态。</p>
<p>一个简易的Reduce的优化实现同样可以用分治思想来实现，即把<span class="math notranslate nohighlight">\(1\)</span>到<span class="math notranslate nohighlight">\(p/2-1\)</span>的Reduce结果存到编号为<span class="math notranslate nohighlight">\(1\)</span>的设备中，然后把<span class="math notranslate nohighlight">\(p/2\)</span>到<span class="math notranslate nohighlight">\(p\)</span>的Reduce结果存到<span class="math notranslate nohighlight">\(p/2\)</span>上。最后，可以把<span class="math notranslate nohighlight">\(p/2\)</span>的结果发送至<span class="math notranslate nohighlight">\(1\)</span>，执行<span class="math notranslate nohighlight">\(f\)</span>，并把最后的结果存至<span class="math notranslate nohighlight">\(i\)</span>。假设<span class="math notranslate nohighlight">\(f\)</span>的运行时间复杂度为常数并且其输出信息的长度<span class="math notranslate nohighlight">\(l\)</span>不改变，Reduce的时间复杂度仍然为<span class="math notranslate nohighlight">\((a+b \times l) \times \log p\)</span>。</p>
</div>
<div class="section" id="allreduce">
<h3><span class="section-number">11.5.1.4. </span>AllReduce<a class="headerlink" href="#allreduce" title="Permalink to this heading">¶</a></h3>
<p>集合通信通过引入AllReduce算子，从而将Reduce函数<span class="math notranslate nohighlight">\(f\)</span>的结果存至所有设备上。<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>展示了设备1，设备2和设备3共同调用AllReduce来进行加和的初始和结束状态。</p>
<p>一种简单的AllReduce实现方法是首先调用Reduce算法并将聚合结果存到编号为<span class="math notranslate nohighlight">\(1\)</span>的设备上。然后，再调用Broadcast算子将聚合结果广播到所有的设备。这种简单的AllReduce实现的时间复杂度为<span class="math notranslate nohighlight">\((a+b \times l) \times \log p\)</span>。</p>
</div>
<div class="section" id="gather">
<h3><span class="section-number">11.5.1.5. </span>Gather<a class="headerlink" href="#gather" title="Permalink to this heading">¶</a></h3>
<p>Gather算子可以将全部设备的数据全部收集（Gather）到编号为<span class="math notranslate nohighlight">\(i\)</span>的设备上。
<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>展示了设备1调用Gather来收集全部设备的数据的初始和结束状态。</p>
<p>在收集函数（Gather
Function）符合结合律和交换律的情况下，可以通过将其设为Reduce算子中的<span class="math notranslate nohighlight">\(f\)</span>来实现Gather算子。但是，在这种情况下，无论是基于链表还是数组的实现，在每一步的Reduce操作中<span class="math notranslate nohighlight">\(f\)</span>的时间复杂度和输出长度<span class="math notranslate nohighlight">\(l\)</span>都发生了改变。因此，Gather的时间复杂度是<span class="math notranslate nohighlight">\(a \times \log p + (p-1) \times b \times l\)</span>。这是因为在算法的每一阶段<span class="math notranslate nohighlight">\(t\)</span>，传输的信息长度为<span class="math notranslate nohighlight">\(2^{t} \times l\)</span>。</p>
</div>
<div class="section" id="allgather">
<h3><span class="section-number">11.5.1.6. </span>AllGather<a class="headerlink" href="#allgather" title="Permalink to this heading">¶</a></h3>
<p>AllGather算子会把收集的结果分发到全部的设备上。
<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>展示了设备1，设备2和设备3共同调用AllGather的初始和结束状态。</p>
<p>在这里，一个简单的方法是使用Gather和Broadcast算子把聚合结果先存到编号为1的设备中，再将其广播到剩余的设备上。这会产生一个<span class="math notranslate nohighlight">\(a \times \log p + (p-1) \times b \times l + (a+p \times l \times b) \times \log p\)</span>的时间复杂度，因为在广播时，如果忽略链表/数组实现所带来的额外空间开销，每次通信的长度为<span class="math notranslate nohighlight">\(pl\)</span>而不是<span class="math notranslate nohighlight">\(l\)</span>。简化后，得到了一个<span class="math notranslate nohighlight">\(a \times \log p + p \times l \times b \times \log p\)</span>
的时间复杂度。在一个基于<a class="reference external" href="https://link.springer.com/book/10.1007/978-3-030-25209-0">超立方体</a>的算法下，可以将其进一步优化到和Gather算子一样的时间复杂度<span class="math notranslate nohighlight">\(a \times \log p + (p-1) \times b \times l\)</span>，由于篇幅问题此处便不再赘述。</p>
</div>
<div class="section" id="scatter">
<h3><span class="section-number">11.5.1.7. </span>Scatter<a class="headerlink" href="#scatter" title="Permalink to this heading">¶</a></h3>
<p>Scatter算子可以被视作Gather算子的逆运算：把一个存在于编号为<span class="math notranslate nohighlight">\(i\)</span>的设备上，长度为<span class="math notranslate nohighlight">\(p\)</span>（信息长度为<span class="math notranslate nohighlight">\(p \times l\)</span>）的链式数据结构<span class="math notranslate nohighlight">\(L\)</span>中的值分散到每个设备上，使得编号为<span class="math notranslate nohighlight">\(i\)</span>的设备会得到<span class="math notranslate nohighlight">\(L[i]\)</span>的结果。
<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>展示了设备1调用Scatter的初始和结束状态。</p>
<p>可以通过模仿Gather算法设计一个简易的Scatter实现：每一步的运算中，我们把现在的子链继续对半切分，并把前半段和后半段作为子问题进行递归。这时候，在算法的每一阶段<span class="math notranslate nohighlight">\(t\)</span>，传输的信息长度为<span class="math notranslate nohighlight">\(l \times 2^{(m-t)}\)</span>，其中<span class="math notranslate nohighlight">\(m\)</span>是算法总共运行的步骤，不会超过<span class="math notranslate nohighlight">\(\log p\)</span>
（见Broadcast算子的介绍）。最终，Scatter算子的简易实现和Gather算子一样都有<span class="math notranslate nohighlight">\(a \times \log p + (p-1) \times b \times l\)</span>
的时间复杂度。在机器学习系统中，Scatter算子经常同时被用于链式数据结构和可切分的数据结构，例如张量在一个维度上的<span class="math notranslate nohighlight">\(p\)</span>等分等。</p>
</div>
</div>
<div class="section" id="id4">
<h2><span class="section-number">11.5.2. </span>基于AllReduce的梯度平均算法<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h2>
<p>下面讨论如何利用AllReduce算子实现大型集群中的高效梯度平均。首先，参照前面的分析，可以考虑一种简单的计算平均梯度的方法：在集群中分配一个设备收集本地梯度，并在计算平均梯度后再将其广播到全部的设备。这种做法易于实现，但是引入了两个问题。首先，多台设备同时给该聚合设备发送数据时，聚合设备会因严重的带宽不足产生网络拥塞。其次，单台设备需要负担大量的梯度平均计算，而受限于单台设备上的有限算力，这种计算往往会受限于算力瓶颈。</p>
<p>为了解决上述问题，可以引入AllReduce算子的Reduce-Broadcast实现来优化算法，其设计思路是：通过让全部的节点参与到梯度的网络通信和平均计算中，将巨大的网络和算力开销均摊给全部节点。这种做法可以解决先前单个梯度聚合节点的问题。假设有<span class="math notranslate nohighlight">\(M\)</span>个设备，每个设备存有一个模型副本，该模型由<span class="math notranslate nohighlight">\(N\)</span>个参数/梯度构成。那么按照AllReduce算子的要求，需要先将全部的参数按照设备数量切分成<span class="math notranslate nohighlight">\(M\)</span>个分区（Partition），使得每个分区具有<span class="math notranslate nohighlight">\(N/M\)</span>个参数。首先给出这个算法的初始和结束状态。如
<a class="reference internal" href="#ch10-collective-operators"><span class="std std-numref">图11.5.1</span></a>的AllReduce的例子所示，该例子含有3个设备。在每个设备有一个模型副本的情况下，这个副本有3个参数。那么按照AllReduce的分区方法，参数会被划分成3个分区（3个设备），而每一个分区则有1个参数（<span class="math notranslate nohighlight">\(N/M\)</span>，<span class="math notranslate nohighlight">\(N\)</span>代表3个参数，<span class="math notranslate nohighlight">\(M\)</span>代表3个设备）。在这个例子中，假定设备1拥有参数2,4,6，设备2拥有参数1,2,3，设备3拥有参数4,8,12，那么在使用一个AllReduce算子进行计算过后，全部的设备都将拥有梯度相加后的结果7,14,21，其中分区1的结果7是由3个设备中分区1的初始结果相加而成（7
= 1 + 2 +
4）。为了计算平均梯度，每个设备只需要在最后将梯度之和除以设备数量即可（分区1的最终结果为7除以3）。</p>
<div class="figure align-default" id="id10">
<span id="ch10-allreduce-process"></span><a class="reference internal image-reference" href="../_images/ch10-allreduce-process.png"><img alt="../_images/ch10-allreduce-process.png" src="../_images/ch10-allreduce-process.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图11.5.2 </span><span class="caption-text">AllReduce算法的过程</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>AllReduce算子会把梯度的计算拆分成<span class="math notranslate nohighlight">\(M-1\)</span>个Reduce算子和<span class="math notranslate nohighlight">\(M-1\)</span>个Broadcast算子（其中<span class="math notranslate nohighlight">\(M\)</span>是节点的数量）。其中，Reduce算子用于计算出梯度的加和，Broadcast算子用于把梯度之和广播给全部的节点。
<a class="reference internal" href="#ch10-allreduce-process"><span class="std std-numref">图11.5.2</span></a>展示了一个AllReduce算子的执行过程。AllReduce算子由Reduce算子开始，在第一个Reduce算子中，AllReduce算子会对全部节点进行配对（Pairing），让它们共同完成梯度相加的操作。在
<a class="reference internal" href="#ch10-allreduce-process"><span class="std std-numref">图11.5.2</span></a>的第一个Reduce算子中，设备1和设备2进行了配对共同对分区1的数据相加。其中，设备2把本地的梯度数据1发送给设备1，设备1将接收到的梯度数据1和本地的分区1内的梯度数据2进行相加，计算出中间梯度相加的结果3。与此同时，设备1和设备3进行配对，共同完成对分区3的数据相加。而设备3和设备2进行配对，共同完成对于分区2的数据相加。</p>
<p>上述的Reduce算子对梯度的分布式计算实现了以下的性能优化:</p>
<ul class="simple">
<li><p><strong>网络优化：</strong>
全部设备都同时在接收和发送数据，利用起了每个设备的入口（Ingress）和出口（Egress）带宽。因此在AllReduce算法的过程中，可利用的带宽是<span class="math notranslate nohighlight">\(M * B\)</span>，其中<span class="math notranslate nohighlight">\(M\)</span>是节点数量，<span class="math notranslate nohighlight">\(B\)</span>是节点带宽，从而让系统实现网络带宽上的可扩展性。</p></li>
<li><p><strong>算力优化：</strong>
全部设备的处理器都参与了梯度相加的计算。因此在AllReduce算法的过程中，可利用的处理器是<span class="math notranslate nohighlight">\(M * P\)</span>，其中<span class="math notranslate nohighlight">\(M\)</span>是节点数量，<span class="math notranslate nohighlight">\(P\)</span>是单个设备的处理器数量，从而让系统实现计算上的可扩展性。</p></li>
<li><p><strong>负载均衡：</strong>
由于数据分区是平均划分的，因此每次设备分摊到的通信和计算开销是相等的。</p></li>
</ul>
<p>在接下来的Reduce算子中，AllReduce算法会对不同数据分区选择另外的配对方法。例如，在
<a class="reference internal" href="#ch10-allreduce-process"><span class="std std-numref">图11.5.2</span></a>
的第二个Reduce算子中，AllReduce算法会将设备1和设备3进行配对，负责分区1的数据相加。将设备1和设备2进行配对，负责分区2。将设备2和设备3进行配对，负责分区3。在一个3个节点的AllReduce集群里，在2个Reduce算子完成后，就计算出了每个分区的数据相加结果（分区1的数据相加结果7此时在设备3上，分区2的数据相加结果14此时在设备1上，分区3的数据相加结果21此时在设备2上）。</p>
<p>接下来，AllReduce算法将进入Broadcast阶段。这一阶段的过程和Reduce算子类似，核心区别是节点进行配对后，它们不再进行数据相加，而是将Reduce的计算结果进行广播。在
<a class="reference internal" href="#ch10-allreduce-process"><span class="std std-numref">图11.5.2</span></a>
中的第一个Broadcast算子中，设备1会将分区2的结果14直接写入设备3的分区2中。设备2会将分区3的结果21直接写入设备1中。设备3会将分区1的结果直接写入设备2中。在一个3个节点的AllReduce集群中，我们会重复2次Broadcast算子将每个分区的Reduce结果告知全部的节点。</p>
<p>在本节中，我们讨论了AllReduce的其中一种常用实现方法。根据集群网络拓扑的不同，人们也会用以下的方法来实现AllReduce：<a class="reference external" href="https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4">树形结构</a>，<a class="reference external" href="https://github.com/baidu-research/baidu-allreduce">环形结构</a>，<a class="reference external" href="https://arxiv.org/abs/1811.05233%7D">二维环面结构</a>以及<a class="reference external" href="https://github.com/NVIDIA/nccl/issues/320">CollNet</a>。在此我们不展开讨论。</p>
</div>
<div class="section" id="id5">
<h2><span class="section-number">11.5.3. </span>集合通信算法性能分析<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h2>
<p>在讨论集合通信算子的性能时，人们经常会使用一些数值化指标量化不同的算法实现。在计算点对点通信所需的时间时，会在信息长度上乘以一个系数<span class="math notranslate nohighlight">\(b\)</span>。这个数值化指标就是算法带宽（Algorithm
Bandwidth），泛指单位时间内执行操作（通信和计算等）的数量。一般计算公式为<span class="math notranslate nohighlight">\(b = s/t\)</span>，其中<span class="math notranslate nohighlight">\(s\)</span>代指操作的大小，<span class="math notranslate nohighlight">\(t\)</span>指操作指定的两个端点之间所经过的时间。以P2P通信举例，可以通过衡量一个大小已知的信息<span class="math notranslate nohighlight">\(m\)</span>在执行Send函数时所花的时间来确定两个设备之间网络的带宽。</p>
<p>前文提到，在计算点对点通信所需的时间是，会在信息长度之上乘以一个系数b。这个系数就是算法带宽，泛指单位时间内执行操作（通信，计算等）的数量。一般计算公式为<span class="math notranslate nohighlight">\(b = s/t\)</span>，其中<span class="math notranslate nohighlight">\(s\)</span>代指操作的大小，<span class="math notranslate nohighlight">\(t\)</span>指操作指定的两个端点之间所经过的时间。以点到点通信举例，我们可以通过衡量一个大小已知的信息<span class="math notranslate nohighlight">\(m\)</span>在执行send函数时所花的时间来确定两个处理单元之间网络的带宽。</p>
<p>虽然算法带宽的计算方法既简单又高效，但很难将其拓展至对于集合通信算子的带宽计算。这是因为，取决于具体算子和算法实现的不同，一个集合通信算子在执行过程中测得的算法带宽往往会远小于硬件本身的最高带宽。在实际运行相应的测试中，经常能观测到随着设备增加，算法带宽呈下降趋势。为了解决这一问题，NCCL提出了总线带宽（Bus
Bandwidth）这一数值化指标，将根据每个集合通信算子的分析所测得的算法带宽乘以一个校正系数（Correction
Factor），从而给出贴近实际硬件表现的带宽值。下面给出常见算子的校正系数：</p>
<ul class="simple">
<li><p>AllReduce：对于在设备<span class="math notranslate nohighlight">\(n_1, n_2, \cdots, n_p\)</span> 上的值
<span class="math notranslate nohighlight">\(v_1, v_2, \cdots, v_p\)</span> 计算
<span class="math notranslate nohighlight">\(v_1 o v_2 o \cdots o v_p\)</span>（其中<span class="math notranslate nohighlight">\(o\)</span>为符合结合律的算子），再存回每个设备中。在不考虑实际实现算法和网络拓扑的情况下，这个操作在理论上只需要<span class="math notranslate nohighlight">\(2 \times (p-1)\)</span>次数据传输，其中包含在每个设备上分开进行的<span class="math notranslate nohighlight">\(p-1\)</span>次
<span class="math notranslate nohighlight">\(o\)</span>的运算，以及最后 <span class="math notranslate nohighlight">\(p\)</span>
次最终数据值的广播，再减去第一个设备的运算和最后一个设备的广播对运行时间的影响。假设每个设备对于外界所有信息处理的带宽为<span class="math notranslate nohighlight">\(B\)</span>，可以得出对于<span class="math notranslate nohighlight">\(S\)</span>个在不同设备上的数据运行AllReduce算子能得到最优情况下的运行时间：<span class="math notranslate nohighlight">\(t = (2 \times S \times (p-1)) / (p*B)\)</span>，进行简化后可得
<span class="math notranslate nohighlight">\(B = (S/t) \times (2 \times (p-1)/p) = b (2 \times (p-1)/p)\)</span>。这里的
<span class="math notranslate nohighlight">\(2(p-1)/p\)</span>便是校正系数。</p></li>
<li><p>ReduceScatter：对于每个设备来说，可以把ReduceScatter理解为只执行AllReduce中的聚合部分。对此，只需要考虑上面分析中的<span class="math notranslate nohighlight">\(n-1\)</span>次<span class="math notranslate nohighlight">\(op\)</span>的运算，整理后可得<span class="math notranslate nohighlight">\(B = (S/t) \times ((p-1)/p) = b \times ((p-1)/p)\)</span>。即校正系数为<span class="math notranslate nohighlight">\(b \times ((p-1)/p)\)</span>。</p></li>
<li><p>AllGather：对于每个设备来说，可以把AllGather理解为只执行AllReduce中的广播部分，同理可得<span class="math notranslate nohighlight">\(B = (S/t) \times ((p-1)/p) = b \times ((p-1)/p)\)</span>。即校正系数为<span class="math notranslate nohighlight">\(b \times ((p-1)/p)\)</span>。</p></li>
<li><p>Broadcast：与AllReduce不同的是，Broadcast中所有数据需要从算子本身的发送者发出。即使在上面分治的情况下，也需要等待所有子问题运行结束才能确保Broadcast算子本身的正确性。因此，在计算带宽时，瓶颈仍为发送者对于外界所有信息处理的带宽，所以
<span class="math notranslate nohighlight">\(B = S/t\)</span>，即校正系数为<span class="math notranslate nohighlight">\(1\)</span>。</p></li>
<li><p>Reduce：Reduce需要将所有数据送往算子的接收者，因此校正系数为<span class="math notranslate nohighlight">\(1\)</span>。</p></li>
</ul>
<p>由于Gather和Scatter的带宽计算与实际聚合/分散时的数据结构相关性更高，故不给出特定的校正系数。</p>
</div>
<div class="section" id="id6">
<h2><span class="section-number">11.5.4. </span>利用集合通信优化模型训练的实践<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h2>
<p>针对不同的集群，机器学习系统往往会灵活组合不同集合通信算子来最大化通信效率。下面提供两个案例分析：ZeRO和DALL-E。</p>
<div class="section" id="zero">
<h3><span class="section-number">11.5.4.1. </span>ZeRO<a class="headerlink" href="#zero" title="Permalink to this heading">¶</a></h3>
<p>ZeRO是微软提出的神经网络优化器，在实践中成功训练了2020年世界上最大的语言模型（高达1700亿参数）。在训练这个级别的神经网络时优化器本身的参数，反向计算时的梯度，以及模型参数本身都会对加速器内存空间产生极大的压力。通过简易的计算不难得出，1700亿参数的模型在32位浮点表示情况下会占用至少680GB的内存，远超于现在内存最高的加速器A100
（最高内存80GB）。于是，需要考虑如何高效地把模型切成数份存储在不同的加速器上，以及如何高效地通过使用集合通信算子来进行模型训练和推理。这里，介绍三个主要的关于集合通信的优化技术：</p>
<ul class="simple">
<li><p><strong>单一节点上的参数存储：</strong>
现代集群中节点内部加速器的带宽远大于节点之间的带宽。为此，需要尽量减少节点间的通信，并且保证大部分通信仅存在于节点内部的加速器之间。在观察模型切片时，又可得模型本身前向和反向计算时需要在不同切片之间进行的通信远小于不同模型副本梯度平均的通信量。针对这一特性，ZeRO选择了将单一模型的全部切片存储到同一节点内部，从而大大提高了训练效率。</p></li>
<li><p><strong>基于AllGather算子的前向计算：</strong>
假设模型中的参数在层级上呈线性，便可按照参数在网络上的顺序从前到后将其分别存储到不同加速器中。在前向时，可以注意到某一层的计算仅依赖于其相邻层的参数。对此，可以对所有包含模型参数的加速器进行一次AllGather计算，用来提取每一层的后一层的参数，以及计算该层本身的激活值。为了节约内存，在AllGather操作结束后需要立即丢弃除了该层以外其他层的参数。</p></li>
<li><p><strong>基于ReduceScatter算子的梯度平均：</strong>
在反向计算时我们只需要前一层的参数来计算本层的激活值和梯度，因此只需要再次使用AllGather来完成每个加速器上的梯度计算。同时，在聚集梯度后，对于每个加速器仅需要和加速器的编号相同的层数对应的梯度。对此，可以使用ReduceScatter算子直接把相应的梯度存到编号为<span class="math notranslate nohighlight">\(i\)</span>的加速器上，而不是通常情况下使用AllReduce算子。</p></li>
</ul>
</div>
<div class="section" id="dall-e">
<h3><span class="section-number">11.5.4.2. </span>DALL-E<a class="headerlink" href="#dall-e" title="Permalink to this heading">¶</a></h3>
<p>DALL-E是OpenAI提出的一个基于文字的图片生成模型，模型同样拥有高达120亿的参数。在训练时，除了运用到ZeRO所使用的AllGather
+ ReduceScatter
技巧，OpenAI团队在其他细节上做了进一步的优化。这里，介绍两个主要的关于集合通信的优化技术：</p>
<ul class="simple">
<li><p><strong>矩阵分解：</strong>
集合通信算子的运行速度和信息本身的长度正相关。在模型训练中，这代表了模型参数本身的大小。对此，DALL-E
选择用矩阵分解（Matrix
Factorization）的方法先把高维张量调整为一个二维矩阵，通过分解后分开用集合通信算子进行传输，从而大大减少了通信量。</p></li>
<li><p><strong>自定义数据类型：</strong>
一种减少通信量的方法在于修改数据类型本身。显然地，可以使用16位的半精度浮点数，相比正常的32位参数表示可以节省近一倍的通信量。但是，在实践中发现低精度的数据类型会使得模型收敛不稳定，导致最终训练效果大打折扣。为此，OpenAI分析了DALL–E的模型结构，并把其中的参数根据对数据类型精度的敏感性分为了三类。其中对精度最敏感的一类照常使用32位浮点表示并只通过AllReduce算子来同步，而最不敏感的参数则照常通过矩阵分解进行压缩和传输。对于比较敏感的一类，例如Adam优化器其中的动能（Moments）和方差（Variance）参数，OpenAI
基于 IEEE 754
标准实现了两个全新的数据类型：1-6-9和0-6-10（其中第一表示正负所需的位数，第二表示指数所需的位数，第三表示有效数字所需的位数），在节省空间的同时保证了训练的收敛。</p></li>
</ul>
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">11.5.5. </span>集合通信在数据并行的实践<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h2>
<p>数据并行作为最广泛使用的分布式训练方法，是集合通信首先需要支持的范式。
对于数据并行的支持，机器学习系统通常提供了两个级别的抽象：在第一种级别的抽象里，机器学习系统更与硬件耦合，可以直接调用集合通信算子的库；在另一种级别的抽象里，机器学习系统更偏向神经网络实现，通过内部调用集合通信算子实现分布式训练和推理的机器学习框架。作为算法工程师，通常会接触到后者的抽象（包括Horovod、KungFu、TensorFlow
Distributed等），而作为集群的维护者，往往需要深入了解前者的运行原理和具体的调试方法。以
PyTorch 举例，在torch.distributed
命名空间（Namespace）下实现了一系列方便开发者使用的分布式模型训练和推理函数。在其内部，会根据实际运行的集群调用更底层的集合通信算子库，例如MPI，NCCL（前面已有介绍，适用于GPU分布式训练），Gloo（适用于CPU分布式训练）等。下面具体对比PyTorch
Distributed和NCCL在AllReduce应用方面的差异：
以下代码通过PyTorch自带的分布式数据并行（Distributed Data
Parallel，DDP）方法完成了一次简易的机器学习模型计算。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 基于PyTorch DDP高层次封装实现AllReduce算法</span>

<span class="k">def</span> <span class="nf">ddp_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">ToyModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="c1"># 通过调用DDP（分布式数据并行）方法将模型在每个处理器上完成初始化</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>

    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">ddp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

    <span class="c1"># 在反向计算时，框架内部会执行AllReduce算法</span>
    <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>下面代码通过Gloo的Python
接口pygloo和Ray完成了一个二维张量的AllReduce计算。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 基于pygloo底层接口实现AllReduce算法</span>

<span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">gloo_allreduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">pygloo</span><span class="o">.</span><span class="n">rendezvous</span><span class="o">.</span><span class="n">Context</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="o">...</span>

    <span class="n">Sendbuf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">recvbuf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Sendbuf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">Sendptr</span> <span class="o">=</span> <span class="n">Sendbuf</span><span class="o">.</span><span class="n">ctypes</span><span class="o">.</span><span class="n">data</span>
    <span class="n">recvptr</span> <span class="o">=</span> <span class="n">recvbuf</span><span class="o">.</span><span class="n">ctypes</span><span class="o">.</span><span class="n">data</span>

    <span class="c1"># 标明发送者和接收者并直接调用AllReduce算法</span>
    <span class="n">pygloo</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">Sendptr</span><span class="p">,</span> <span class="n">recvptr</span><span class="p">,</span>
                    <span class="n">Sendbuf</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">pygloo</span><span class="o">.</span><span class="n">glooDataType_t</span><span class="o">.</span><span class="n">glooFloat32</span><span class="p">,</span>
                    <span class="n">pygloo</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">pygloo</span><span class="o">.</span><span class="n">allreduceAlgorithm</span><span class="o">.</span><span class="n">RING</span><span class="p">)</span>
</pre></div>
</div>
<p>可以注意到，PyTorch
Distributed并没有显式地调用集合通信算子，而是通过DistributedDataParallel方法将分布式训练和非分布式训练之间的不同隐藏了起来。如果需要在不同集群上运行这段代码，只需要在setup
函数内对应地更改PyTorch使用的底层集合通信库即可。在backward函数被调用时，才会真正地使用AllReduce算法。相比，如果想要直接使用Gloo，不仅需要一步一步地创建通信所需要的数据结构，同时也很难和现有的模型训练框架无缝连接。</p>
</div>
<div class="section" id="id8">
<h2><span class="section-number">11.5.6. </span>集合通信在混合并行的实践<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h2>
<p>随着深度学习的发展，模型和训练数据集的规模呈爆发式增长，单机的算力和存储能力已无法满足需求，因此，分布式训练技术成为行业发展趋势。</p>
<p>本章前几节已总结当前常用的分布式并行训练技术方案，如数据并行、模型并行和流水线并行，在复杂场景下，往往需要不同技术点组合使用，才能达到训练大模型的高性能。华为MindSpore开源框架提供混合并行的能力，来支撑大模型分布式训练，用户可以根据自己的需要进行灵活组合。以下通过简单代码示例来说明如何在MindSpore中组合使用数据并行、模型并行和流水线并行训练技术，其他大模型训练技术的使用方法请参照官网教程。</p>
<p>以下代码利用set_auto_parallel_context接口设置并行模式和可用于训练的卡数，同时利用该接口设置流水线并行中的stage数量。通过扩展nn.Cell,
定义了简单的神经网络模型，其中self.matmul1和self.matmul2的两个矩阵乘操作，调用shard接口来配置切分策略，如matmul1将第一个输入按照行切成4份，实则是在数据维度上切分，是数据并行的样例，而matmul2对第二个输入进行列切，采用了模型并行的方式。为了实现流水线并行，以下代码调用nn.PipelineCell接口来包装net_with_loss，并指定流水线并行所需的微批次大小。最后，通过model.train接口来对神经网络进行混合并行训练。</p>
<p>MindSpore提供了shard接口来允许用户配置切分策略。在这种切分的场景下，需要在必要的时候插入集合通信算子来保证计算逻辑的正确性：第一种是切分了单一算子的情况，将算子切分到多卡进行计算，为了保证计算结果和单卡计算结果一致，需要集合通信算子来将多卡计算的部分结果同步聚合到每张卡上；第二种是多算子情况下，相邻算子的切分方式不同，前继算子的计算结果排布在不同的卡上，后续算子的计算需要用到非当前卡上的数据才能进行，此时需要一个集合通信算子来重新排布前继算子的计算结果。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 基于MindSpore对模型进行混合并行分布式训练</span>

<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="c1"># 设置并行模式为半自动并行，同时设置训练的卡数</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;semi_auto_parallel&quot;</span><span class="p">,</span> <span class="n">device_num</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># 设置流水线并行的stage数量</span>
<span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="o">=</span><span class="n">stages</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DenseMatMulNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DenseMutMulNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 通过shard定义算子切分的方式：matmul1是数据并行的样例，matmul2是模型并行的样例</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>

<span class="c1"># 定义训练数据集</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DATA_PATH&#39;</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DenseMatMulNet</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="c1"># 用PipelineCell接口包装神经网络，第二个参数指定MicroBatch Size</span>
<span class="n">net_pipeline</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PipelineCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">micro_size</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net_pipeline</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">)</span>
<span class="c1"># 对模型进行迭代训练</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#ch10-redistribution"><span class="std std-numref">图11.5.3</span></a>
展示了上述代码中matmul1和matmul2在调用shard函数后的数据排布情况。matmul1算子将输入<span class="math notranslate nohighlight">\(X\)</span>按照行切成4份后，分别放置在4个计算设备上（D1-D4），<span class="math notranslate nohighlight">\(W\)</span>不切分，则以复制的形式放置在4个计算设备上，此时matmul1算子计算的结果<span class="math notranslate nohighlight">\(Y\)</span>，以行切的形式被放置在不同设备上，而matmul2算子在做计算时，需要<span class="math notranslate nohighlight">\(Y\)</span>的全量数据，因此两个计算算子之间需要插入AllGather集合通信算子，来从4个不同的设备上收集到<span class="math notranslate nohighlight">\(Y\)</span>的全量数据。MindSpore能够自动识别不同切分方式的算子之间应该插入哪种集合通信算子，并且将该逻辑对用户隐藏，只暴露出shard接口供用户配置，开发者可以通过合理的策略配置，来减少算子间重排布通信算子在神经网络计算图中的占比，以提升混合并行分布式训练的端到端速率。</p>
<div class="figure align-default" id="id11">
<span id="ch10-redistribution"></span><a class="reference internal image-reference" href="../_images/ch10-redistribution.png"><img alt="../_images/ch10-redistribution.png" src="../_images/ch10-redistribution.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">图11.5.3 </span><span class="caption-text">相邻算子之间插入集合通信算子举例</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">11.5. 集合通信</a><ul>
<li><a class="reference internal" href="#id2">11.5.1. 常见集合通信算子</a><ul>
<li><a class="reference internal" href="#id3">11.5.1.1. 通信模型</a></li>
<li><a class="reference internal" href="#broadcast">11.5.1.2. Broadcast</a></li>
<li><a class="reference internal" href="#reduce">11.5.1.3. Reduce</a></li>
<li><a class="reference internal" href="#allreduce">11.5.1.4. AllReduce</a></li>
<li><a class="reference internal" href="#gather">11.5.1.5. Gather</a></li>
<li><a class="reference internal" href="#allgather">11.5.1.6. AllGather</a></li>
<li><a class="reference internal" href="#scatter">11.5.1.7. Scatter</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id4">11.5.2. 基于AllReduce的梯度平均算法</a></li>
<li><a class="reference internal" href="#id5">11.5.3. 集合通信算法性能分析</a></li>
<li><a class="reference internal" href="#id6">11.5.4. 利用集合通信优化模型训练的实践</a><ul>
<li><a class="reference internal" href="#zero">11.5.4.1. ZeRO</a></li>
<li><a class="reference internal" href="#dall-e">11.5.4.2. DALL-E</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id7">11.5.5. 集合通信在数据并行的实践</a></li>
<li><a class="reference internal" href="#id8">11.5.6. 集合通信在混合并行的实践</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="cluster.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>11.4. 机器学习集群架构</div>
         </div>
     </a>
     <a id="button-next" href="parameter_servers.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>11.6. 参数服务器</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>