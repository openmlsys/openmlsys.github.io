<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <title>
   11.2. 实现方法 — 机器学习系统：设计和实现 1.0.0 documentation
  </title>
  <link href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/sphinx_materialdesign_theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fontawesome/all.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fonts.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/basic.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/d2l.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/sphinx_highlight.js">
  </script>
  <script src="../_static/d2l.js">
  </script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <link href="../_static/favicon.png" rel="shortcut icon"/>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="cluster.html" rel="next" title="11.4. 机器学习集群架构"/>
  <link href="overview.html" rel="prev" title="11.1. 系统概述"/>
 </head>
 <body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer">
   <header class="mdl-layout__header mdl-layout__header--waterfall">
    <div class="mdl-layout__header-row">
     <nav class="mdl-navigation breadcrumb">
      <a class="mdl-navigation__link" href="index.html">
       <span class="section-number">
        11.
       </span>
       分布式训练
      </a>
      <i class="material-icons">
       navigate_next
      </i>
      <a class="mdl-navigation__link is-active">
       <span class="section-number">
        11.2.
       </span>
       实现方法
      </a>
     </nav>
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <form action="../search.html" class="form-inline pull-sm-right" method="get">
       <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label class="mdl-button mdl-js-button mdl-button--icon" for="waterfall-exp" id="quick-search-icon">
         <i class="material-icons">
          search
         </i>
        </label>
        <div class="mdl-textfield__expandable-holder">
         <input class="mdl-textfield__input" id="waterfall-exp" name="q" placeholder="Search" type="text"/>
         <input name="check_keywords" type="hidden" value="yes"/>
         <input name="area" type="hidden" value="default"/>
        </div>
       </div>
       <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
        Quick search
       </div>
      </form>
      <a class="mdl-button mdl-js-button mdl-button--icon" href="../_sources/chapter_distributed_training/methods.rst.txt" id="button-show-source" rel="nofollow">
       <i class="material-icons">
        code
       </i>
      </a>
      <div class="mdl-tooltip" data-mdl-for="button-show-source">
       Show Source
      </div>
     </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <a class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
       <i class="fab fa-github">
       </i>
       GitHub
      </a>
     </nav>
    </div>
   </header>
   <header class="mdl-layout__drawer">
    <!-- Title -->
    <span class="mdl-layout-title">
     <a class="title" href="../index.html">
      <img alt="机器学习系统：设计和实现" class="logo" src="../_static/logo-with-text.png"/>
     </a>
    </span>
    <div class="globaltoc">
     <span class="mdl-layout-title toc">
      Table Of Contents
     </span>
     <nav class="mdl-navigation">
      <ul class="current">
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface/index.html">
         1. 前言
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_introduction/index.html">
         2. 导论
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/applications.html">
           2.1. 机器学习应用
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/design.html">
           2.2. 机器学习框架的设计目标
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/architecture.html">
           2.3. 机器学习框架的基本组成原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/ecosystem.html">
           2.4. 机器学习系统生态
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/readers.html">
           2.5. 图书结构和读者
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_programming_interface/index.html">
         3. 编程接口
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/development_history.html">
           3.1. 机器学习系统编程模型的演进
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">
           3.2. 机器学习工作流
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">
           3.3. 定义深度神经网络
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">
           3.4. C/C++编程接口
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/ml_programming_paradigm.html">
           3.5. 机器学习框架的编程范式
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/summary.html">
           3.6. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/summary.html#id2">
           3.7. 扩展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_computational_graph/index.html">
         4. 计算图
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">
           4.1. 计算图的设计背景和作用
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">
           4.2. 计算图的基本构成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">
           4.3. 计算图的生成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">
           4.4. 计算图的调度
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/summary.html">
           4.5. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/summary.html#id2">
           4.6. 扩展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_advanced/index.html">
         5. 第二部分：进阶篇
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_frontend_and_ir/index.html">
         6. AI编译器和前端技术
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/ai_compiler_design_principle.html">
           6.1. AI编译器设计原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">
           6.2. AI编译器前端技术概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">
           6.3. 中间表示
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/ad.html">
           6.4. 自动微分
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">
           6.5. 类型系统和静态分析
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">
           6.6. 常见前端编译优化方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/summary.html">
           6.7. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">
           6.8. 扩展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_backend_and_runtime/index.html">
         7. 编译器后端和运行时
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/overview.html">
           7.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">
           7.2. 计算图优化
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">
           7.3. 算子选择
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">
           7.4. 内存分配
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">
           7.5. 计算调度与执行
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/op_compiler.html">
           7.6. 算子编译器
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/summary.html">
           7.7. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">
           7.8. 扩展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_accelerator/index.html">
         8. 硬件加速器
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">
           8.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">
           8.2. 加速器基本组成原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">
           8.3. 加速器基本编程原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_practise.html">
           8.4. 加速器实践
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/summary.html">
           8.5. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/summary.html#id2">
           8.6. 扩展阅读
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/summary.html#id3">
           8.7. 参考文献
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_data_processing/index.html">
         9. 数据处理框架
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/requirements.html">
           9.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/program_model.html">
           9.2. 易用性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/performance.html">
           9.3. 高效性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/data_order.html">
           9.4. 保序性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/extension.html">
           9.5. 单机数据处理性能的扩展
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/summary.html">
           9.6. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/summary.html#id2">
           9.7. 扩展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_model_deployment/index.html">
         10. 模型部署
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">
           10.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">
           10.2. 训练模型到推理模型的转换及优化
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_compression.html">
           10.3. 模型压缩
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_inference.html">
           10.4. 模型推理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_security.html">
           10.5. 模型的安全保护
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/summary.html">
           10.6. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/summary.html#id2">
           10.7. 扩展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1 current">
        <a class="reference internal" href="index.html">
         11. 分布式训练
        </a>
        <ul class="current">
         <li class="toctree-l2">
          <a class="reference internal" href="overview.html">
           11.1. 系统概述
          </a>
         </li>
         <li class="toctree-l2 current">
          <a class="current reference internal" href="#">
           11.2. 实现方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="#id6">
           11.3. 流水线并行
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="cluster.html">
           11.4. 机器学习集群架构
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="collective.html">
           11.5. 集合通信
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="parameter_servers.html">
           11.6. 参数服务器
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="summary.html">
           11.7. 总结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="summary.html#id2">
           11.8. 拓展阅读
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_extension/index.html">
         12. 第三部分：拓展篇
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_recommender_system/index.html">
         13. 深度学习推荐系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/system_architecture.html">
           13.1. 系统基本组成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/multi_stage_recommender_system.html">
           13.2. 多阶段推荐系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/model_update.html">
           13.3. 模型更新
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/case_study.html">
           13.4. 案例分析：支持在线模型更新的大型推荐系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/summary.html">
           13.5. 小结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/summary.html#id2">
           13.6. 扩展阅读
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_recommender_system/summary.html#id3">
           13.7. 参考文献
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_federated_learning/index.html">
         14. 联邦学习系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/overview.html">
           14.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">
           14.2. 横向联邦学习
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">
           14.3. 纵向联邦学习
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">
           14.4. 隐私加密算法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/outlook.html">
           14.5. 展望
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_federated_learning/summary.html">
           14.6. 小结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
         15. 强化学习系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">
           15.1. 强化学习介绍
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">
           15.2. 单节点强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">
           15.3. 分布式强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/marl.html">
           15.4. 多智能体强化学习
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">
           15.5. 多智能体强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/summary.html">
           15.6. 小结
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">
           15.7. 参考文献
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_explainable_AI/index.html">
         16. 可解释性AI系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">
           16.1. 背景
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">
           16.2. 可解释AI定义
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">
           16.3. 可解释AI算法现状介绍
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id17">
           16.4. 可解释AI系统及实践
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id21">
           16.5. 未来可解释AI
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id22">
           16.6. 参考文献
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_rl_sys/index.html">
         17. 机器人系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">
           17.1. 机器人系统概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_rl_sys/ros.html">
           17.2. 通用机器人操作系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_rl_sys/ros_code_ex.html">
           17.3. 案例分析：使用机器人操作系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_rl_sys/summary.html">
           17.4. 总结
          </a>
         </li>
        </ul>
       </li>
      </ul>
      <ul>
       <li class="toctree-l1">
        <a class="reference internal" href="../appendix_machine_learning_introduction/index.html">
         附录：机器学习介绍
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">
           1. 神经网络
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">
           2. 梯度下降与反向传播
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">
           3. 经典机器学习方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">
           4. 参考文献
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </nav>
    </div>
   </header>
   <main class="mdl-layout__content" tabindex="0">
    <script src="../_static/sphinx_materialdesign_theme.js " type="text/javascript">
    </script>
    <header class="mdl-layout__drawer">
     <!-- Title -->
     <span class="mdl-layout-title">
      <a class="title" href="../index.html">
       <img alt="机器学习系统：设计和实现" class="logo" src="../_static/logo-with-text.png"/>
      </a>
     </span>
     <div class="globaltoc">
      <span class="mdl-layout-title toc">
       Table Of Contents
      </span>
      <nav class="mdl-navigation">
       <ul class="current">
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface/index.html">
          1. 前言
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_introduction/index.html">
          2. 导论
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/applications.html">
            2.1. 机器学习应用
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/design.html">
            2.2. 机器学习框架的设计目标
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/architecture.html">
            2.3. 机器学习框架的基本组成原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/ecosystem.html">
            2.4. 机器学习系统生态
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/readers.html">
            2.5. 图书结构和读者
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_programming_interface/index.html">
          3. 编程接口
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/development_history.html">
            3.1. 机器学习系统编程模型的演进
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">
            3.2. 机器学习工作流
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">
            3.3. 定义深度神经网络
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">
            3.4. C/C++编程接口
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/ml_programming_paradigm.html">
            3.5. 机器学习框架的编程范式
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/summary.html">
            3.6. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/summary.html#id2">
            3.7. 扩展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_computational_graph/index.html">
          4. 计算图
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">
            4.1. 计算图的设计背景和作用
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">
            4.2. 计算图的基本构成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">
            4.3. 计算图的生成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">
            4.4. 计算图的调度
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/summary.html">
            4.5. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/summary.html#id2">
            4.6. 扩展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_advanced/index.html">
          5. 第二部分：进阶篇
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_frontend_and_ir/index.html">
          6. AI编译器和前端技术
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/ai_compiler_design_principle.html">
            6.1. AI编译器设计原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">
            6.2. AI编译器前端技术概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">
            6.3. 中间表示
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/ad.html">
            6.4. 自动微分
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">
            6.5. 类型系统和静态分析
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">
            6.6. 常见前端编译优化方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/summary.html">
            6.7. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/summary.html#id2">
            6.8. 扩展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_backend_and_runtime/index.html">
          7. 编译器后端和运行时
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/overview.html">
            7.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">
            7.2. 计算图优化
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">
            7.3. 算子选择
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">
            7.4. 内存分配
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">
            7.5. 计算调度与执行
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/op_compiler.html">
            7.6. 算子编译器
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/summary.html">
            7.7. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/summary.html#id2">
            7.8. 扩展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_accelerator/index.html">
          8. 硬件加速器
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">
            8.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">
            8.2. 加速器基本组成原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">
            8.3. 加速器基本编程原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_practise.html">
            8.4. 加速器实践
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/summary.html">
            8.5. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/summary.html#id2">
            8.6. 扩展阅读
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/summary.html#id3">
            8.7. 参考文献
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_data_processing/index.html">
          9. 数据处理框架
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/requirements.html">
            9.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/program_model.html">
            9.2. 易用性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/performance.html">
            9.3. 高效性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/data_order.html">
            9.4. 保序性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/extension.html">
            9.5. 单机数据处理性能的扩展
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/summary.html">
            9.6. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/summary.html#id2">
            9.7. 扩展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_model_deployment/index.html">
          10. 模型部署
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">
            10.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">
            10.2. 训练模型到推理模型的转换及优化
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_compression.html">
            10.3. 模型压缩
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_inference.html">
            10.4. 模型推理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_security.html">
            10.5. 模型的安全保护
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/summary.html">
            10.6. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/summary.html#id2">
            10.7. 扩展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1 current">
         <a class="reference internal" href="index.html">
          11. 分布式训练
         </a>
         <ul class="current">
          <li class="toctree-l2">
           <a class="reference internal" href="overview.html">
            11.1. 系统概述
           </a>
          </li>
          <li class="toctree-l2 current">
           <a class="current reference internal" href="#">
            11.2. 实现方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="#id6">
            11.3. 流水线并行
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="cluster.html">
            11.4. 机器学习集群架构
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="collective.html">
            11.5. 集合通信
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="parameter_servers.html">
            11.6. 参数服务器
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="summary.html">
            11.7. 总结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="summary.html#id2">
            11.8. 拓展阅读
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_extension/index.html">
          12. 第三部分：拓展篇
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_recommender_system/index.html">
          13. 深度学习推荐系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/system_architecture.html">
            13.1. 系统基本组成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/multi_stage_recommender_system.html">
            13.2. 多阶段推荐系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/model_update.html">
            13.3. 模型更新
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/case_study.html">
            13.4. 案例分析：支持在线模型更新的大型推荐系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/summary.html">
            13.5. 小结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/summary.html#id2">
            13.6. 扩展阅读
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_recommender_system/summary.html#id3">
            13.7. 参考文献
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_federated_learning/index.html">
          14. 联邦学习系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/overview.html">
            14.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/horizontal_fl.html">
            14.2. 横向联邦学习
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/vertical_fl.html">
            14.3. 纵向联邦学习
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/privacy_encryption_algorithm.html">
            14.4. 隐私加密算法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/outlook.html">
            14.5. 展望
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_federated_learning/summary.html">
            14.6. 小结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
          15. 强化学习系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">
            15.1. 强化学习介绍
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">
            15.2. 单节点强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/distributed_node_rl.html">
            15.3. 分布式强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/marl.html">
            15.4. 多智能体强化学习
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">
            15.5. 多智能体强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/summary.html">
            15.6. 小结
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/summary.html#id2">
            15.7. 参考文献
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_explainable_AI/index.html">
          16. 可解释性AI系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">
            16.1. 背景
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#ai">
            16.2. 可解释AI定义
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id2">
            16.3. 可解释AI算法现状介绍
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id17">
            16.4. 可解释AI系统及实践
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id21">
            16.5. 未来可解释AI
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html#id22">
            16.6. 参考文献
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_rl_sys/index.html">
          17. 机器人系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_rl_sys/rl_sys_intro.html">
            17.1. 机器人系统概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_rl_sys/ros.html">
            17.2. 通用机器人操作系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_rl_sys/ros_code_ex.html">
            17.3. 案例分析：使用机器人操作系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_rl_sys/summary.html">
            17.4. 总结
           </a>
          </li>
         </ul>
        </li>
       </ul>
       <ul>
        <li class="toctree-l1">
         <a class="reference internal" href="../appendix_machine_learning_introduction/index.html">
          附录：机器学习介绍
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">
            1. 神经网络
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">
            2. 梯度下降与反向传播
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">
            3. 经典机器学习方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html#id4">
            4. 参考文献
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </nav>
     </div>
    </header>
    <div class="document">
     <div class="page-content" role="main">
      <div class="section" id="id1">
       <h1>
        <span class="section-number">
         11.2.
        </span>
        实现方法
        <a class="headerlink" href="#id1" title="Permalink to this heading">
         ¶
        </a>
       </h1>
       <p>
        下面讨论分布式训练系统实现的常用并行方法。首先给出并行方法的设计目标以及分类。然后详细描述各个并行方法。
       </p>
       <div class="section" id="id2">
        <h2>
         <span class="section-number">
          11.2.1.
         </span>
         概述
         <a class="headerlink" href="#id2" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         分布式训练系统的设计目标是：将单节点训练系统转换成
         <strong>
          等价的
         </strong>
         并行训练系统，从而在不影响模型精度的条件下完成训练过程的加速。一个单节点训练系统往往如
         <a class="reference internal" href="#ch10-single-node">
          <span class="std std-numref">
           图11.2.1
          </span>
         </a>
         所示。一个训练过程会由多个数据小批次（mini-batch）完成。在图中，一个数据小批次被标示为
         <strong>
          数据
         </strong>
         。训练系统会利用数据小批次生成梯度，提升模型精度。这个过程由一个训练
         <strong>
          程序
         </strong>
         实现。在实际中，这个程序往往实现了一个多层神经网络的执行过程。该神经网络的执行由一个计算图（Computational
Graph）表示。这个图有多个相互连接的算子（Operator），每个算子会拥有计算参数。每个算子往往会实现一个神经网络层（Neural
Network Layer），而参数则代表了这个层在训练中所更新的的权重（Weights）。
        </p>
        <div class="figure align-default" id="id7">
         <span id="ch10-single-node">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-single-node.png">
          <img alt="../_images/ch10-single-node.png" src="../_images/ch10-single-node.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           图11.2.1
          </span>
          <span class="caption-text">
           单节点训练系统
          </span>
          <a class="headerlink" href="#id7" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         为了更新参数，计算图的执行分为前向计算和反向计算两个阶段。前向计算的第一步会将数据读入第一个算子，该算子会根据当前的参数，计算出计算给下一个算子的数据。算子依次重复这个前向计算的过程（执行顺序：算子1，算子2，算子3），直到最后一个算子结束。最后的算子随之马上开始反向计算。反向计算中，每个算子依次计算出梯度（执行顺序：梯度3，梯度2，梯度1），并利用梯度更新本地的参数。反向计算最终在第一个算子结束。反向计算的结束也标志本次数据小批次的结束，系统随之读取下一个数据小批次，继续更新模型。
        </p>
        <p>
         给定一个模型训练任务，人们会对
         <strong>
          数据
         </strong>
         和
         <strong>
          程序
         </strong>
         切分（Partition），从而完成并行加速。
         <a class="reference internal" href="#ch10-parallel-methods">
          <span class="std std-numref">
           表11.2.1
          </span>
         </a>
         总结了不同的切分方法。单节点训练系统可以被归类于单程序单数据模式。而假如用户希望使用更多的设备实现并行计算，首先可以选择对数据进行分区，并将同一个程序复制到多个设备上并行执行。这种方式是单程序多数据模式，常被称为数据并行（Data
Parallelism）。另一种并行方式是对程序进行分区（模型中的算子会被分发给多个设备分别完成）。这种模式是多程序单数据模式，常被称为模型并行（Model
Parallelism）。当训练超大型智能模型时，开发人员往往要同时对数据和程序进行切分，从而实现最高程度的并行。这种模式是多程序多数据模式，常被称为混合并行（Hybrid
Parallelism）。
        </p>
        <span id="ch10-parallel-methods">
        </span>
        <table class="docutils align-default" id="id8" style="margin-left:auto;margin-right:auto;margin-top:10px;margin-bottom:20px;">
         <caption>
          <span class="caption-number">
           表11.2.1
          </span>
          <span class="caption-text">
           分布式训练方法分类
          </span>
          <a class="headerlink" href="#id8" title="Permalink to this table">
           ¶
          </a>
         </caption>
         <colgroup>
          <col style="width: 12%"/>
          <col style="width: 44%"/>
          <col style="width: 44%"/>
         </colgroup>
         <thead>
          <tr class="row-odd">
           <th class="head">
            <p>
             分类
            </p>
           </th>
           <th class="head">
            <p>
             单数据
            </p>
           </th>
           <th class="head">
            <p>
             多数据
            </p>
           </th>
          </tr>
         </thead>
         <tbody>
          <tr class="row-even">
           <td>
            <p>
             单程序
            </p>
           </td>
           <td>
            <p>
             单程序单数据：单点执行
            </p>
           </td>
           <td>
            <p>
             单程序多数据：数据并行
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             多程序
            </p>
           </td>
           <td>
            <p>
             多程序单数据：模型并行
            </p>
           </td>
           <td>
            <p>
             多程序多数据：混合并行
            </p>
           </td>
          </tr>
         </tbody>
        </table>
        <p>
         接下来详细讲解各种并行方法的执行过程。
        </p>
       </div>
       <div class="section" id="id3">
        <h2>
         <span class="section-number">
          11.2.2.
         </span>
         数据并行
         <a class="headerlink" href="#id3" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         数据并行往往可以解决单节点算力不足的问题。这种并行方式在人工智能框架中最为常见，具体实现包括：TensorFlow
DistributedStrategy、PyTorch Distributed、Horovod
DistributedOptimizer等。在一个数据并行系统中，假设用户给定一个训练批大小为
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         ，并且希望使用
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个并行设备来加速训练。那么，该训练批大小会被分为
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个分区，每个设备会分配到
         <span class="math notranslate nohighlight">
          \(N/M\)
         </span>
         个训练样本。这些设备共享一个训练程序的副本，在不同数据分区上独立执行、计算梯度。不同的设备（假设设备编号为
         <span class="math notranslate nohighlight">
          \(i\)
         </span>
         ）会根据本地的训练样本计算出梯度
         <span class="math notranslate nohighlight">
          \(G_i\)
         </span>
         。为了确保训练程序参数的一致性，本地梯度
         <span class="math notranslate nohighlight">
          \(G_i\)
         </span>
         需要聚合，计算出平均梯度
         <span class="math notranslate nohighlight">
          \((\sum_{i=1}^{N} G_i) / N\)
         </span>
         。最终，训练程序利用平均梯度修正模型参数，完成小批次的训练。
        </p>
        <p>
         <a class="reference internal" href="#ch10-data-parallel">
          <span class="std std-numref">
           图11.2.2
          </span>
         </a>
         展示了两个设备构成的数据并行训练系统（Data
Parallel Training
System）的例子。假设用户给定的数据批大小是64，那么每个设备会分配到32个训练样本，并且具有相同的神经网络参数（程序副本）。本地的训练样本会依次通过这个程序副本中的算子，完成前向计算和反向计算。在反向计算的过程中，程序副本会生成局部梯度。不同设备上对应的局部梯度（如设备1和设备2上各自的梯度1）会进行聚合，从而计算平均梯度。这个聚合的过程往往由集合通信的AllReduce操作完成。
        </p>
        <div class="figure align-default" id="id9">
         <span id="ch10-data-parallel">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-data-parallel.png">
          <img alt="../_images/ch10-data-parallel.png" src="../_images/ch10-data-parallel.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           图11.2.2
          </span>
          <span class="caption-text">
           数据并行训练系统
          </span>
          <a class="headerlink" href="#id9" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
       </div>
       <div class="section" id="id4">
        <h2>
         <span class="section-number">
          11.2.3.
         </span>
         模型并行
         <a class="headerlink" href="#id4" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <p>
         模型并行往往用于解决单节点内存不足的问题。一个常见的内存不足场景是模型中含有大型算子，例如深度神经网络中需要计算大量分类的全连接层。完成这种大型算子计算所需的内存可能超过单设备的内存容量。那么需要对这个大型算子进行切分。假设这个算子具有
         <span class="math notranslate nohighlight">
          \(P\)
         </span>
         个参数，而系统拥有
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         个设备，那么可以将
         <span class="math notranslate nohighlight">
          \(P\)
         </span>
         个参数平均分配给
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         个设备（每个设备分配
         <span class="math notranslate nohighlight">
          \(P/N\)
         </span>
         个参数），从而让每个设备负责更少的计算量，能够在内存容量的限制下完成前向计算和反向计算。这种切分方式是模型并行训练系统（Model
Parallelism Training
System）的一种应用，也被称为
         <strong>
          算子内并行
         </strong>
         （Intra-operator
Parallelism）。
        </p>
        <div class="figure align-default" id="id10">
         <span id="ch10-model-parallel-intra-op">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-model-parallel-intra-op.png">
          <img alt="../_images/ch10-model-parallel-intra-op.png" src="../_images/ch10-model-parallel-intra-op.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           图11.2.3
          </span>
          <span class="caption-text">
           模型并行训练系统：算子内并行
          </span>
          <a class="headerlink" href="#id10" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         <a class="reference internal" href="#ch10-model-parallel-intra-op">
          <span class="std std-numref">
           图11.2.3
          </span>
         </a>
         给出了一个由两个设备实现的算子内并行的例子。在这个例子中，假设一个神经网络具有两个算子，算子1的计算（包含正向和反向计算）需要预留16GB的内存，算子2的计算需要预留1GB的内存。而本例中的设备最多可以提供10GB的内存。为了完成这个神经网络的训练，需要对算子1实现并行。具体做法是，将算子1的参数平均分区，设备1和设备2各负责其中部分算子1的参数。由于设备1和设备2的参数不同，因此它们各自负责程序分区1和程序分区2。在训练这个神经网络的过程中，训练数据（按照一个小批次的数量）会首先传给算子1。由于算子1的参数分别由两个设备负责，因此数据会被广播（Broadcast）给这两个设备。不同设备根据本地的参数分区完成前向计算，生成的本地计算结果需要进一步合并，发送给下游的算子2。在反向计算中，算子2的数据会被广播给设备1和设备2，这些设备根据本地的算子1分区各自完成局部的反向计算。计算结果进一步合并计算回数据，最终完成反向计算。
        </p>
        <p>
         另一种内存不足的场景是：模型的总内存需求超过了单设备的内存容量。在这种场景下，假设总共有
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         个算子和
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个设备，可以将算子平摊给这
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个设备，让每个设备仅需负责
         <span class="math notranslate nohighlight">
          \(N/M\)
         </span>
         个算子的前向和反向计算，降低设备的内存开销。这种并行方式是模型并行的另一种应用，被称为
         <strong>
          算子间并行
         </strong>
         （Inter-operator
Parallelism）。
        </p>
        <div class="figure align-default" id="id11">
         <span id="ch10-model-parallel-inter-op">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-model-parallel-inter-op.png">
          <img alt="../_images/ch10-model-parallel-inter-op.png" src="../_images/ch10-model-parallel-inter-op.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           图11.2.4
          </span>
          <span class="caption-text">
           模型并行训练系统：算子间并行
          </span>
          <a class="headerlink" href="#id11" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         <a class="reference internal" href="#ch10-model-parallel-inter-op">
          <span class="std std-numref">
           图11.2.4
          </span>
         </a>
         给出了一个由两个设备实现的算子间并行的例子。在这个例子中，假设一个神经网络具有两个算子，算子1和算子2各自需要10GB的内存完成计算，则模型总共需要20GB的内存。而每个设备仅能提供10GB内存。在这个例子中，用户可以把算子1放置在设备1上，算子2放置在设备2上。在前向计算中，算子1的输出会被发送（Send）给下游的设备2。设备2接收（Receive）来自上游的数据，完成算子2的前向计算。在反向计算中，设备2将算子2的反向计算结果发送给设备1。设备1完成算子1的反向计算，完成本次小批次（Mini-Batch）的训练。
        </p>
       </div>
       <div class="section" id="id5">
        <h2>
         <span class="section-number">
          11.2.4.
         </span>
         混合并行
         <a class="headerlink" href="#id5" title="Permalink to this heading">
          ¶
         </a>
        </h2>
        <div class="figure align-default" id="id12">
         <span id="ch10-hybrid-parallel">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-hybrid-parallel.png">
          <img alt="../_images/ch10-hybrid-parallel.png" src="../_images/ch10-hybrid-parallel.png" style="width: 800px;"/>
         </a>
         <p class="caption">
          <span class="caption-number">
           图11.2.5
          </span>
          <span class="caption-text">
           混合并行系统
          </span>
          <a class="headerlink" href="#id12" title="Permalink to this image">
           ¶
          </a>
         </p>
        </div>
        <p>
         在训练大型人工智能模型中，往往会同时面对算力不足和内存不足的问题。因此，需要混合使用数据并行和模型并行，这种方法被称为混合并行。
         <a class="reference internal" href="#ch10-hybrid-parallel">
          <span class="std std-numref">
           图11.2.5
          </span>
         </a>
         提供了一个由4个设备实现的混合并行的例子。在这个例子中，首先实现算子间并行解决训练程序内存开销过大的问题：该训练程序的算子1和算子2被分摊到了设备1和设备2上。进一步，通过数据并行添加设备3和设备4，提升系统算力。为了达到这一点，对训练数据进行分区（数据分区1和数据分区2），并将模型（算子1和算子2）分别复制到设备3和设备4。在前向计算的过程中，设备1和设备3上的算子1副本同时开始，计算结果分别发送（Send）给设备2和设备4完成算子2副本的计算。在反向计算中，设备2和设备4同时开始计算梯度，本地梯度通过AllReduce操作进行平均。反向计算传递到设备1和设备3上的算子1副本结束。
        </p>
       </div>
      </div>
      <div class="section" id="id6">
       <h1>
        <span class="section-number">
         11.3.
        </span>
        流水线并行
        <a class="headerlink" href="#id6" title="Permalink to this heading">
         ¶
        </a>
       </h1>
       <p>
        除了数据并行和模型并行以外，流水线并行是另一种常用的实现分布式训练的方法。流水线并行往往被应用在大型模型并行系统中。这种系统通过算子内并行和算子间并行解决单设备内存不足的问题。然而，这类系统的运行中，计算图中的下游设备（Downstream
Device）需要长期持续处于空闲状态，等待上游设备（Upstream
Device）的计算完成，才可以开始计算，这极大降低了设备的平均使用率。这种现象称为模型并行气泡（Model
Parallelism Bubble）。
       </p>
       <p>
        为了减少气泡，通常可以在训练系统中构建流水线。这种做法是将训练数据中的每一个小批次划分为多个微批次（Micro-Batch）。假设一个小批次有
        <span class="math notranslate nohighlight">
         \(D\)
        </span>
        个训练样本，将其划分为
        <span class="math notranslate nohighlight">
         \(M\)
        </span>
        个微批次，那么一个微批次就有
        <span class="math notranslate nohighlight">
         \(D/M\)
        </span>
        个数据样本。每个微批次依次进入训练系统，完成前向计算和反向计算，计算出梯度。每个微批次对应的梯度将会缓存，等到全部微批次完成，缓存的梯度会被加和，算出平均梯度（等同于整个小批次的梯度），完成模型参数的更新。
       </p>
       <div class="figure align-default" id="id13">
        <span id="ch10-pipeline-parallel">
        </span>
        <a class="reference internal image-reference" href="../_images/ch10-pipeline-parallel.png">
         <img alt="../_images/ch10-pipeline-parallel.png" src="../_images/ch10-pipeline-parallel.png" style="width: 800px;"/>
        </a>
        <p class="caption">
         <span class="caption-number">
          图11.3.1
         </span>
         <span class="caption-text">
          流水线并行训练系统（Pipeline Parallel Training System）
         </span>
         <a class="headerlink" href="#id13" title="Permalink to this image">
          ¶
         </a>
        </p>
       </div>
       <p>
        <a class="reference internal" href="#ch10-pipeline-parallel">
         <span class="std std-numref">
          图11.3.1
         </span>
        </a>
        给出了一个流水线训练系统的执行例子。在本例中，模型参数需要切分给4个设备存储。为了充分利用这4个设备，将小批次切分为两个微批次。假设
        <span class="math notranslate nohighlight">
         \(F_{i,j}\)
        </span>
        表示第
        <span class="math notranslate nohighlight">
         \(j\)
        </span>
        个微批次的第
        <span class="math notranslate nohighlight">
         \(i\)
        </span>
        个前向计算任务，
        <span class="math notranslate nohighlight">
         \(B_{i,j}\)
        </span>
        表示第
        <span class="math notranslate nohighlight">
         \(j\)
        </span>
        个微批次的第
        <span class="math notranslate nohighlight">
         \(i\)
        </span>
        个反向计算任务。当设备1完成第一个微批次的前向计算后（表示为
        <span class="math notranslate nohighlight">
         \(F_{0,0}\)
        </span>
        ），会将中间结果发送给设备2，触发相应的前向计算任务（表示为
        <span class="math notranslate nohighlight">
         \(F_{1,0}\)
        </span>
        ）。与此同时，设备1也可以开始第二个微批次的前向计算任务（表示为
        <span class="math notranslate nohighlight">
         \(F_{0,1}\)
        </span>
        ）。前向计算会在流水线的最后一个设备，即设备3，完成。
       </p>
       <p>
        系统于是开始反向计算。设备4开始第1个微批次的反向计算任务（表示为
        <span class="math notranslate nohighlight">
         \(B_{3,0}\)
        </span>
        ）。该任务完成后的中间结果会被发送给设备3，触发相应的反向计算任务（表示为
        <span class="math notranslate nohighlight">
         \(B_{2,0}\)
        </span>
        ）。与此同时，设备4会缓存对应第1个微批次的梯度，接下来开始第2个微批次计算（表示为
        <span class="math notranslate nohighlight">
         \(B_{3,1}\)
        </span>
        ）。当设备4完成了全部的反向计算后，会将本地缓存的梯度进行相加，并且除以微批次数量，计算出平均梯度，该梯度用于更新模型参数。
       </p>
       <p>
        需要注意的是，计算梯度往往需要前向计算中产生的激活值。经典模型并行系统中会将激活值缓存在内存中，反向计算时就可以直接使用，避免重复计算。而在流水线训练系统中，由于内存资源紧张，前向计算中的激活值往往不会缓存，而是在反向计算中重新计算（Recomputation）。
       </p>
       <p>
        在使用流水线训练系统中，时常需要调试微批次的大小，从而达到最优的系统性能。当设备完成前向计算后，必须等到全部反向计算开始，在此期间设备会处于空闲状态。在
        <a class="reference internal" href="#ch10-pipeline-parallel">
         <span class="std std-numref">
          图11.3.1
         </span>
        </a>
        中，可以看到设备1在完成两个前向计算任务后，要等很长时间才能开始两个反向计算任务。这其中的等待时间即被称为流水线气泡（Pipeline
Bubble）。为了减少设备的等待时间，一种常见的做法是尽可能地增加微批次的数量，从而让反向计算尽可能早开始。然而，使用非常小的微批次，可能会造成微批次中的训练样本不足，从而无法充分的利用起来硬件加速器中的海量计算核心。因此最优的微批次数量由多种因素（如流水线深度、微批次大小和加速器计算核心数量等）共同决定。
       </p>
      </div>
     </div>
     <div class="side-doc-outline">
      <div class="side-doc-outline--content">
       <div class="localtoc">
        <p class="caption">
         <span class="caption-text">
          Table Of Contents
         </span>
        </p>
        <ul>
         <li>
          <a class="reference internal" href="#">
           11.2. 实现方法
          </a>
          <ul>
           <li>
            <a class="reference internal" href="#id2">
             11.2.1. 概述
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id3">
             11.2.2. 数据并行
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id4">
             11.2.3. 模型并行
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id5">
             11.2.4. 混合并行
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a class="reference internal" href="#id6">
           11.3. 流水线并行
          </a>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <div class="clearer">
     </div>
    </div>
    <div class="pagenation">
     <a accesskey="P" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="overview.html" id="button-prev" role="botton">
      <i class="pagenation-arrow-L fas fa-arrow-left fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Previous
       </span>
       <div>
        11.1. 系统概述
       </div>
      </div>
     </a>
     <a accesskey="N" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="cluster.html" id="button-next" role="botton">
      <i class="pagenation-arrow-R fas fa-arrow-right fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Next
       </span>
       <div>
        11.4. 机器学习集群架构
       </div>
      </div>
     </a>
    </div>
   </main>
  </div>
 </body>
</html>