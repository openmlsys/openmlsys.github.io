<!DOCTYPE html>
<html>
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
  <meta content="ie=edge" http-equiv="x-ua-compatible"/>
  <title>
   10.2. 分布式方法 — 机器学习系统：设计和实现 1.0.0 documentation
  </title>
  <link href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/sphinx_materialdesign_theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fontawesome/all.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/fonts.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/basic.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/d2l.css" rel="stylesheet" type="text/css"/>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/d2l.js">
  </script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <link href="../_static/favicon.png" rel="shortcut icon"/>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="pipeline.html" rel="next" title="10.3. 流水线并行"/>
  <link href="overview.html" rel="prev" title="10.1. 系统概述"/>
 </head>
 <body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer">
   <header class="mdl-layout__header mdl-layout__header--waterfall">
    <div class="mdl-layout__header-row">
     <nav class="mdl-navigation breadcrumb">
      <a class="mdl-navigation__link" href="index.html">
       <span class="section-number">
        10.
       </span>
       分布式训练
      </a>
      <i class="material-icons">
       navigate_next
      </i>
      <a class="mdl-navigation__link is-active">
       <span class="section-number">
        10.2.
       </span>
       分布式方法
      </a>
     </nav>
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <form action="../search.html" class="form-inline pull-sm-right" method="get">
       <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label class="mdl-button mdl-js-button mdl-button--icon" for="waterfall-exp" id="quick-search-icon">
         <i class="material-icons">
          search
         </i>
        </label>
        <div class="mdl-textfield__expandable-holder">
         <input class="mdl-textfield__input" id="waterfall-exp" name="q" placeholder="Search" type="text"/>
         <input name="check_keywords" type="hidden" value="yes"/>
         <input name="area" type="hidden" value="default"/>
        </div>
       </div>
       <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
        Quick search
       </div>
      </form>
      <a class="mdl-button mdl-js-button mdl-button--icon" href="../_sources/chapter_distributed_training/methods.rst.txt" id="button-show-source" rel="nofollow">
       <i class="material-icons">
        code
       </i>
      </a>
      <div class="mdl-tooltip" data-mdl-for="button-show-source">
       Show Source
      </div>
     </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
     <div class="mdl-layout-spacer">
     </div>
     <nav class="mdl-navigation">
      <a class="mdl-navigation__link" href="https://github.com/openmlsys/openmlsys-zh">
       <i class="fab fa-github">
       </i>
       GitHub
      </a>
     </nav>
    </div>
   </header>
   <header class="mdl-layout__drawer">
    <!-- Title -->
    <span class="mdl-layout-title">
     <a class="title" href="../index.html">
      <span class="title-text">
       机器学习系统：设计和实现
      </span>
     </a>
    </span>
    <div class="globaltoc">
     <span class="mdl-layout-title toc">
      Table Of Contents
     </span>
     <nav class="mdl-navigation">
      <ul class="current">
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_introduction/index.html">
         1. 导论
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">
           1.1. 机器学习应用
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">
           1.2. 机器学习系统的需求
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">
           1.3. 机器学习系统基本组成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_introduction/applicable_readers.html">
           1.4. 适用读者
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_programming_interface/index.html">
         2. 编程接口
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/development_history.html">
           2.1. 机器学习系统编程模型的演进
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">
           2.2. 机器学习工作流
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">
           2.3. 定义深度神经网络
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">
           2.4. C/C++编程接口
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_programming_interface/summary.html">
           2.5. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_computational_graph/index.html">
         3. 计算图
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">
           3.1. 计算图的设计背景和作用
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">
           3.2. 计算图的基本构成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">
           3.3. 计算图的生成
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">
           3.4. 计算图的调度
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_computational_graph/summary.html">
           3.5. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_advanced/index.html">
         4. 第二部分：进阶篇
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_frontend_and_ir/index.html">
         5. 编译器前端
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">
           5.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">
           5.2. 中间表示
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/ad.html">
           5.3. 自动微分
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">
           5.4. 类型系统和静态分析
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">
           5.5. 常见前端编译优化方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_frontend_and_ir/summary.html">
           5.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_backend_and_runtime/index.html">
         6. 编译器后端和运行时
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/overview.html">
           6.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">
           6.2. 计算图优化
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">
           6.3. 算子选择
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">
           6.4. 内存分配
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">
           6.5. 计算调度与执行
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_backend_and_runtime/summary.html">
           6.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_accelerator/index.html">
         7. 硬件加速器
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">
           7.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">
           7.2. 加速器基本组成原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">
           7.3. 加速器基本编程原理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_accelerator/summary.html">
           7.4. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_data_processing/index.html">
         8. 数据处理框架
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/requirements.html">
           8.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/program_model.html">
           8.2. 易用性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/performance.html">
           8.3. 高效性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/data_order.html">
           8.4. 保序性设计
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/extension.html">
           8.5. 单机数据处理性能的扩展
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_data_processing/summary.html">
           8.6. 章节总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_model_deployment/index.html">
         9. 模型部署
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">
           9.1. 概述
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">
           9.2. 训练模型到推理模型的转换及优化
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_compression.html">
           9.3. 模型压缩
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_inference.html">
           9.4. 模型推理
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/model_security.html">
           9.5. 模型的安全保护
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_model_deployment/summary.html">
           9.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1 current">
        <a class="reference internal" href="index.html">
         10. 分布式训练
        </a>
        <ul class="current">
         <li class="toctree-l2">
          <a class="reference internal" href="overview.html">
           10.1. 系统概述
          </a>
         </li>
         <li class="toctree-l2 current">
          <a class="current reference internal" href="#">
           10.2. 分布式方法
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="pipeline.html">
           10.3. 流水线并行
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="collective.html">
           10.4. 集合通讯
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="parameter_servers.html">
           10.5. 参数服务器
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="summary.html">
           10.6. 总结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_preface_extension/index.html">
         11. 第三部分：拓展篇
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_federated_learning/index.html">
         12. 联邦学习系统
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
         13. 强化学习系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">
           13.1. 强化学习介绍
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">
           13.2. 单节点强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/marl.html">
           13.3. 多智能体强化学习
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">
           13.4. 多智能体强化学习系统
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_reinforcement_learning/summary.html">
           13.5. 小结
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_explainable_AI/index.html">
         14. 可解释性AI系统
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">
           14.1. 可解释机器学习系统
          </a>
         </li>
        </ul>
       </li>
      </ul>
      <ul>
       <li class="toctree-l1">
        <a class="reference internal" href="../appendix_machine_learning_introduction/index.html">
         附录：机器学习介绍
        </a>
        <ul>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">
           1. 神经网络
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">
           2. 梯度下降与反向传播
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">
           3. 经典机器学习方法
          </a>
         </li>
        </ul>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../chapter_references/index.html">
         参考文献
        </a>
       </li>
      </ul>
     </nav>
    </div>
   </header>
   <main class="mdl-layout__content" tabindex="0">
    <script src="../_static/sphinx_materialdesign_theme.js " type="text/javascript">
    </script>
    <header class="mdl-layout__drawer">
     <!-- Title -->
     <span class="mdl-layout-title">
      <a class="title" href="../index.html">
       <span class="title-text">
        机器学习系统：设计和实现
       </span>
      </a>
     </span>
     <div class="globaltoc">
      <span class="mdl-layout-title toc">
       Table Of Contents
      </span>
      <nav class="mdl-navigation">
       <ul class="current">
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_introduction/index.html">
          1. 导论
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/machine_learning_applications.html">
            1.1. 机器学习应用
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/requirements_for_machine_learning_systems.html">
            1.2. 机器学习系统的需求
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/components_of_machine_learning_systems.html">
            1.3. 机器学习系统基本组成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_introduction/applicable_readers.html">
            1.4. 适用读者
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_programming_interface/index.html">
          2. 编程接口
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/development_history.html">
            2.1. 机器学习系统编程模型的演进
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/ml_workflow.html">
            2.2. 机器学习工作流
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/neural_network_layer.html">
            2.3. 定义深度神经网络
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/c_python_interaction.html">
            2.4. C/C++编程接口
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_programming_interface/summary.html">
            2.5. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_computational_graph/index.html">
          3. 计算图
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/background_and_functionality.html">
            3.1. 计算图的设计背景和作用
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/components_of_computational_graph.html">
            3.2. 计算图的基本构成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/generation_of_computational_graph.html">
            3.3. 计算图的生成
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/schedule_of_computational_graph.html">
            3.4. 计算图的调度
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_computational_graph/summary.html">
            3.5. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_advanced/index.html">
          4. 第二部分：进阶篇
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_frontend_and_ir/index.html">
          5. 编译器前端
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/overview_of_frontend.html">
            5.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/intermediate_representation.html">
            5.2. 中间表示
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/ad.html">
            5.3. 自动微分
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/type_system_and_static_analysis.html">
            5.4. 类型系统和静态分析
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/common_frontend_optimization_pass.html">
            5.5. 常见前端编译优化方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_frontend_and_ir/summary.html">
            5.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_backend_and_runtime/index.html">
          6. 编译器后端和运行时
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/overview.html">
            6.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/graph_optimizer.html">
            6.2. 计算图优化
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/kernel_selecter.html">
            6.3. 算子选择
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/memory_allocator.html">
            6.4. 内存分配
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/compute_schedule_and_execute.html">
            6.5. 计算调度与执行
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_backend_and_runtime/summary.html">
            6.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_accelerator/index.html">
          7. 硬件加速器
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_introduction.html">
            7.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_architecture.html">
            7.2. 加速器基本组成原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/accelerator_programming.html">
            7.3. 加速器基本编程原理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_accelerator/summary.html">
            7.4. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_data_processing/index.html">
          8. 数据处理框架
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/requirements.html">
            8.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/program_model.html">
            8.2. 易用性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/performance.html">
            8.3. 高效性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/data_order.html">
            8.4. 保序性设计
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/extension.html">
            8.5. 单机数据处理性能的扩展
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_data_processing/summary.html">
            8.6. 章节总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_model_deployment/index.html">
          9. 模型部署
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_deployment_introduction.html">
            9.1. 概述
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_converter_and_optimizer.html">
            9.2. 训练模型到推理模型的转换及优化
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_compression.html">
            9.3. 模型压缩
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_inference.html">
            9.4. 模型推理
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/model_security.html">
            9.5. 模型的安全保护
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_model_deployment/summary.html">
            9.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1 current">
         <a class="reference internal" href="index.html">
          10. 分布式训练
         </a>
         <ul class="current">
          <li class="toctree-l2">
           <a class="reference internal" href="overview.html">
            10.1. 系统概述
           </a>
          </li>
          <li class="toctree-l2 current">
           <a class="current reference internal" href="#">
            10.2. 分布式方法
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="pipeline.html">
            10.3. 流水线并行
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="collective.html">
            10.4. 集合通讯
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="parameter_servers.html">
            10.5. 参数服务器
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="summary.html">
            10.6. 总结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_preface_extension/index.html">
          11. 第三部分：拓展篇
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_federated_learning/index.html">
          12. 联邦学习系统
         </a>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_reinforcement_learning/index.html">
          13. 强化学习系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/rl_introduction.html">
            13.1. 强化学习介绍
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/single_node_rl.html">
            13.2. 单节点强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/marl.html">
            13.3. 多智能体强化学习
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/marl_sys.html">
            13.4. 多智能体强化学习系统
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_reinforcement_learning/summary.html">
            13.5. 小结
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_explainable_AI/index.html">
          14. 可解释性AI系统
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../chapter_explainable_AI/explainable_ai.html">
            14.1. 可解释机器学习系统
           </a>
          </li>
         </ul>
        </li>
       </ul>
       <ul>
        <li class="toctree-l1">
         <a class="reference internal" href="../appendix_machine_learning_introduction/index.html">
          附录：机器学习介绍
         </a>
         <ul>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/neural_network.html">
            1. 神经网络
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/gradient_descent.html">
            2. 梯度下降与反向传播
           </a>
          </li>
          <li class="toctree-l2">
           <a class="reference internal" href="../appendix_machine_learning_introduction/classic_machine_learning.html">
            3. 经典机器学习方法
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l1">
         <a class="reference internal" href="../chapter_references/index.html">
          参考文献
         </a>
        </li>
       </ul>
      </nav>
     </div>
    </header>
    <div class="document">
     <div class="page-content" role="main">
      <section id="id1">
       <h1>
        <span class="section-number">
         10.2.
        </span>
        分布式方法
        <a class="headerlink" href="#id1" title="Permalink to this headline">
         ¶
        </a>
       </h1>
       <p>
        我们会讨论分布式训练系统实现的常用并行方法。我们首先给出并行方法的设计目标以及分类。然后，我们会详细描述各个并行方法。
       </p>
       <section id="id2">
        <h2>
         <span class="section-number">
          10.2.1.
         </span>
         概述
         <a class="headerlink" href="#id2" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <figure class="align-default" id="id6">
         <span id="ch10-single-node">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-single-node.svg">
          <img alt="../_images/ch10-single-node.svg" src="../_images/ch10-single-node.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图10.2.1
           </span>
           <span class="caption-text">
            单节点训练系统
           </span>
           <a class="headerlink" href="#id6" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         分布式训练系统的设计目标是：将单节点训练系统转化成
         <strong>
          等价的
         </strong>
         并行训练系统，从而在不影响模型精度的条件下完成训练过程的加速。一个单节点训练系统往往如
         <a class="reference internal" href="#ch10-single-node">
          <span class="std std-numref">
           图10.2.1
          </span>
         </a>
         所示。一个训练过程会由多个数据小批次（mini-batch）完成。在图中，一个数据小批次被标示为
         <strong>
          数据
         </strong>
         。训练系统会利用数据小批次来生成梯度，提升模型精度。这个过程由一个训练
         <strong>
          程序
         </strong>
         实现。在实际中，这个程序往往实现了一个多层神经网络的执行过程。
该神经网络的执行由一个计算图（Computational
Graph）表达。这个图有多个相互连接的算子（Operator），每个算子会拥有计算参数。每个算子往往会实现一个神经网络层（Neural
Network Layer），而参数则代表了这个层在训练中所更新的的权重（Weights）。
        </p>
        <p>
         为了更新参数，计算图的执行会分为
         <strong>
          前向
         </strong>
         传播和
         <strong>
          反向
         </strong>
         传播两个阶段。前向传播的第一步会将数据读入第一个算子，该算子会根据当前的参数，计算出传播给下一个算子的数据。算子依次重复这个前向传播的过程（算子1
-&gt; 算子2 -&gt;
算子3），直到最后一个算子结束。最后的算子随之马上开始反向传播。反向传播中，每个算子依次计算出梯度（梯度3
-&gt; 梯度2 -&gt;
梯度1），并利用梯度更新本地的参数。反向传播最终在第一个算子结束。反向传播的结束也标志本次数据小批次的结束，系统随之读取下一个小批次，继续更新模型。
        </p>
        <span id="ch10-parallel-methods">
        </span>
        <table class="docutils align-default" id="id7" style="margin-left:auto;margin-right:auto;margin-top:10px;margin-bottom:20px;">
         <caption>
          <span class="caption-number">
           表10.2.1
          </span>
          <span class="caption-text">
           分布式训练方法分类
          </span>
          <a class="headerlink" href="#id7" title="Permalink to this table">
           ¶
          </a>
         </caption>
         <colgroup>
          <col style="width: 12%"/>
          <col style="width: 44%"/>
          <col style="width: 44%"/>
         </colgroup>
         <thead>
          <tr class="row-odd">
           <th class="head">
            <p>
            </p>
           </th>
           <th class="head">
            <p>
             单数据
            </p>
           </th>
           <th class="head">
            <p>
             多数据
            </p>
           </th>
          </tr>
         </thead>
         <tbody>
          <tr class="row-even">
           <td>
            <p>
             单程序
            </p>
           </td>
           <td>
            <p>
             单程序单数据：单点执行
            </p>
           </td>
           <td>
            <p>
             单程序多数据：数据并行
            </p>
           </td>
          </tr>
          <tr class="row-odd">
           <td>
            <p>
             多程序
            </p>
           </td>
           <td>
            <p>
             多程序单数据：模型并行
            </p>
           </td>
           <td>
            <p>
             多程序多数据：混合并行
            </p>
           </td>
          </tr>
         </tbody>
        </table>
        <p>
         给定一个单节点训练系统，人们会对
         <strong>
          数据
         </strong>
         和
         <strong>
          程序
         </strong>
         分区（Partition），从而完成并行加速。
         <a class="reference internal" href="#ch10-parallel-methods">
          <span class="std std-numref">
           表10.2.1
          </span>
         </a>
         总结了不同的切分方法。单节点训练系统可以被归类于
单程序单数据模式。而假如用户希望使用更多的设备来实现并行计算，他们首先可以选择对数据进行分区，并将同一个程序复制到多个设备上并行执行。这种方式是单程序多数据模式，常被称为
         <strong>
          数据并行
         </strong>
         （Data
Parallelism）。另一种并行方式是对程序进行分区：程序的算子会被分发给多个设备按照依次完成。这种模式是
多程序单数据模式，常被称为
         <strong>
          模型并行
         </strong>
         （Model
Parallelism）。当训练超大型智能模型时，开发人们往往要同时对数据和程序进行切分，从而实现最高程度的并行。这种模式是多程序多数据模式，常被称为
         <strong>
          混合并行
         </strong>
         （Hybrid
Parallelism）。
        </p>
        <p>
         接下来，我们详细讲解各种并行方法的执行过程。
        </p>
       </section>
       <section id="id3">
        <h2>
         <span class="section-number">
          10.2.2.
         </span>
         数据并行
         <a class="headerlink" href="#id3" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <figure class="align-default" id="id8">
         <span id="ch10-data-parallel">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-data-parallel.svg">
          <img alt="../_images/ch10-data-parallel.svg" src="../_images/ch10-data-parallel.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图10.2.2
           </span>
           <span class="caption-text">
            数据并行训练系统
           </span>
           <a class="headerlink" href="#id8" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         数据并行往往可以解决单节点的算力不足。这种并行方式在人工智能框架中最为常见，具体实现包括：TensorFlow
DistributedStrategy，PyTorch Distributed，Horovod
DistributedOptimizer等。在一个数据并行系统中，假设用户给定一个训练批大小
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         ，并且希望使用
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个并行设备来加速训练。那么，该训练批大小会被分为
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个分区，每个设备会分配到
         <span class="math notranslate nohighlight">
          \(N/M\)
         </span>
         个训练样本。这些设备共享一个训练程序的副本，在不同数据分区上独立执行，计算梯度。不同的设备（假设设备编号为
         <span class="math notranslate nohighlight">
          \(i\)
         </span>
         ）会根据本地的训练样本估计出梯度
         <span class="math notranslate nohighlight">
          \(G_i\)
         </span>
         。为了确保训练程序参数的一致性，本地梯度
         <span class="math notranslate nohighlight">
          \(G_i\)
         </span>
         需要聚合，计算出平均梯度
         <span class="math notranslate nohighlight">
          \((\sum_{i=1}^{N} G_i) / N\)
         </span>
         。最终，训练程序利用平均梯度修正模型参数，完成小批量的训练。
        </p>
        <p>
         <a class="reference internal" href="#ch10-data-parallel">
          <span class="std std-numref">
           图10.2.2
          </span>
         </a>
         展示了2个设备构成的数据并行例子。假设用户给定的批大小（Batch
Size）是64，那么每个设备会分配到32个训练样本，并且具有相同的神经网络参数（程序副本）。本地的训练样本会依次通过这个程序副本中的算子，完成前向传播和反向传播。在反向传播的过程中，程序副本会生成局部梯度。不同设备上对应的局部梯度（如设备1和设备2上各自的梯度1）会进行聚合，从而计算平均梯度。这个聚合的过程往往由集合通讯库（Collective
Communication）的Allreduce操作来完成。
        </p>
       </section>
       <section id="id4">
        <h2>
         <span class="section-number">
          10.2.3.
         </span>
         模型并行
         <a class="headerlink" href="#id4" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <figure class="align-default" id="id9">
         <span id="ch10-model-parallel-intra-op">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-model-parallel-intra-op.svg">
          <img alt="../_images/ch10-model-parallel-intra-op.svg" src="../_images/ch10-model-parallel-intra-op.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图10.2.3
           </span>
           <span class="caption-text">
            模型并行系统：算子内并行
           </span>
           <a class="headerlink" href="#id9" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         模型并行往往用于解决单节点的内存不足问题。一个常见的内存不足场景是模型中含有大型算子，例如说深度神经网络中需要计算大量分类的全连接层（Fully
Connected
Layer）。完成这种大型算子计算所需的内存可能超过单设备的内存容量。那么我们需要对这个大型算子进行切分。假设这个算子具有
         <span class="math notranslate nohighlight">
          \(P\)
         </span>
         个参数，而我们拥有
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         个设备，那么我们可以将
         <span class="math notranslate nohighlight">
          \(P\)
         </span>
         个参数平均分配给
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         个设备（每个设备分配
         <span class="math notranslate nohighlight">
          \(P/N\)
         </span>
         个参数），从而让每个设备负责更少的计算量，能够在内存容量的限制下完成前向传播和反向传播中所需的计算。这种切分方式是模型并行的应用，被称为
         <strong>
          算子内并行
         </strong>
         （Intra-operator
Parallelism）。
        </p>
        <p>
         <a class="reference internal" href="#ch10-model-parallel-intra-op">
          <span class="std std-numref">
           图10.2.3
          </span>
         </a>
         给出了一个由2个设备实现的算子内并行的例子。在这个例子中，假设一个神经网络具有2个算子，算子1的计算（包含正向和反向传播）需要预留16G的内存，算子2的计算需要预留1G的内存。而本例中的设备最多可以提供10G的内存。为了完成这个神经网络的训练，我们需要对算子1实现并行。具体做法是，将算子1的参数平均分区，设备1和设备2各负责其中部分算子1的参数。由于设备1和设备2的参数不同，因此它们各自负责程序分区1和程序分区2。在训练这个神经网络的过程中，数据（小批量）会首先传给算子1。由于算子1的参数分别由2个设备负责，因此数据会被广播给这2个设备。不同设备根据本地的参数分区完成前向计算，生成的本地计算结果需要进一步合并（Combine），发送给下游的算子2。在反向传播中，算子2的数据会被广播给设备1和设备2，这些设备根据本地的算子1分区各自完成局部的反向计算。计算结果进一步合并传播回数据，最终完成反向传播。
        </p>
        <p>
         另一种内存不足的场景是：模型的总内存需求超过了单设备的内存容量。在这种场景下，假如我们总共有
         <span class="math notranslate nohighlight">
          \(N\)
         </span>
         个算子和
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个设备，我们可以将算子平摊给这
         <span class="math notranslate nohighlight">
          \(M\)
         </span>
         个设备，让每个设备仅需负责
         <span class="math notranslate nohighlight">
          \(N/M\)
         </span>
         个算子的前向和反向计算，降低设备的内存开销。这种并行方式是模型并行的另一种应用，被称为
         <strong>
          算子间并行
         </strong>
         （Inter-operator
Parallelism）。
        </p>
        <figure class="align-default" id="id10">
         <span id="ch10-model-parallel-inter-op">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-model-parallel-inter-op.svg">
          <img alt="../_images/ch10-model-parallel-inter-op.svg" src="../_images/ch10-model-parallel-inter-op.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图10.2.4
           </span>
           <span class="caption-text">
            模型并行系统：算子间并行
           </span>
           <a class="headerlink" href="#id10" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         <a class="reference internal" href="#ch10-model-parallel-inter-op">
          <span class="std std-numref">
           图10.2.4
          </span>
         </a>
         给出了一个由2个设备实现的算子间并行的例子。在这个例子中，假设一个神经网络具有2个算子，算子1和算子2各自需要10G的内存完成计算，则模型总共需要20G的内存。而每个设备仅能提供10G内存。在这个例子中，用户可以把算子1放置在设备1上，算子2放置在设备2上。在前向传播中，算子1的输出会被发送（Send）给下游的设备2。设备2接收（Receive）来自上游的数据，完成算子2的前向计算。在反向传播中，设备2将算子2的反向计算结果发送给设备1。设备1完成算子1的反向计算，完成本次训练。
        </p>
       </section>
       <section id="id5">
        <h2>
         <span class="section-number">
          10.2.4.
         </span>
         混合并行
         <a class="headerlink" href="#id5" title="Permalink to this headline">
          ¶
         </a>
        </h2>
        <figure class="align-default" id="id11">
         <span id="ch10-hybrid-parallel">
         </span>
         <a class="reference internal image-reference" href="../_images/ch10-hybrid-parallel.svg">
          <img alt="../_images/ch10-hybrid-parallel.svg" src="../_images/ch10-hybrid-parallel.svg" width="800px"/>
         </a>
         <figcaption>
          <p>
           <span class="caption-number">
            图10.2.5
           </span>
           <span class="caption-text">
            混合并行系统
           </span>
           <a class="headerlink" href="#id11" title="Permalink to this image">
            ¶
           </a>
          </p>
         </figcaption>
        </figure>
        <p>
         在训练大型人工智能模型中，我们往往会同时面对算力不足和内存不足。因此，我们需要混合使用数据并行和模型并行，这种方法被称为混合并行。
         <a class="reference internal" href="#ch10-hybrid-parallel">
          <span class="std std-numref">
           图10.2.5
          </span>
         </a>
         提供了一个由4个设备实现的混合并行的例子。在这个例子中，我们首先实现算子间并行来解决训练程序内存开销过大的问题：该训练程序的算子1和算子2被分摊到了设备1和设备2上。进一步，我们通过数据并行来添加3和设备4，提升系统算力。为了达到这一点，我们对训练数据进行分区（数据分区1和数据分区2），并将模型（算子1和算子2）分配复制到设备3和设备4上生成可以并行执行的程序副本。在前向计算的过程中，设备1和设备3上的算子1副本同时开始，计算结果分别发送（Send）给设备2和设备4完成算子2副本的计算。在反向计算中，设备2和设备4同时开始计算梯度，本地梯度通过Allreduce进行平均。反向计算传递到设备1和设备3上的算子1副本结束。
        </p>
       </section>
      </section>
     </div>
     <div class="side-doc-outline">
      <div class="side-doc-outline--content">
       <div class="localtoc">
        <p class="caption">
         <span class="caption-text">
          Table Of Contents
         </span>
        </p>
        <ul>
         <li>
          <a class="reference internal" href="#">
           10.2. 分布式方法
          </a>
          <ul>
           <li>
            <a class="reference internal" href="#id2">
             10.2.1. 概述
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id3">
             10.2.2. 数据并行
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id4">
             10.2.3. 模型并行
            </a>
           </li>
           <li>
            <a class="reference internal" href="#id5">
             10.2.4. 混合并行
            </a>
           </li>
          </ul>
         </li>
        </ul>
       </div>
      </div>
     </div>
     <div class="clearer">
     </div>
    </div>
    <div class="pagenation">
     <a accesskey="P" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="overview.html" id="button-prev" role="botton">
      <i class="pagenation-arrow-L fas fa-arrow-left fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Previous
       </span>
       <div>
        10.1. 系统概述
       </div>
      </div>
     </a>
     <a accesskey="N" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" href="pipeline.html" id="button-next" role="botton">
      <i class="pagenation-arrow-R fas fa-arrow-right fa-lg">
      </i>
      <div class="pagenation-text">
       <span class="pagenation-direction">
        Next
       </span>
       <div>
        10.3. 流水线并行
       </div>
      </div>
     </a>
    </div>
   </main>
  </div>
 </body>
</html>